[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DP4SS",
    "section": "",
    "text": "Introduction\n \n\n\nData Programming for the Social Sciences (DP4SS):\n\nA gentle introduction to data programming in R for social science audiences\n\n\n\n\n\nThis textbook consists of adapted lecture notes from a graduate level introductory course in data science that is offered at Arizona State University in the School of Public Affairs.\nIf you are interested in a career in data science check out the M.S. in Program Evaluation and Data Analytics at ASU.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "0_00-ds-toolkit.html",
    "href": "0_00-ds-toolkit.html",
    "title": "Your Data Science Toolkit",
    "section": "",
    "text": "This section introduces the three tools you will need to create data science projects:\n\nA data programming language (R)\nA project “environment” (R Studio)\nData-driven documents (R Markdown)",
    "crumbs": [
      "Your Data Science Toolkit"
    ]
  },
  {
    "objectID": "0_10-core-r.html",
    "href": "0_10-core-r.html",
    "title": "1  The R Language",
    "section": "",
    "text": "1.1 Key Concepts\nR is a specialized programming language created by statisticians for data analysis and visualization.",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The **R** Language</span>"
    ]
  },
  {
    "objectID": "0_10-core-r.html#key-concepts",
    "href": "0_10-core-r.html#key-concepts",
    "title": "1  The R Language",
    "section": "",
    "text": "R Console\nBase R\nPackages\nComprehensive R Archive Network (CRAN)",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The **R** Language</span>"
    ]
  },
  {
    "objectID": "0_10-core-r.html#r-an-open-source-language-for-statistical-computing",
    "href": "0_10-core-r.html#r-an-open-source-language-for-statistical-computing",
    "title": "1  The R Language",
    "section": "1.2 R: An Open Source Language for Statistical Computing",
    "text": "1.2 R: An Open Source Language for Statistical Computing\nR is a language that was designed for statistical computing, the art of combining computer science tools for problem-solving with models from statistics. The goal is to turn raw data into useful, actionable insights. This field has come to be known as Data Science.\nR is an open source language, which means that applications built in R are not only free, but users are allowed to access and modify the source code.\nAs a result of this design approach, it is extremely easy to develop and adapt code in R. Because of the freedom this provides, R users have expanded the power and functionality of Core R for nearly a quarter century.\nCustom applications and tools that users create for R are called packages (also called libraries when you are loading them). Packages are programs designed to perform a specific type of analysis or visualization.\nThe best part of R is how easy it is to access cutting edge software by installing new packages in a two lines of code:\n\ninstall.packages( \"tidyverse\" )   # install the package\nlibrary( \"tidyverse\" )            # load the package\n\n\nPopular R Packages: [ A RECENT LIST ]",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The **R** Language</span>"
    ]
  },
  {
    "objectID": "0_10-core-r.html#r-as-a-social-network",
    "href": "0_10-core-r.html#r-as-a-social-network",
    "title": "1  The R Language",
    "section": "1.3 R as a Social Network",
    "text": "1.3 R as a Social Network\nThe R Foundation is a nonprofit that maintains the R language and ensures it remains free and accessible to everyone in the world. Packages are shared through the Comprehensive R Archival Network (The CRAN), a group of servers housed primarily at universities that store R packages so they can be quickly downloaded and deployed.\nThere are over 15,000 packages that users have created for R. They perform a wide variety of tasks such as data preparation, specialized statistical analysis, custom data visualizations, or specific analytical tasks such as text analysis or network analysis.\nThis functionality is a primary reason R has become one of the most popular languages used by academics and data scientists. Provides a very simple way for people to develop cool tools and share them with the world. It became popular because it was built by smart and creative people, who attracted other smart and creative people, who created cool tools, which then attracted more smart and creative people. R Package Downloads",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The **R** Language</span>"
    ]
  },
  {
    "objectID": "0_10-core-r.html#r-as-an-operating-system",
    "href": "0_10-core-r.html#r-as-an-operating-system",
    "title": "1  The R Language",
    "section": "1.4 R as an Operating System",
    "text": "1.4 R as an Operating System\nR is a programming language. We can think of a programming languages as instructions that are evaluated and carried out by a computer. R, then, is simply one way to give instructions to computers.\nThis is a limited view of R, though. It is better understood as an operating system for data science software. Just as Windows allows you to turn on your computer, open a web browser, moved files around, and write a paper using MS Word, R allows you to access the CRAN, install and run packages, and manage files while organizing large data projects. Just like Windows would be a very boring piece of software without all of the applications you run while on the computer, R would be a boring language without all of the packages it can run.\n\n\n\n\n\nR is both a programming language and programming environment.",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The **R** Language</span>"
    ]
  },
  {
    "objectID": "0_10-core-r.html#downloading-installing-r",
    "href": "0_10-core-r.html#downloading-installing-r",
    "title": "1  The R Language",
    "section": "1.5 Downloading & Installing R",
    "text": "1.5 Downloading & Installing R\nYou can download and install R quickly and easily from the Comprehensive R Archive Network, or CRAN. It is a decentralized website that’s hosted and updated by academic institutions all over the world. In other words, R would survive a semi-global catastrophic event. It contains:\n\nThe latest version and past versions of R\nExtensions, also called packages, for R\nPackage and version documentation\nBooks, blogs, conferences, news, etc.",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The **R** Language</span>"
    ]
  },
  {
    "objectID": "0_10-core-r.html#r-console",
    "href": "0_10-core-r.html#r-console",
    "title": "1  The R Language",
    "section": "1.6 R Console",
    "text": "1.6 R Console\nAfter installing, when you open Base R directly you will see the command-line interface, or a console. This is used to type R code is directly evaluated by the environment, a process known as working interactively.\nWhile this is practice is a quick way to run some simple code, it is difficult to develop complex programs in real-time (it would be like writing a play while it is being acted out). A more typical and organized way to create data recipes is through scripting, which we address below.",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The **R** Language</span>"
    ]
  },
  {
    "objectID": "0_10-core-r.html#extending-rs-functionality-packages",
    "href": "0_10-core-r.html#extending-rs-functionality-packages",
    "title": "1  The R Language",
    "section": "1.7 Extending R’s Functionality: Packages",
    "text": "1.7 Extending R’s Functionality: Packages\nPackages are collections of new commands, a.k.a. functions, that are developed and shared by the worldwide R userbase. Packages greatly expand the power and functionality of base R, the “vanilla” or unmodified version of R. While CRAN is the most popular package archive, others include Bioconductor and GitHub.\n\nIf R were the Constitution of a nation, packages would be its amendments - they not only provide more freedom for the user, they address new ideas and practices that were unforeseen by R’s founders. More on packages:\n\nFunctions and packages are developed in response to identified needs\nIf your needs are unmet by base R, there’s likely a package for it\nAltogether, there are over 18,000 packages on CRAN, alone\nThere are tens of thousands of unpublished packages\nEntire ecosystems of packages exist, e.g, Tidyverse\n\n\n\n\n\n\n\nPackages give users more freedom and resolve issues unforeseen by R’s founders.\n\n\n\n\n\nYou can install packages in R by calling the install.packages() function, with the package name in quotations:\n\ninstall.packages(\"my_package\")\n\nOnce installed, you can load packages by calling the library() function, without quotations.\nNote: You only need to install a package once. However, you must load each package every time you start R:\n\nlibrary(my_package)\n\nNote: “Packages” and “libraries” are two words for the same thing. They both refer to a set of functions that have been “packaged” or are organized into a “library” to be shared.\n\n\nFun Fact: R is an implementation of an older programming language, S. John Chambers first developed S in 1976 to make statistical analysis a point-and-click, interactive, and user-friendly process. However, Chambers’ underlying philosophy reflects the use of R packages to this day:\n\n“We wanted users to be able to be in in an interactive environment, where they did not consciously think of themselves as programming. Then as their needs became clearer and their sophistication increases, they should be able to slide gradually into programming, when the language and system aspects would become more important.”",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The **R** Language</span>"
    ]
  },
  {
    "objectID": "0_10-core-r.html#resources",
    "href": "0_10-core-r.html#resources",
    "title": "1  The R Language",
    "section": "1.8 Resources",
    "text": "1.8 Resources\nThere’s a litany of online and print resources introducing the R language. Here are a few that we find instructive:\n\nI) Full-Length Introductions to R:\n\n“Part I: Foundations, Introduction to R” (Lecy, 2018)\n“Intro to R: Nuts * Bolts” (Crawford, 2018)\n\nII) Publications & Articles:\n\n“What is R? Introduction to R and the R Environment” (CRAN, 2001)\n“R: A Language for Data Analysis and Graphics” (Ihaka & Gentleman, 1996)\n\nIII) Handouts & Cheat Sheets:\n\n“R: Some Helpful Vocabulary” (Lecy, 2017)\n“Base R Cheat Sheet” (RStudio, 2016)\n\nIV) Videos:\n\n“R in 60 Seconds” (Lecy, 2018)\n“John Chambers Interview [On the History of S & R]” (Statistical Learning, 2013)",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The **R** Language</span>"
    ]
  },
  {
    "objectID": "0_20-rstudio.html",
    "href": "0_20-rstudio.html",
    "title": "2  R Studio",
    "section": "",
    "text": "2.1 Key Concepts\nThis chapter introduces RStudio, a Graphical User Interface (GUI) that makes it easier to use powerful features in R and manage large projects.\nNew vocabulary:",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Studio</span>"
    ]
  },
  {
    "objectID": "0_20-rstudio.html#key-concepts",
    "href": "0_20-rstudio.html#key-concepts",
    "title": "2  R Studio",
    "section": "",
    "text": "Integrated Development Environment (IDE)\nRStudio Panes:\n\nConsole\nSource\nPlots\nViewer\nEnvironment\nHistory\n\nWorkspace & Global Environment",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Studio</span>"
    ]
  },
  {
    "objectID": "0_20-rstudio.html#what-is-rstudio",
    "href": "0_20-rstudio.html#what-is-rstudio",
    "title": "2  R Studio",
    "section": "2.2 What is RStudio?",
    "text": "2.2 What is RStudio?\nRecall that R is both a language and an environment. RStudio is an Integrated Development Environment, or IDE, which is an enhanced, feature-rich programming environment with an easy-to-use graphical user interface, or GUI. While the base R environment is mostly text, RStudio has intuitive icons (hence, “graphical”) for point-and-click, automated operations.\n\nRStudio’s layout is comprised of a menu, console, and a series of panes, or windows in the RStudio interface. Most panes are feature-rich and all panes serve a key purpose, but we’ll only focus on the most critical panes for getting started in RStudio.\n\n\n\n\n\n\nThe RStudio environment with four open panes: The Source, Console, Environment, and Viewer panes.",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Studio</span>"
    ]
  },
  {
    "objectID": "0_20-rstudio.html#downloading-installing-rstudio",
    "href": "0_20-rstudio.html#downloading-installing-rstudio",
    "title": "2  R Studio",
    "section": "2.3 Downloading & Installing RStudio",
    "text": "2.3 Downloading & Installing RStudio\nRStudio requires R 3.0.1+. If you don’t already have R, download it here.\n\nDOWNLOAD R\n\n\nRStudio is free, open source, and easy to install. Select the Desktop edition on their download page:\n\nDOWNLOAD R STUDIO",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Studio</span>"
    ]
  },
  {
    "objectID": "0_20-rstudio.html#a-tour-of-rstudio",
    "href": "0_20-rstudio.html#a-tour-of-rstudio",
    "title": "2  R Studio",
    "section": "2.4 A Tour of RStudio",
    "text": "2.4 A Tour of RStudio\nRStudio is comprised of a main menu and a series of panes, each with their own purpose and features. We focus on the following:\n\nConsole Pane\nSource Pane\nPlots Pane\nViewer Pane\nEnvironment Pane\nHistory Pane",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Studio</span>"
    ]
  },
  {
    "objectID": "0_20-rstudio.html#the-console-pane",
    "href": "0_20-rstudio.html#the-console-pane",
    "title": "2  R Studio",
    "section": "2.5 The Console Pane",
    "text": "2.5 The Console Pane\nThe console pane is where R expressions are evaluated. In other words, this is where your code is executed. Recall that working in-console is also known as working interactively and, typically, working in-console is more often for “quick and dirty” tasks, like printing contents of your working directory.\n\nThe Console Pane\n\n\n\n\n\n\nThe console is where code is executed and is typically used for “quick and dirty” tasks.\n\n\n\n\n\nThe console panel lists your current working directory. Notably, even when using point-and-click mechanics to, e.g. import data or change directories, the code for such tasks will still execute in the console. Such click-to-code operations are called macros.\n\nPro Tip: When writing a script, especially when writing out new directory paths, it’s sometimes quicker to use use a click-to-code operation and simply copy and paste the macro code from the console to your script.",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Studio</span>"
    ]
  },
  {
    "objectID": "0_20-rstudio.html#the-source-pane",
    "href": "0_20-rstudio.html#the-source-pane",
    "title": "2  R Studio",
    "section": "2.6 The Source Pane",
    "text": "2.6 The Source Pane\nThe source pane contains any opened scripts. In starting a new R session, this pane isn’t visible until you’ve opened a new or preexisting script. Multiple scripts may be opened at one time and are navigable using tabs along the top of the source pane.\n\nThe Source Pane\n\n\n\n\n\n\nThe source pane contains opened scripts. It won’t appear until you’ve opened at least one.\n\n\n\n\n\nDepending on the type of script, e.g. plain text scripts (.R), publications (.Rmd), presentations (.Rpres), and apps, each script provides different options in the pane’s toolbar. Common options include:\n\nShow in New Window: Open the script in a separate window; valuable for two or more monitors\nSave Current Document: Update an existing script or title and save a new script\nFind/Replace: Both conventional and advanced options to find and replace code\nRun: Run the line of code where the cursor is, or multiple lines if highlighted\nShow Document Outline: View (and jump to) script’s table of contents",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Studio</span>"
    ]
  },
  {
    "objectID": "0_20-rstudio.html#plots-viewer-help-panes",
    "href": "0_20-rstudio.html#plots-viewer-help-panes",
    "title": "2  R Studio",
    "section": "2.7 Plots, Viewer, & Help Panes",
    "text": "2.7 Plots, Viewer, & Help Panes\nThe Plots, Viewer, and Help panes are used to viewing data visualizations, HTML output, and helpful documentation.\n\n\n2.7.1 The Plots Pane\nThe Plots pane allows users to view, export, and publish non-interactive data visualizations. R uses the built-in graphics package by default, but a variety of packages exist such as lattice and ggplot2. While the output displayed is not interactive, it is responsive, i.e it will re-render its scales appropriately if you change the height or width of a plot. Notably, the “Zoom” option opens visualizations in a new window, while the “Export” option allows you to save the image with user-defined dimensions and in a variety of formats.\n\n\n\n2.7.2 The Viewer Pane\nThe Viewer pane renders interactive graphics in HTML with the same options as the Plots pane. Brevity aside, it’s awesome.\n\n\nThe Plots & Viewer Panes\n\n\n\n\n\n\nThe Plots and Viewer panes visualize non-interactive and interactive graphics, respectively.\n\n\n\n\n\n\n\n2.7.3 The Help Pane\nThe Help pane is one of the most valuable panes for any R user. By calling function help() with a dataset, package, or bare function name (i.e. a function name without ()), its documentation, if available, appears here.\n\nNote: Unless you’re using external data or custom functions, there’s almost always documentation. Whether it’s the unit of measurement for a variable in a dataset or the limits you can specify for a function argument, this little nook in RStudio is invaluable to new and advanced users, alike.\n\nPro Tip: Instead of the help() function, you can use the ? function before an object name, e.g. ?install.packages.\n\n\nThe Help Pane\n\n\n\n\n\n\nThe invaluable Help pane displays documentation for packages, functions, and datasets.",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Studio</span>"
    ]
  },
  {
    "objectID": "0_20-rstudio.html#the-environment-and-history-panes",
    "href": "0_20-rstudio.html#the-environment-and-history-panes",
    "title": "2  R Studio",
    "section": "2.8 The Environment and History Panes",
    "text": "2.8 The Environment and History Panes\nThe Environment and History panes display the objects in your environment and the history of your in-console commands.\n\n\n2.8.1 The Environment Pane\nAgain, R is both a language and an environment. The Environment pane displays objects that are stored within your session’s workspace, or global environment, which must be recreated or reloaded with each new session. Note the following options:\n\nEnvironment: Opens a dropdown menu to select different environments, e.g. package environments\nLoad Workspace: Opens a file explorer to load previously saved workspaces and their objects\nImport Datasets: Opens a dropdown menu to read in datasets that you can store in objects\nClear Objects from Workspace: Removes all objects stored in the global environment\n\n\n\nThe Environment Pane\n\n\n\n\n\n\nThe environment pane displays any objects you’ve imported, loaded, or stored in your global environment.\n\n\n\n\n\n\n\n2.8.2 The History Pane\nThe History pane documents every command you’ve executed in your session. When you select a line, you can paste it directly into the console pane with “To Console” or directly into the source pane with “To Source”.\n\n\nThe History Pane\n\n\n\n\n\n\nThe history pane records every command you’ve run in your session.",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Studio</span>"
    ]
  },
  {
    "objectID": "0_20-rstudio.html#customizing-your-pane-layout",
    "href": "0_20-rstudio.html#customizing-your-pane-layout",
    "title": "2  R Studio",
    "section": "2.9 Customizing Your Pane Layout",
    "text": "2.9 Customizing Your Pane Layout\nIn RStudio, you can customize both where panes are displayed as well as which panes to show by default.\n\n\n2.9.1 Layouts for Beginners: Taking Great Panes\nPanes cannot be removed entirely from the RStudio interface, you but can shuffle them by order of importance. Click the “Tools” dropdown in themenu, “Global Options…”, and “Pane Layout”. We recommend focusing on those discussed in this chapter.\n\n\nCustomizing Pane Layout\n\n\n\n\n\n\nYou can customize which panes appear, and where, in Global Options’ Pane Layout.\n\n\n\n\n\n\n\n2.9.2 Less Important Panes, or Panes in the Rear\nThere are a couple of panes worth mentioning for new users. However, they are seldom used by advanced users:\n\nFiles: Set working directories and create, copy, rename, and delete folders\nPackages: Install, load, update, unload, and uninstall added (“User Library”) and built-in (“System Library”) packages\n\n\nPro Tip: You probably won’t use these panes often. One of the benefits of scripted languages is that they can be reproduced by other users. As a rule, since much of your work will require loading packages, you should include the library() function with script-dependent packages at the start of every work. The same applies to working directories with the setwd() function.\n\n\n\n2.9.3 Expanding to Fullscreen: Focus on the Pane\nIf you want to expand a pane, or “zoom”, to fullscreen mode, select “View” in the RStudio menu and “Panes”.\n\n\nToggling Fullscreen Panes\n\n\n\n\n\n\nYou can expand any pane to fullscreen mode in the “View” dropdown of the RStudio menu.",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Studio</span>"
    ]
  },
  {
    "objectID": "0_20-rstudio.html#the-rstudio-menu-file-session-help",
    "href": "0_20-rstudio.html#the-rstudio-menu-file-session-help",
    "title": "2  R Studio",
    "section": "2.10 The RStudio Menu: File, Session, & Help",
    "text": "2.10 The RStudio Menu: File, Session, & Help\nThe RStudio menu allows you to do virtually everythign we’ve seen in each pane and more. The following tours a few key menu sections we’ve not yet seen, including how to open new scripts, handle sessions, and access R-related cheat sheets.\n\n\n2.10.1 The File Submenu: Saving & Loading\nThe File submenu is the start of every scripted data product in RStudio. Just select “New File” and a litany of possible data products, about which we’ll learn more over time, are available to open. Remaining options relate to the saving and loading of scripts, projects, and datasets.\n\n\nThe File Submenu\n\n\n\n\n\n\n“File” handles all new, saving, and loading operations.\n\n\n\n\n\n\n\n2.10.2 The Session Submenu: Sessions, Directories, & Restarting R\nThe Session submenu is a critical part of any R session. While you can always recreate a session by recreating objects, you can save computing time by loading saved session file. This submenu also allows you to set your working directory.\n\nNote: There are times when you just have to restart R - maybe you started an infinite recursion loop or maybe you attempted to read in the data from a month’s worth of Harrier Jet landings to local memory - if so, the “Restart R” option is here for you.\n\n\nThe Session Submenu\n\n\n\n\n\n\nSave and load sessions, restart R, or handle directories in the session submenu.\n\n\n\n\n\n\n\n2.10.3 The Help Submenu: Cheat Sheets\nThe help menu has one main draw (for now): cheat sheets. Selecting a cheat sheet will automatically download a cheat sheet on an R-related topic of your choosing. Typically, cheat sheets summarize RStudio related packages and data products, but there is one for base R. For example, you can find the RStudio IDE cheat sheet here.\n\n\nThe Help Submenu\n\n\n\n\n\n\nGrab cheat sheets on the fly from the Help submenu.",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Studio</span>"
    ]
  },
  {
    "objectID": "0_20-rstudio.html#global-options-aesthetic-functional-preferences",
    "href": "0_20-rstudio.html#global-options-aesthetic-functional-preferences",
    "title": "2  R Studio",
    "section": "2.11 Global Options: Aesthetic & Functional Preferences",
    "text": "2.11 Global Options: Aesthetic & Functional Preferences\nGlobal options are accessed in the “Tools” submenu and allow users to modify their RStudio interface in myriad ways, both aesthetically and functionally. We recommend new users experiment with these options and visit a few notable modifications.\n\n\nAccessing Global Options\n\n\n\n\n\n\nAccess “Global Options” in the “Tools” submenu.\n\n\n\n\n\n\n2.11.1 Code: Autoformatting & Behavior\nThe “Code” section affects the way R automatically formats your code and how you choose to write and run it. It has sensible defaults, many of which you may not be prepared to appreciate quite yet. For now, consider the following:\n\nIndentating: In the “Editing” tab, consider a “tab width” that works best for you. When indenting, would you prefer two characters (i.e. spaces), or four? The former allows more compact code. The latter allows for more intepretable code.\nGuide Margin: In the “Display” tab, consider applying a “margin column” of 80 or 100 characters (i.e. spaces). This creates a subtle guide in your scripts that helps keep code concise and readable. Even basic code within basic code can create, what RStudio’s Chief Scientist Hadley Wickham refers to as, “Dagwood sandwich” code.\n\n\n\n\n2.11.2 Appearance: Express Yourself Intepretably\nThe Appearance section allow you to customize the size, font, and color of your text as well as the “theme” colors of your RStudio interface. Here, “theme” is both functional and aesthetic. For example, darker themes are more conducive to night owls. For all themes, certain syntax uses different colors to make your code more interpretable - keep this in mind for each theme!\n\n\nNotable Global Options: Code & Appearance\n\n\n\n\n\n\nGlobal options allow you to set preferences that can significantly impact your experience with RStudio.",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Studio</span>"
    ]
  },
  {
    "objectID": "0_20-rstudio.html#further-resources",
    "href": "0_20-rstudio.html#further-resources",
    "title": "2  R Studio",
    "section": "2.12 Further Resources",
    "text": "2.12 Further Resources\nThe following resources are helpful in learning more about RStudio and coding conventions:\n\nI) Full Introductions to RStudio\n\n“What are R and RStudio?” (Ismay & Kim, 2019)\n“Intro to R: Nuts & Bolts” (Crawford, 2018)\n\nII) About RStudio\n\nRStudio Homepage (RStudio, 2019)\nRStudio About Page (RStudio, 2019)\nRStudio Product Page (RStudio, 2019)\n\nIII) Conventions\n\n“The State of Naming Conventions in R” (Baath, 2012)",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Studio</span>"
    ]
  },
  {
    "objectID": "0_30-data-driven-docs.html",
    "href": "0_30-data-driven-docs.html",
    "title": "3  Data-Driven Docs",
    "section": "",
    "text": "3.1 What Are Data-Driven Docs?\nData-driven documents are formats that combine text and analysis (data+code).\nIn doing so, they promote transparency and reproducability. For any given table, figure, or model in the document you should be able to easily discern how it was created, from what data, and what analysis was used.\nPopular formats include things like R Markdown documents and Jupyter notebooks.",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data-Driven Docs</span>"
    ]
  },
  {
    "objectID": "0_30-data-driven-docs.html#how-do-data-driven-docs-work",
    "href": "0_30-data-driven-docs.html#how-do-data-driven-docs-work",
    "title": "3  Data-Driven Docs",
    "section": "3.2 How Do Data-Driven Docs Work?",
    "text": "3.2 How Do Data-Driven Docs Work?\nAll of the document formats build from a simple text formatting convention called markdown.\nTo create an R Markdown document, you need three things:\n\nA header to specify the document type.\n\nSome text (formatted in markdown).\n\nSome code (inside a “code chunk”).\n\nYou can download a sample template HERE.",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data-Driven Docs</span>"
    ]
  },
  {
    "objectID": "0_30-data-driven-docs.html#what-is-markdown",
    "href": "0_30-data-driven-docs.html#what-is-markdown",
    "title": "3  Data-Driven Docs",
    "section": "3.3 What is Markdown?",
    "text": "3.3 What is Markdown?\nMarkdown is a simple set of rules used to format text. It has been adopted broadly by the data science community and is used on GitHub, Stackoverflow, and now in R Studio.\nTo give just a couple of examples of how it works:\nUnordered Lists\n* First item\n* Second item\n* Third item\n    * First nested item\n    * Second nested item\n\nFirst item\nSecond item\nThird item\n\nFirst nested item\nSecond nested item\n\n\nHyperlinks\nCreate links by wrapping the link text in square brackets [ ], and the URL in adjacent parentheses ( ).\nI find that [Google News](https://news.google.com) over-curates my media diet.\nI find that Google News over-curates my media diet.\n\nTables\n| Title 1          | Title 2          | \n|------------------|------------------|\n| First entry      | Second entry     |  \n| Third entry      | Fourth entry     |  \n| Fifth entry      | Sixth entry      |\n\n\n\nTitle 1\nTitle 2\n\n\n\n\nFirst entry\nSecond entry\n\n\nThird entry\nFourth entry\n\n\nFifth entry\nSixth entry\n\n\n\nYou can see a full list of markdown rules HERE.",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data-Driven Docs</span>"
    ]
  },
  {
    "objectID": "0_30-data-driven-docs.html#knitting-r-markdown-files",
    "href": "0_30-data-driven-docs.html#knitting-r-markdown-files",
    "title": "3  Data-Driven Docs",
    "section": "3.4 Knitting R Markdown Files",
    "text": "3.4 Knitting R Markdown Files\nCode is placed inside of “chunks” in the documents:\n\nWhen you “knit” a file R Studio will run all of code, embed the output into your document, and then convert the file to whichever type you have specified in the file header.",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data-Driven Docs</span>"
    ]
  },
  {
    "objectID": "0_30-data-driven-docs.html#output-types",
    "href": "0_30-data-driven-docs.html#output-types",
    "title": "3  Data-Driven Docs",
    "section": "3.5 Output Types",
    "text": "3.5 Output Types\nYou can select from many different document types, including HTML pages, Microsoft word, presentation formats, or dashboards.\n\n  \nCheck out these examples:\nR Markdown Formats\nR Markdown Gallery\n\n\n\n\n3.5.1 HTML Pages\n---\noutput: html_document\n---\n\n\n\n\n3.5.2 Dashboards\n---\noutput: flexdashboard::flex_dashboard:\n---\n\n[ dashboard example ] [ source code ] [ blog about the tracker ]\n\n\n\n3.5.3 PDFs\n---\noutput: pdf_document\n---",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data-Driven Docs</span>"
    ]
  },
  {
    "objectID": "0_30-data-driven-docs.html#installation",
    "href": "0_30-data-driven-docs.html#installation",
    "title": "3  Data-Driven Docs",
    "section": "3.6 Installation",
    "text": "3.6 Installation\nYou will need the following programs to generate data-driven documents in R:\n\nBase R installation CRAN\nR Studio download page\nPandoc (comes with R Studio by default)\n\nWhen you first try to knit a file, you might get a message that you need the following packages:\n\nrmarkdown\n\nknitr\n\nThese can be installed in the usual manner:\ninstall.packages( \"rmarkdown\" )\ninstall.packages( \"knitr\" )\nPDFs:\nIf you would like to knit to PDF you need one additional program. TeX creates publication-quality PDF files. The open-source version is called MiKTeX download page.\nIf you have problems, you can find some nice tutorials like this one: https://www.reed.edu/data-at-reed/software/R/r_studio_pc.html\nSpecialized packages:\nSome document output formats require specific R packages. For example:\n\njournal templates\n\ndashboards\n\nr websites\n\nbooks in bookdown\n\nYou can find many of these packages on the R Markdown templates page.",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data-Driven Docs</span>"
    ]
  },
  {
    "objectID": "0_31-markdown.html",
    "href": "0_31-markdown.html",
    "title": "4  Markdown Guide",
    "section": "",
    "text": "4.1 Headers",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown Guide</span>"
    ]
  },
  {
    "objectID": "0_31-markdown.html#headers",
    "href": "0_31-markdown.html#headers",
    "title": "4  Markdown Guide",
    "section": "",
    "text": "# Heading One (h1)\n\n## Heading Two (h2)\n\n### Heading Three (h3)\n\n#### Heading Four (h4)\n\n##### Heading Five (h5)\n\n###### Heading Six (h6)",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown Guide</span>"
    ]
  },
  {
    "objectID": "0_31-markdown.html#text-style",
    "href": "0_31-markdown.html#text-style",
    "title": "4  Markdown Guide",
    "section": "4.2 Text Style",
    "text": "4.2 Text Style\nWith Markdown, it is possible to emphasize words by making them *italicized*, using *astericks* or _underscores_, or making them **bold**, using **double astericks** or __double underscores__. \n\nOf course, you can combine those two formats, with both _**bold and italicized**_ text, using any combination of the above syntax. \n\nYou can also add a strikethrough to text using a ~~double tilde~~.\nWith Markdown, it is possible to emphasize words by making them italicized, using astericks or underscores, or making them bold, using double astericks or double underscores.\nOf course, you can combine those two formats, with both bold and italicized text, using any combination of the above syntax.\nYou can also add a strikethrough to text using a double tilde.",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown Guide</span>"
    ]
  },
  {
    "objectID": "0_31-markdown.html#lists",
    "href": "0_31-markdown.html#lists",
    "title": "4  Markdown Guide",
    "section": "4.3 Lists",
    "text": "4.3 Lists\n\n4.3.1 Unordered\n* First item\n* Second item\n* Third item\n    * First nested item\n    * Second nested item\n\nFirst item\nSecond item\nThird item\n\nFirst nested item\nSecond nested item\n\n\n\n\n4.3.2 Ordered\n1. First item\n2. Second item\n3. Third item\n    1. First nested item\n    2. Second nested item\n\nFirst item\nSecond item\nThird item\n\nFirst nested item\nSecond nested item",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown Guide</span>"
    ]
  },
  {
    "objectID": "0_31-markdown.html#hyperlinks",
    "href": "0_31-markdown.html#hyperlinks",
    "title": "4  Markdown Guide",
    "section": "4.4 Hyperlinks",
    "text": "4.4 Hyperlinks\nCreate links by wrapping the link text in square brackets [ ], and the URL in adjacent parentheses ( ).\n[Google News](https://news.google.com)\nGoogle News",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown Guide</span>"
    ]
  },
  {
    "objectID": "0_31-markdown.html#images",
    "href": "0_31-markdown.html#images",
    "title": "4  Markdown Guide",
    "section": "4.5 Images",
    "text": "4.5 Images\nInsert images in a similar way, but add an exclamation mark in front of square brackets ![ ], and the image file name goes in the parentheses ( ).\n![alt_text_here](image_file.png)\nThe alt text appears when the image cannot be located, or is read by devices for the blind when the mouse hovers over the image. It\nIt is common practice to place all of the image files in an “assets” or “images” folder to keep your directory tidy. You can reference files inside a folder using the folder name and the forward slash:\n![a flower](images/flower.jpg)\n\n\n\na flower\n\n\n\nOr you can link directly to an image online using the URL address of the image:\n![](https://mountainlodgesofnepal.com/wp-content/uploads/2021/09/N1.png)\n\n\nIf you want to include images you need to add the image file to the images folder in this repository. Web-friendly formats like PNG, JPEG, or GIF are preferred.\nYou can just drag and drop the images into the folder, and GitHub will automatically initiate an upload option. After the upload progress bar is complete, click on the green “Commit changes” icon to complete the upload process.\nYou will then reference images in text using markdown syntax (see the Markddown Guide above for more details):",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown Guide</span>"
    ]
  },
  {
    "objectID": "0_31-markdown.html#tables",
    "href": "0_31-markdown.html#tables",
    "title": "4  Markdown Guide",
    "section": "4.6 Tables",
    "text": "4.6 Tables\n| Title 1          | Title 2          | Title 3         | Title 4         |\n|------------------|------------------|-----------------|-----------------|\n| First entry      | Second entry     | Third entry     | Fourth entry    |\n| Fifth entry      | Sixth entry      | Seventh entry   | Eight entry     |\n| Ninth entry      | Tenth entry      | Eleventh entry  | Twelfth entry   |\n| Thirteenth entry | Fourteenth entry | Fifteenth entry | Sixteenth entry |\n\n\n\n\n\n\n\n\n\n\nTitle 1\nTitle 2\nTitle 3\nTitle 4\n\n\n\n\nFirst entry\nSecond entry\nThird entry\nFourth entry\n\n\nFifth entry\nSixth entry\nSeventh entry\nEight entry\n\n\nNinth entry\nTenth entry\nEleventh entry\nTwelfth entry\n\n\nThirteenth entry\nFourteenth entry\nFifteenth entry\nSixteenth entry",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown Guide</span>"
    ]
  },
  {
    "objectID": "0_31-markdown.html#videos",
    "href": "0_31-markdown.html#videos",
    "title": "4  Markdown Guide",
    "section": "4.7 Videos",
    "text": "4.7 Videos\nIf you would like to post a video:\n\nUpload the video to a video platform like YouTube, and insert an iframe of the video into your document.\n\nIn YouTube, you will find iframes by clicking on the Share option, and the “Embed” icon.\n\n\n\n\niframe\n\n\n\nPaste this text into your markdown document, and it will add the video to your page:\n\n&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/jlYKZmTvzi0?si=sjSbc6CCBmwNm_td\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen&gt;&lt;/iframe&gt;",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown Guide</span>"
    ]
  },
  {
    "objectID": "0_31-markdown.html#blockquotes",
    "href": "0_31-markdown.html#blockquotes",
    "title": "4  Markdown Guide",
    "section": "4.8 Blockquotes",
    "text": "4.8 Blockquotes\n\n4.8.1 Single line\n&gt; My mom always said life was like a box of chocolates. You never know what you're gonna get.\n\nMy mom always said life was like a box of chocolates. You never know what you’re gonna get.\n\n\n\n4.8.2 Multiline\n&gt; What do you get when you cross an insomniac, an agnostic and a dyslexic?\n&gt;\n&gt; Someone who stays up all night wondering whether there really is a dog.\n&gt;\n&gt; – _Hal Incandenza_\n\nWhat do you get when you cross an insomniac, an agnostic and a dyslexic?\nSomeone who stays up all night wondering whether there really is a dog.\n– Hal Incandenza",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown Guide</span>"
    ]
  },
  {
    "objectID": "0_31-markdown.html#horizontal-rule",
    "href": "0_31-markdown.html#horizontal-rule",
    "title": "4  Markdown Guide",
    "section": "4.9 Horizontal Rule",
    "text": "4.9 Horizontal Rule\n---",
    "crumbs": [
      "Your Data Science Toolkit",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Markdown Guide</span>"
    ]
  },
  {
    "objectID": "1_00-learning-r.html",
    "href": "1_00-learning-r.html",
    "title": "5  Learning to Program",
    "section": "",
    "text": "5.1 Key Concepts\nLearning to Learn",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Learning to Program</span>"
    ]
  },
  {
    "objectID": "1_00-learning-r.html#key-concepts",
    "href": "1_00-learning-r.html#key-concepts",
    "title": "5  Learning to Program",
    "section": "",
    "text": "As you begin learning a new programming language you need to master nouns and verbs.\nNouns represent the jargon specific to the field (objects, arguments, assignment, etc.).\nVerbs represent functions.\nFluency takes time! Find ways to practice “speaking R” productively.\n\n\n\nR has thousands of packages and new ones are created daily. There is no way to learn everything about the language.\nThe goal, rather, is figuring out how to quickly master new functions so you can leverage R for whatever problem you are working on.\nThis requires learning to read help files and adapt code, and learning how to get help when you are stuck.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Learning to Program</span>"
    ]
  },
  {
    "objectID": "1_00-learning-r.html#learning-how-to-learn",
    "href": "1_00-learning-r.html#learning-how-to-learn",
    "title": "5  Learning to Program",
    "section": "5.2 Learning How to Learn",
    "text": "5.2 Learning How to Learn\n\nLet’s begin this chapter by stating and unpacking a few assertions:\n\nLearning R is very similar to learning a spoken language.\nFluency in a programming language is different than fluency in a spoken language.\nThe ultimate end to learning R is to learn how to learn so you can quickly master whatever packages are necessary for your current project.\n\n\n\n\n5.2.1 Similarities Between Human & Machine Languages\nThe similarities between human languages (e.g. English, Arabic, Mandarin) and machine languages (e.g. R, Python, JavaScript) are astounding, but a nuanced discussion as to why is beyond the scope of this introduction.\nAs examples, however, both human and machine languages:\n\nPersist or disappear depending on the size of communities that use them\nDerive from older languages and have evolved into new languages\nDiffer semantically but are arranged with similar grammatical elements (e.g. nouns, verbs)\n\n\nWhy is this important? You learn R the same way you would a foreign language:\n\nVocabulary: You need to learn words for things before all else.\nReinforcement: You learn by doing and repeating.\nImmersion: You can’t be fluent without being immersed. Get involved in professional and volunteer work, podcasts, and regular reading about your topic.\nNecessity: You learn best when success or survival depends on it. Use R in projects that have deadlines as a commitment mechanism.\n\nLanguages are Social: You learn faster through collaboration, meetups, forums, conferences.\n\nIf this is your first programming language, you will get frustrated at times. Take a step back and remember that after a semester of Spanish you can only operate at the level of a three-year old. You only know one verb tense, a few dozen verbs, and several hundred words. You have so many emotions that you can’t express in your new language!\nSimilarly, you are going to have so many ideas that you want to to translate into code! More realistically you will end up melting down in office hours and then taking a nap.\n\n\nFun Fact: The popular ggplot2 data viz package is based on Leland Wilkinson’s framework, “The Grammar of Graphics” (1999), i.e. the gg in ggplot2. Similarly, the dplyr package is described as a “grammar of data manipulation”, in which functions are described as “verbs”.\n\n\n\n\n5.2.2 You Can’t Learn Everything in R (Nor Should You)\nThere are thousands of R packages. The body of knowledge in the field is growing exponentially. It’s impossible (and unnecessary) to learn (or retain) everything the language offers.\nData science is a fast-moving field of practice.\n\nNew R packages are released every day.\nPackages and functions are constantly being updated, replaced, consolidated, or deprecated.\nThere are nuances to every computational science applied to each natural or social science.\n\nIn sum, to remain current in R, one must remain current with… everything.\nAccepting these limitations is liberating. It allows one to focus on an ab initio, or “first principles” approach to learning R. That is, build solid foundations, then specialize as necessary.\n\n\n\n\n“I am always doing that which I cannot do, in order that I may learn how to do it.” ~ Pablo Picasso\n\n\n\n\nNailed it\n\n\n\n\n\n\n\n5.2.3 The Ultimate Objective: Learning How to Learn R\nThere are few telltale signs that you’re becoming fluent in R. For example, you may:\n\nAchieve the same results as your old code with significantly less code\nUse search engines less frequently or not at all while coding\nBe approached by others for collaborative work\nAppraise the quality of code and data products\nBegin dreaming in R (just kidding, not kidding…)\n\nThe goal of “learning R” is just learning how to learn R. This means knowing when to step on the gas and try to power through a problem, and knowing when to hit the brake in order to take a step back, assess, and make a plan. Knowing when and how to ask for help is a basic level of situational awareness you need to develop.\nFor example, imagine you’ve reached some impasse that the “first principles” approach to R fails to resolve. Do you:\n\nPore over online forums?\nExplore documentation and vignettes?\nSearch for packages with potential solutions?\nBoldly install and learn said package(s) without hesitation?\nRecover when fail to resolve the issue? Fail and recover again?\nRemain firm in your conviction that you will resolve the issue?\n\n\n\n\n\n\n\n\n\n\n\n\nThe struggle is real. Source: XKCD\n\n\nThis is only one of many hypothetical scenarios. Learning how to learn R is not a matter of learning something new everyday (e.g. a function, package, technique, or theory), but the cyclical process of learning in the face of necessity, shoring up your knowledge over some indefinite period, and learning again when the need arises. Above all, you’ll know you’ve learned how to learn R when you no longer doubt your ability to resolve what once seemed impossible.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Learning to Program</span>"
    ]
  },
  {
    "objectID": "1_00-learning-r.html#you-are-not-alone-in-your-quest",
    "href": "1_00-learning-r.html#you-are-not-alone-in-your-quest",
    "title": "5  Learning to Program",
    "section": "5.3 You are not alone in your quest",
    "text": "5.3 You are not alone in your quest\n\nWhen working on labs make sure you have a game plan for success. If you are stuck for more than 20 minutes on the same problem without making progress you should (1) phone a friend, (2) post to the Get Help discussion board on the course site, (3) attend a review session, or (4) schedule virtual office hours.\nYour morale is a limited commodity, so spend it wisely! If you are doing the wrong thing over and over while stuck you might actually be reinforcing bad habits.\nThe qualifier without making progress is a relative term. You will spend more than 20 minutes on a problem without arriving at a solution. Stuck means you don’t understand the question or you are out of things to try.\nTechnically, for 3-credit class you should be spending about 12 hours per week on the class (3 hours of class time, 9 hours on homework/projects). If it’s a condensed semester (7.5 week schedule) that translates to 24 hours per week.\nThis will vary for everyone depending on background knowledge, speed, and interest! But it’s a good rule of thumb to keep in mind, and a good reminder that you are not slow or incapable - there is a reason that they call education “investment in human capital”!",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Learning to Program</span>"
    ]
  },
  {
    "objectID": "1_00-learning-r.html#troubleshooting-in-r",
    "href": "1_00-learning-r.html#troubleshooting-in-r",
    "title": "5  Learning to Program",
    "section": "5.4 Troubleshooting in R",
    "text": "5.4 Troubleshooting in R\nYour code is going to fail. A lot.\n\nWe encounter errors all the time. The following is a brief overview for recognizing a problem, as well as myriad strategies to overcome one.\n\n\n5.4.1 Recognizing & Understanding Trouble\n\n“The first step in fixing a broken program is getting it to fail repeatably.” (Tom Duff)\n\nTwo simple criteria must be met in order to recognize trouble with your code (Peng et al. 2017):\n\nYou expect your code to behave in a certain way\nYour code doesn’t behave as expected\n\nOftentimes, the best case scenario is that R will throw an error message. Error messages signal that R has encountered some fatal flaw in your code and, in consequence, nothing happens.\nThe next best scenario is that R will throw a warning message. Warning messages signal that R has successfully evaluated and executed code, but something unexpected happened, and R wants you to know about it.\nThe worst and most pernicious scenario is when your code runs perfectly fine, without error or warning messages, yet the output isn’t exactly what you’d wanted. If you recognize this, that’s great. If you don’t recognize this, errors flow downstream and can be difficult to detect and fix. Always inspect your output.\n\n\nPro Tip: Oftentimes, users place too much emphasis on fixing their expected results and fail to consider their original expectations. Never neglect revisiting your expectations, as it’s possible that they, not your code, may explain an error (ibid.).",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Learning to Program</span>"
    ]
  },
  {
    "objectID": "1_00-learning-r.html#further-resources",
    "href": "1_00-learning-r.html#further-resources",
    "title": "5  Learning to Program",
    "section": "6.1 Further Resources",
    "text": "6.1 Further Resources\nThe following resources may prove valuable for both learning how to learn R and troubleshooting:\nI) Full-Length Text\n\n“Intro to R: Nuts & Bolts” (Crawford, 2018)\n\nII) Documentation & Vignettes\n\n“Vignettes: Long-Form Documentation” (Wickham, 2019)\n\nIII) Forums, Email Lists, & Communities\n\nStack Overflow\nCross Validated\nR Mailing Lists\nReddit’s r/Rstats\nTwitter’s #Rstats\n\nIV) Asking R-Related Questions\n\n“Hot to Ask Questions the Smart Way” (Raymond, 2014)\n“How to Make a Great R Reproducible Example” (Stack Overflow, 2018)\n“Posting Guide: Hot to Ask Good Questions that Prompt Useful Answers” (Plate, 2013)\nErrors vs Warnings (Rex Analytics)\n\nV) ASCII Dragons\n\ndanadaldoss\nDoug\nAhmad",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Learning to Program</span>"
    ]
  },
  {
    "objectID": "1_10-getting-help.html",
    "href": "1_10-getting-help.html",
    "title": "5  Getting Help",
    "section": "",
    "text": "5.1 Key Concepts\nYou will get stuck while learning to code.\nR has lots of resources for problem-shooting. You should familiarize yourself with the following:",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting Help</span>"
    ]
  },
  {
    "objectID": "1_10-getting-help.html#key-concepts",
    "href": "1_10-getting-help.html#key-concepts",
    "title": "5  Getting Help",
    "section": "",
    "text": "Error messages\nDocumentation\nDiscussion boards\nVignettes",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting Help</span>"
    ]
  },
  {
    "objectID": "1_10-getting-help.html#here-there-be-ascii-dragons",
    "href": "1_10-getting-help.html#here-there-be-ascii-dragons",
    "title": "5  Getting Help",
    "section": "5.2 Here There be ASCII Dragons",
    "text": "5.2 Here There be ASCII Dragons",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting Help</span>"
    ]
  },
  {
    "objectID": "1_10-getting-help.html#help",
    "href": "1_10-getting-help.html#help",
    "title": "5  Getting Help",
    "section": "5.2 HELP!",
    "text": "5.2 HELP!\nYour code is going to fail. A lot. Bugs and broken dreams code are a normal part of any programming task. They never go away, but you get better at diagnosing and fixed them!\n\n\nHere There be ASCII Dragons\n\nThe following is a brief taxonomy of errors in R, and some strategies to overcome them.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting Help</span>"
    ]
  },
  {
    "objectID": "1_10-getting-help.html#what-does-it-mean-for-code-to-fail",
    "href": "1_10-getting-help.html#what-does-it-mean-for-code-to-fail",
    "title": "5  Getting Help",
    "section": "5.3 What Does It Mean for Code to Fail?",
    "text": "5.3 What Does It Mean for Code to Fail?\nTwo simple criteria must be met in order to recognize trouble with your code (Peng et al. 2017):\n\nYou expect your code to behave in a certain way\nYour code doesn’t behave as expected\n\nOftentimes, the best case scenario is that R will throw an error message. Error messages signal that R has encountered some fatal flaw in your code and, in consequence, nothing happens.\nThe next best scenario is that R will throw a warning message. Warning messages signal that R has successfully evaluated and executed code, but something unexpected happened, and R wants you to know about it.\n\n\nSource: Rex Analytics\n\n\nThe worst and most pernicious scenario is when your code runs perfectly fine, without error or warning messages, yet the output isn’t exactly what you’d wanted. If you recognize this, that’s great. If you don’t recognize this, errors flow downstream and can be difficult to detect and fix. Always inspect your output.\nOftentimes, users place too much emphasis on fixing their expected results and fail to consider their original expectations. Never neglect revisiting your expectations, as it’s possible that they, not your code, may explain an error (Peng et al. 2017).",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting Help</span>"
    ]
  },
  {
    "objectID": "1_10-getting-help.html#documentation",
    "href": "1_10-getting-help.html#documentation",
    "title": "5  Getting Help",
    "section": "5.4 Documentation",
    "text": "5.4 Documentation\nWhen you’re in a jam, the foremost strategy for resolving said jam is to read any relevant documentation.\n\n5.4.1 Built-In Documentation\nAll base R functions, as well as packages, functions, and datasets downloaded from CRAN (Comprehensive R Archie Network) contain documentation that can be accessed via the help() or ? functions.\nWhen called, the Help pane displays built-in documentation, which typically contains names, descriptions, templates, arguments, values, references, related objects, and examples.\nObserve the following calls to help() on function median():\n\nhelp(median)\n?median\n\n\n\n\n\n\nBuilt-in documentation is displayed with function help() and an object name.\n\n\n\n\n\n\n\n5.4.2 External Documentation\nDatasets found “in the wild” often have documentation containing metadata, variable definitions, and other helpful information, while many functions have external documentation outside of CRAN.\n\n\nDocumentation for RStudio’s “dplyr” Package\n\n\n\n\n\n\nMore stylized documentation on package dplyr from the Tidyverse website.\n\n\n\n\n\nDocumentation for the Pheonix Fire Department Data\n\n\n\n\n\n\nDocumentation on operational statistics data on the City of Phoenix Open Data portal.\n\n\n\n\n\n\n\n\n5.4.3 Vignettes\n\n“A vignette is like a book chapter or an academic paper: It can describe the problem that your package is designed to solve, and then show the reader how to solve it.” (Hadley Wickham)\n\nSome R packages have vignettes, or more comprehensive and thorough overviews than typical documentation. Vignettes are written for the express purpose of learning how to use a package. While vignettes are easy to find in a conventional search engine, you can open a vignette from within R by calling function browseVignettes() and a quotes package name. For example:\n\nbrowseVignettes(\"tidyr\")\n\n\n\n\n\n\nAvailable vignettes appear in your browser with browseVignettes()",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting Help</span>"
    ]
  },
  {
    "objectID": "1_10-getting-help.html#troubleshooting-web-searches-forums",
    "href": "1_10-getting-help.html#troubleshooting-web-searches-forums",
    "title": "5  Getting Help",
    "section": "6.1 Troubleshooting: Web Searches & Forums",
    "text": "6.1 Troubleshooting: Web Searches & Forums\nWhile documentation tends to require a bit more reading, there’s a good chance that someone has run into the same problem you’ve encounters. The internet has a long memory and the following strategies show how to exploit it.\n\n6.1.1 Quoting Your Console\nYou can take advantage of error and warning messages by copying them from your console and pasting them directly into an online search engine, verbatim and quoted. It often helps to include the function or package name outside of the quoted console message.\n\n\nQuoting Error Messages in Search Engines\n\n\n\n\n\n\nQuoting your console’s error message in a search engine often yields helpful results.\n\n\n\n\n\n\n6.1.2 Forums & Archives\nOne of the best troubleshooting strategies is to search for answers in online forums like Stack Overflow or Cross Validated. Archives like the “R-help” mailing list are also searchable.\n\n\nPotential Solutions as Seen on Stack Overflow\n\n\n\n\n\n\nStack Overflow often provides multiple potential solutions. The best rise to the top.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting Help</span>"
    ]
  },
  {
    "objectID": "1_10-getting-help.html#consulting-humans",
    "href": "1_10-getting-help.html#consulting-humans",
    "title": "5  Getting Help",
    "section": "7.1 Consulting Humans",
    "text": "7.1 Consulting Humans\nAsking for help from another human should always be your last recourse after exhausted all of the above strategies. That said, if you’re still in a jam, there are several sources for human help:\n\nMailing lists like “R-help” and forums like Stack Overflow\nMessage boards in universities and MOOCs (massive open online classes)\nR user communities, e.g. Reddit’s r/Rstats\nEmail and in-person inquiries\n\n\n\n\n\n\n\nThere is an incredible amount of good will in the R community, but try to be respectful.\n\n\n\n\n\nImportantly, there are established best practices in asking R-related questions. Make sure to:\n\nComply with any and all forum, community, and mailing list rules\nIn email headers or topic threads, label with a succinct but detailed description\nClearly state the expected behavior and unexpected result of your code\nProvide a reproducible example, including code and imitation data\nInclude real or imitation output, e.g. error messages\nIf using random number generation, include the seed\nEnsure your code is legible and organized\nInclude your R and RStudio versions\nInclude your operating system\nBe polite\n\n\n\nPro Tip: While not the best for specific coding problems, consider “tweeting” a question that includes the hashtag “#rstats”. Followers of #rstats often respond quickly, accurately, and sometimes in great numbers.\n\n\n\n7.1.1 A Brief Note on Language Precision\nWhether your poring over documentation, performing web, forum, and archive searches, or asking another human, language precision is key to troubleshooting. Hence, building your R vocabulary is critical to learning how to learn R. For example, instead of searching: “How do I combine two tables that share a column?”, search “join data frames on key variable r”, or something similar. The latter term, with precise R-specific jargon, is much more quick to fructify.",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting Help</span>"
    ]
  },
  {
    "objectID": "1_10-getting-help.html#further-resources",
    "href": "1_10-getting-help.html#further-resources",
    "title": "5  Getting Help",
    "section": "7.2 Further Resources",
    "text": "7.2 Further Resources\nThe following resources may prove valuable for both learning how to learn R and troubleshooting:\nI) Full-Length Text\n\n“Intro to R: Nuts & Bolts” (Crawford, 2018)\n\nII) Documentation & Vignettes\n\n“Vignettes: Long-Form Documentation” (Wickham, 2019)\n\nIII) Forums, Email Lists, & Communities\n\nStack Overflow\nCross Validated\nR Mailing Lists\nReddit’s r/Rstats\nTwitter’s #Rstats\n\nIV) Asking R-Related Questions\n\n“Hot to Ask Questions the Smart Way” (Raymond, 2014)\n“How to Make a Great R Reproducible Example” (Stack Overflow, 2018)\n“Posting Guide: Hot to Ask Good Questions that Prompt Useful Answers” (Plate, 2013)\nErrors vs Warnings (Rex Analytics)\n\nV) More ASCII Dragons\n\ndanadaldoss\nDoug\nAhmad",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting Help</span>"
    ]
  },
  {
    "objectID": "2_10-calculator.html",
    "href": "2_10-calculator.html",
    "title": "6  R: An Overdesigned Calculator",
    "section": "",
    "text": "6.1 Introduction\nChief among R’s many capabilities are basic mathematical operations, making it a severely overdesigned calculator. The only thing a TI-84 has on R is that you can’t play Super Mario Brothers with the latter - yet (package developers… if your listening…).\nSo what?\nArithmetic operations in R are key to transforming your data, whether calculating property code violations per capita or converting U.S. dollars to Indian rupees. In short, you use them to make new variables from existing ones. When we combine arithmetic with objects that store one or more values, we’re dangerously close to practicing algebra.",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R: An Overdesigned Calculator</span>"
    ]
  },
  {
    "objectID": "2_10-calculator.html#key-concepts",
    "href": "2_10-calculator.html#key-concepts",
    "title": "6  R: An Overdesigned Calculator",
    "section": "6.2 Key Concepts",
    "text": "6.2 Key Concepts\nIn this chapter, we’ll learn and practice basic arithmetic functions in R, assign calculations and their resulting values to objects, and use those objects in algebraic operations. Key concepts include:\n\nArithmetic Operators\nOperator Precedence\nAssignment",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R: An Overdesigned Calculator</span>"
    ]
  },
  {
    "objectID": "2_10-calculator.html#key-takeaways",
    "href": "2_10-calculator.html#key-takeaways",
    "title": "6  R: An Overdesigned Calculator",
    "section": "6.3 Key Takeaways",
    "text": "6.3 Key Takeaways\nEverything you need to know in a few bullet points:\n\nArithmetic operators include: +, -, *, /, ^, ( )\nCalculations follow the order of operations\nCreate objects with assignment:\n\nx stores the value 3 after calling x &lt;- 3\n\nNumeric bjects act like variables in algebra:\n\nx + 2 equals 5",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R: An Overdesigned Calculator</span>"
    ]
  },
  {
    "objectID": "2_10-calculator.html#arithmetic-operators",
    "href": "2_10-calculator.html#arithmetic-operators",
    "title": "6  R: An Overdesigned Calculator",
    "section": "6.4 Arithmetic Operators",
    "text": "6.4 Arithmetic Operators\n\n“What sort of free will is left when we come to tabulation and arithmetic? When it will all be a case of twice two makes four?\nYou don’t need free will to determine that twice two is four.” (Dostoyevsky)\n\n\nYou remember arithmetic, right? That peculiar field of mathematics in which people who admittedly “don’t math”, for whatever reason, actually use every day?\nArithmetic operators in R work just like they did in primary school, including addition, subtraction, multiplication, division, and exponentiation:\n\n+ or addition, e.g. 2 + 2\n- or subtraction, e.g. 2 - 2\n* or multiplication, e.g. 2 * 2\n/ or division, e.g. 2 / 2\n^ or exponentiation, e.g. 2 ^ 2\n( ) for order of operations, e.g. ((2 + 2) * 2)\n\n\nThe following example has a number of operations. Run the code to see what happens:\n\n7 + 3       # Addition\n\n8 - 12      # Subtraction\n\n9 * 9       # Multiplication\n\n10 / 3      # Division\n\n10 ^ 3      # Exponentiation\n\n\nEureka! Forget your mobile phone’s calculator app. Install R on it!\n(Plus, you can browse Reddit during class, but it looks like you’re working).\n\n\n6.4.1 Your Turn\n\nInstructions: Perform the following arithmetic operation in R.\nTip: Numeric values in R don’t use commas.\n\n\n# Raise 5 to the fifth power\n\n# Subtract 30 from 100\n\n# Divide 1,000 by 300\n\n\n\n\n\n\n\nArithmetic operators are the most atomic functions in R programming.\n\n\n\n\n\nSource: XKCD",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R: An Overdesigned Calculator</span>"
    ]
  },
  {
    "objectID": "2_10-calculator.html#order-of-operations",
    "href": "2_10-calculator.html#order-of-operations",
    "title": "6  R: An Overdesigned Calculator",
    "section": "6.5 Order of Operations",
    "text": "6.5 Order of Operations\nHere’s another blast from the past: the operator precedence. At least, that’s what it’s called in programming languages. You probably remember it as the order of operations.\nIf you don’t recall the specific rules, perhaps you remember the mnemonic devices: PEMDAS or Please Excuse My Dear Aunt Sally. We are not sure know what Aunt Sally did, but we are pretty sure she deserves whatever punshinment she received.\nPEMDAS reminds us the order arithmetic operations are evaluated, a.k.a. the order of operations:\n\nParenthesis, or expressions inside ( )\nExponents, or raising one value to the power of another with ^\nMultiplication, or multiplying values with *\nDivision, or dividing values with /\nAddition, or adding values with +\nSubtraction, or subtracting values with -\n\nArithmetic operations in R are also evaluated in the same order. Can you guess the results before evaluating the expressions? Press “Run” to execute the code and see the results:\n\n# easy one\n5 + 10 / 5    \n\n# harder\n5 - 10 + 5 \n5 + 10 - 5  \n\n# similarly\n5 * 10 / 5   \n5 / 10 * 5 \n\n# easier\n3 * 2 ^ 2\n\n# hmmm\n2 * (2 + 3) * 3\n\n# huh?\n( 2 + 3 ) / 5 * 2\n( 2 + 3 ) * 5 / 2\n( 2 + 3 ) / 2 * 5\n\n\n\nNote that R is indifferent to order of operations for addition vs subtraction, and multiplication vs division.\nFor cases where both occur the code is just executed from left to right.\n\n\n6.5.1 Your Turn\n\nInstructions:\nThe formula to calculate a monthly mortgage payment based upon the loan amount, annual interest rate, and loan term (in months) is calculated as follows:\n\\[\nPAYMENT = \\frac{principal \\cdot \\frac{interest \\ rate}{12}}{1-(1+\\frac{interest \\ rate}{12})^{- \\ months}}\n\\]\nLet’s say we have a $100,000 loan at a 5% interest rate amortized over 360 months (30 years). The payments would be as follows:\n\\[\nPAYMENT = \\frac{ 100k \\cdot \\frac{0.05}{12}}{1-(1+\\frac{0.05}{12})^{- \\ 360}}\n\\]\nCan you type the formula into R correctly? The payment should come to $536.82 a month.\n\n\n# Type your formula here",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R: An Overdesigned Calculator</span>"
    ]
  },
  {
    "objectID": "2_10-calculator.html#valid-object-names",
    "href": "2_10-calculator.html#valid-object-names",
    "title": "6  R: An Overdesigned Calculator",
    "section": "7.1 Valid Object Names",
    "text": "7.1 Valid Object Names\nWhen creating new objects there are both rules and conventions for naming them.\nThe rules are fairly simple:\n\nR is case sensitive, so b and B are different objects.\nObject names can include letters, periods and underscores.\nObject names can include numbers, but cannot begin with a number.\n\nx.01 &lt;- 99  # good\nx_01 &lt;- 99  # this works\n01.x &lt;- 99  # produces an error\n\n.x &lt;- 99    # this works\n.1x &lt;- 99   # this doesn't\n_x &lt;- 99    # oddly this doesn't\nIn general it is good to name objects so they are easy to remember. You can combine words using one of three conventions:\n\nCamel Caps\nUnderscores\nPeriods\n\nmyData  &lt;- 99  # camel caps\nmy_data &lt;- 99  # underscore\nmy.data &lt;- 99  # dot case\nSome people have strong views on these. You should find something that works for you and be consistent.",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R: An Overdesigned Calculator</span>"
    ]
  },
  {
    "objectID": "2_10-calculator.html#further-resources",
    "href": "2_10-calculator.html#further-resources",
    "title": "6  R: An Overdesigned Calculator",
    "section": "7.2 Further Resources",
    "text": "7.2 Further Resources\nThe following resources are helpful in learning more about arithmetic operators in R:\n\n“Arithmetic Operators” (CRAN)\n“Intro to R: Operators” (Crawford,\n\n\n\n“Quick-R: Operators” (Kabacoff, 2017)",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R: An Overdesigned Calculator</span>"
    ]
  },
  {
    "objectID": "2_10-calculator.html#works-cited",
    "href": "2_10-calculator.html#works-cited",
    "title": "6  R: An Overdesigned Calculator",
    "section": "7.3 Works Cited",
    "text": "7.3 Works Cited\nThe Hangover (2009) The Hunger Games (2010)",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R: An Overdesigned Calculator</span>"
    ]
  },
  {
    "objectID": "2_20-functions.html",
    "href": "2_20-functions.html",
    "title": "7  Introduction to Functions",
    "section": "",
    "text": "7.1 Key Concepts\nA function is a data recipe, and a building block of longer scripts.\nYou can think of functions as input-output machines that take raw data and transform it into useful statistics.\nThey accepts arguments (data or parameters), and return the requested calculation or transformation.\nFor example, the mean() function requires a vector of measurements as the input and return the average measure for the group as the output.",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Functions</span>"
    ]
  },
  {
    "objectID": "2_20-functions.html#key-concepts",
    "href": "2_20-functions.html#key-concepts",
    "title": "7  Introduction to Functions",
    "section": "",
    "text": "7.1.1 VOCABULARY:\n\nThe function function()\narguments\nargument values\ndefault values\nexplicit vs implicit argument calls\nreturn values\narrow vs. equal sign",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Functions</span>"
    ]
  },
  {
    "objectID": "2_20-functions.html#computer-programs-as-recipes",
    "href": "2_20-functions.html#computer-programs-as-recipes",
    "title": "7  Introduction to Functions",
    "section": "7.2 Computer Programs as Recipes",
    "text": "7.2 Computer Programs as Recipes\nComputer programs are powerful because they allow us to codify recipes for complex tasks, save them, share them, and build upon them.\nIn the simplest form, a computer program is like a recipe. We have inputs, steps, and outputs.\nIngredients:\n\n1/3 cup butter\n\n1/2 cup sugar\n\n1/4 cup brown sugar\n\n2 teaspoons vanilla extract\n\n1 large egg\n\n2 cups all-purpose flour\n\n1/2 teaspoon baking soda\n\n1/2 teaspoon kosher salt\n\n1 cup chocolate chips\n\nInstructions:\n\nPreheat the oven to 375 degrees F.\nIn a large bowl, mix butter with the sugars until well-combined.\nStir in vanilla and egg until incorporated.\nAddflour, baking soda, and salt.\n\nStir in chocolate chips.\nBake for 10 minutes.\n\nIn R, the recipe would look something like this:\nfunction( butter=0.33, sugar=0.5, eggs=1, flour=2, temp=375 )\n{\n   dry.goods &lt;- combine( flour, sugar )\n   batter &lt;- mix( dry.goods, butter, eggs )\n   cookies &lt;- bake( batter, temp, time=10 )\n   return( cookies )\n}\nNote that this function to make cookies relies on other functions for each step, combine(), mix(), and bake(). Each of these functions would have to be defined as well, or more likely someone else in the open source community has already written a package called “baking” that contains simple functions for cooking so that you can use them for more complicated recipes.\nYou will find that R allows you to conduct powerful analysis primarily because you can build on top of and extend a lot of existing functionality.\n\n\n\n\n\nAssignment of output values",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Functions</span>"
    ]
  },
  {
    "objectID": "2_20-functions.html#loan-calculator",
    "href": "2_20-functions.html#loan-calculator",
    "title": "7  Introduction to Functions",
    "section": "7.3 Loan Calculator",
    "text": "7.3 Loan Calculator\nLet’s look at a slightly more comlicated example by creating an amortization calculator to determine monthly payments that would be required from a home mortgage loan.\n\n\n\n\n\nExample monthly payment based upon loan amount (P), interest rate (R), and time period of repayment (T).\n\n\n\n\nWhere:\n\nT = time of loan period\nP = loan principal, or total amount borrowed\nR = annual interest rate, or annual percentage rate (APR)\n\nA mortgage calculator considers the total loan amount (the principal), the interest rate or APR (annual percentage rate), and the period of the loan in order to determine how much needs to be paid each month so that payments are distributed equally across the loan term. If we look up a formula, we will find:\n\nWe can simplify this formula a bit by putting everything in monthly periods:\n\\[\nPAYMENTS = \\frac{principal \\cdot interest \\ rate}{1-(1+interest \\ rate)^{- \\ months}}\n\\]\nWhere:\n\nmonths = years T x 12 months\ninterest rate (monthly) = annual interest rate R / 12 months\n\nWhen we translate this mathematical formula into R code, the new function will look like this:\n\n calcMortgage &lt;- function( principal, years, APR )\n {\n   \n    months &lt;- years * 12   # covert years to months\n    int.rate &lt;- APR / 12   # convert annual rate to monthly\n    \n    # amortization formula\n    monthly.payment &lt;- ( principal * int.rate ) / \n                       (1 - (1 + int.rate)^(-months) )\n    \n    monthly.payment &lt;- round( monthly.payment, 2 )\n    \n    return( monthly.payment )\n \n }\n\n\ndc_light_exercise_example-01\n\n\nWhat happens if you omit an argument from the function? Why?",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Functions</span>"
    ]
  },
  {
    "objectID": "2_20-functions.html#defaults",
    "href": "2_20-functions.html#defaults",
    "title": "7  Introduction to Functions",
    "section": "8.1 Defaults",
    "text": "8.1 Defaults\n\ndc_light_exercise_example-03\n\n\nCan you still use custom values for those arguments after defaults are set?",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Introduction to Functions</span>"
    ]
  },
  {
    "objectID": "2_30-assignment.html",
    "href": "2_30-assignment.html",
    "title": "8  Assignment",
    "section": "",
    "text": "8.1 Key Concepts\nAfter reading this chapter you should be able to define the following:",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Assignment</span>"
    ]
  },
  {
    "objectID": "2_30-assignment.html#key-concepts",
    "href": "2_30-assignment.html#key-concepts",
    "title": "8  Assignment",
    "section": "",
    "text": "operators\nobjects\nassignment",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Assignment</span>"
    ]
  },
  {
    "objectID": "2_30-assignment.html#assignment",
    "href": "2_30-assignment.html#assignment",
    "title": "8  Assignment",
    "section": "8.2 Assignment",
    "text": "8.2 Assignment\nWhen we call a function in R, the default behavior of the function is typically to print the results on the screen:\n\ncalcMortgage( principal=100000  )\n\n[1] 536.82\n\n\nIf we are creating a script, however, we often need to save the function outputs at each step. We can do this by assigning output to a new variable.\n\npayments.15.year &lt;- calcMortgage( years=15, principal=100000  )\npayments.30.year &lt;- calcMortgage( years=30, principal=100000  )\n\nThese values are then stored, and can be used later or printed by typing the object name:\n\npayments.15.year\n\n[1] 790.79\n\npayments.30.year\n\n[1] 536.82\n\n\nNote that variable names can include periods or underscores. They can also include numbers, but they cannot start with a number. Like everything in R, they will be case sensitive.",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Assignment</span>"
    ]
  },
  {
    "objectID": "3_10-data-types.html",
    "href": "3_10-data-types.html",
    "title": "9  Vectors and Data Types",
    "section": "",
    "text": "9.1 Key Concepts\nComponents of a Vector\nBasic data types in R",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors and Data Types</span>"
    ]
  },
  {
    "objectID": "3_10-data-types.html#common-vectors-functions",
    "href": "3_10-data-types.html#common-vectors-functions",
    "title": "9  Vectors and Data Types",
    "section": "11.1 Common Vectors Functions",
    "text": "11.1 Common Vectors Functions\nYou will spend a lot of time creating data vectors, transforming variables, generating subsets, cleaning data, and adding new observations. These are all accomplished through functions that act on vectors.\nHere are some common vector functions:\n\n11.1.1 vector length\nWe often need to know how many elements belong to a vector, which we find with the length() function.\n\nlength( strength )\n\n[1] 4\n\n\n\n\n11.1.2 combine\nTo combine several elements into a single vector, or combine two vectors to form one, use the c() function.\n\nc( 1, 2, 3 )        # create a numeric vector\n\n[1] 1 2 3\n\nc( \"a\", \"b\", \"c\" )  # create a character vector\n\n[1] \"a\" \"b\" \"c\"\n\n\nCombining two vectors:\n\nx &lt;- 1:5\ny &lt;- 10:15\nc( x, y )\n\n [1]  1  2  3  4  5 10 11 12 13 14 15\n\n\nCombining two vectors of different data types:\n\nx &lt;- c( 1, 2,3 )\ny &lt;- c( \"a\", \"b\", \"c\" )\nc( x, y )\n\n[1] \"1\" \"2\" \"3\" \"a\" \"b\" \"c\"\n\n\n\nWhat happened to the numeric elements here?",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors and Data Types</span>"
    ]
  },
  {
    "objectID": "3_10-data-types.html#casting",
    "href": "3_10-data-types.html#casting",
    "title": "9  Vectors and Data Types",
    "section": "11.2 Casting",
    "text": "11.2 Casting\nYou can easily move from one data type to another by casting a specific type as another type:\n\n# character casting\nx &lt;- 1:5\nx\n\n[1] 1 2 3 4 5\n\nas.character(x)  # numbers stored as text\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\n\nThe rules for casting vary by data type. Take logical vectors, for example. Re-casting them as character vectors produces an expected result. What about as a numeric vector?\n\ny &lt;- c( TRUE, FALSE, TRUE, TRUE, FALSE )\ny\n\n[1]  TRUE FALSE  TRUE  TRUE FALSE\n\nas.character( y )\n\n[1] \"TRUE\"  \"FALSE\" \"TRUE\"  \"TRUE\"  \"FALSE\"\n\nas.numeric( y )\n\n[1] 1 0 1 1 0\n\n\nIf you are familiar with boolean logic or dummy variables in statistics, it actually makes sense that TRUE would be represented as 1 in numeric form, and FALSE as 0.\nBut in some cases it might not make sense to cast one variable type as another and we can get unexpected or unwanted behavior.\n\nz &lt;- c( \"a\", \"b\", \"c\" )\nz\n\n[1] \"a\" \"b\" \"c\"\n\nas.numeric( z )\n\n[1] NA NA NA\n\n\n\nThe element NA is read as NOT AVAILABLE or NOT APPLICABLE, and is the value R uses to represent missing or deleted data.\nNA’s are really important (and somewhat annoying). We will discuss missing values more in-depth later.",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors and Data Types</span>"
    ]
  },
  {
    "objectID": "3_10-data-types.html#care-when-casting",
    "href": "3_10-data-types.html#care-when-casting",
    "title": "9  Vectors and Data Types",
    "section": "11.3 Care When Casting",
    "text": "11.3 Care When Casting\nCasting will often be induced automatically when you try to combine different types of data. For example, when you add a character element to a numeric vector, the whole vector will be cast as a character vector.\n\nx1 &lt;- 1:5\nx1\n\n[1] 1 2 3 4 5\n\nx1 &lt;- c( x1, \"a\" )   # a vector can only have one data type\nx1                   # all numbers silently recast as characters\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"a\"\n\n\nIf you consider the example above, when a numeric and character vector are combined all elements are re-cast as strings because numbers can be represented as characters but not vice-versa.\nR tries to select a reasonable default type, but sometimes casting will create some strange and unexpected behaviors. Consider some of these examples.\n\ndc_light_exercise_example-01\n\n\nWhich data type will each step produce? Type the case# to see the results.\n\n\nThe answers to case1 and case2 are somewhat intuitive.\n\ncase1  # combine a numeric and logical vector\n\n[1] 1 2 3 1 0 1\n\n\nRecall that TRUE and FALSE are often represented as 1 and 0 in datasets, so they can be recast as numeric elements. The numbers 2 and 3 have no meaning in a logical vector, so we can’t cast a numeric vector as a logical vector. This will default to numeric because we do not lose any information - the one’s and zero’s can always be re-cast back to logical vectors later if necessary.\n\ncase2  # combine a character and logical vector\n\n[1] \"a\"     \"b\"     \"c\"     \"TRUE\"  \"FALSE\" \"TRUE\" \n\n\nSimilarly characters have no meaning in the logical format, so we would have to replace them with NA’s if we converted the character vector to a logical vector.\n\nas.logical( x2 )\n\n[1] NA NA NA\n\n\nSo converting the logical vector to characters allows us to retain all of the information in both vectors.\ncase3 and case4 are a little more nuanced. See the section on factors below to make sense of them.\n\ncase3  # combine a numeric and factor vector\n\n[1] 1 2 3 1 2 3\n\ncase4  # combine a character and factor vector\n\n[1] \"a\" \"b\" \"c\" \"1\" \"2\" \"3\"\n\n\n\nTIP: When you read data in from outside sources, R will sometimes try to guess the data types and store numeric or character vectors as factors. To avoid corrupting your data see the section below on factors for special instructions on re-casting factors as numeric vectors.",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors and Data Types</span>"
    ]
  },
  {
    "objectID": "3_10-data-types.html#numeric-vectors",
    "href": "3_10-data-types.html#numeric-vectors",
    "title": "9  Vectors and Data Types",
    "section": "11.4 Numeric Vectors",
    "text": "11.4 Numeric Vectors\nThere are some specific things to note about each vector type.\nMath operators will only work on numeric vectors.\n\ndc_light_exercise_example-02\n\n\nNote that if we try to run this mathematical function we get an error:\n\n\nMany functions in R are sensitive to the data type of vectors. Mathematical functions, for example, do not make sense when applied to text (character vectors). In many cases R will give an error.\nIn some cases R will silently re-cast the variable, then perform the operation. Be watchful for when silent re-casting occurs because it might have unwanted side effects, such as deleting data or re-coding group levels in the wrong way.\n\n11.4.1 Integers\nIntegers are simple numeric vectors. The integer class is used to save memory since integers require less RAM space than numbers that contain decimals points (you need to allocate space for the numbers to the left and the numbers to the right of the decimal). Google “computer memory allocation” if you are interested in the specifics.\nIf you are doing advanced programming you will be more sensitive to memory allocation and the speed of your code, but in the intro class we will not differentiate between the two types of number vectors. In most cases they result in the same results, unless you are doing advanced numerical analysis where rounding errors matter!\n\nn &lt;- 1:5\nn\n\n[1] 1 2 3 4 5\n\nclass( n )\n\n[1] \"integer\"\n\nn[ 2 ] &lt;- 2.01   # replace the second element with \"2.01\"\nn                # all elements converted to decimals\n\n[1] 1.00 2.01 3.00 4.00 5.00\n\nclass( n )\n\n[1] \"numeric\"",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors and Data Types</span>"
    ]
  },
  {
    "objectID": "3_10-data-types.html#character-vectors",
    "href": "3_10-data-types.html#character-vectors",
    "title": "9  Vectors and Data Types",
    "section": "11.5 Character Vectors",
    "text": "11.5 Character Vectors\nThe most important rule to remember with this data type: when creating character vectors, all text must be enclosed by quotation marks.\nThis one works:\n\nc( \"a\", \"b\", \"c\" )   # this works\n\n[1] \"a\" \"b\" \"c\"\n\n\nThis one will not:\n\nc( a, b, c )  \n# Error: object 'a' not found\n\nWhen you type characters surrounded by quotes then R knows you are creating new text (“strings” of letters in programming speak).\nWhen you type characters that are not surrounded by quotes, R thinks that you are looking for an object in the environment, like the variables we have already created. It gets confused when it doesn’t find the object that you typed.\nIn general, you will use quotes when you are creating character vectors and for arguments in functions. You do not use quotes when you are referencing an active object.\nAn active object is typically a dataset or vector that you have imported or created. You can print a list of all active objects with the ls() function.\n\n11.5.1 Quotes in Arguments\nWhen you first start using R it can be confusing about when quotes are needed around arguments. Take the following example of the color argument (col=) in the plot() function.\n\ngroup &lt;- factor( sample( c(\"treatment\",\"control\"), 100, replace=TRUE ) )\nstrength &lt;- rnorm(100,100,30) + 50 * as.numeric( group==\"treatment\" )\n\npar( mfrow=c(1,2) )\nplot( strength, col=\"blue\", pch=19, bty=\"n\", cex=2 )\nplot( strength, col=group,  pch=19, bty=\"n\", cex=2 )\n\n\n\n\n\n\n\n\nThese graphs show patterns in the strength measures from our study. The first plots all subjects as blue, and the second plots subjects in the treatment group as red, control group as black.\nIn the first plot we are using a text argument to specify a color (col=\"blue\"), so it must be enclosed by quotes.\nIn the second example R selects the color based upon group membership specified by the factor called ‘group’. Since the argument is now referencing an object (col=group), we do not use quotes.\nThe exception here is when your argument requires a number. Numbers are not passed with quotes, or they would be cast as text. For example, (bty=\"n\") tells the plot to not draw a box around the graph, and the cex argument controls the dot size: (cex=2).\n\nI know. I’m with you.",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors and Data Types</span>"
    ]
  },
  {
    "objectID": "3_10-data-types.html#factors",
    "href": "3_10-data-types.html#factors",
    "title": "9  Vectors and Data Types",
    "section": "11.6 Factors",
    "text": "11.6 Factors\nWhen there are categorical variables within our data, or groups, then we use a special vector to keep track of these groups. We could just use numbers (1=female, 0=male) or characters (“male”,“female”), but factors are useful for two reasons.\nFirst, it saves memory. Text is very “expensive” in terms of memory allocation and processing speed, so using simpler data structure makes R faster.\nSecond, when a variable is set as a factor, R recognizes that it represents a group and it can deploy object-oriented functionality. When you use a factor in analysis, R knows that you want to split the analysis up by groups.\n\nheight &lt;- c( 70, 68, 69, 74, 72, 69, 68, 73  )\nstrength &lt;- c(167,185,119,142,175,204,124,117)\nsex &lt;- factor( c(\"male\",\"male\",\"female\",\"female\",\"male\",\"male\",\"female\",\"female\" ) )\n\npar( mfrow=c(1,2) )\nplot( height, strength,            # two numeric vectors: scatter plot\n      pch=19, cex=3, bty=\"n\" )   \nplot( sex, strength )              # factor + numeric: box and whisker plot       \n\n\n\n\n\n\n\n\n\nNote in this example the same plot() function produced two different types of graphs, a scatterplot and a box and whisker plot. How does this work?\nR uses the object type to determine behavior: * If input vectors are both numeric, then produce scatterplot * If input vectors are factor + numeric, then produce a box and whisker.\nThis is called object-oriented programming - the functions adapt based upon the type of object they are working with.\nIt makes the process of creating data recipes much faster! We will revisit this concept later.\n\n \nFactors are more memory efficient than character vectors because they store the underlying data as a numeric vector instead of a categorical (text) vector. Each group in the data is assigned a number, and when printing items the program only has to remember which group corresponds to which number:\n\nas.numeric( sex )\n\n[1] 2 2 1 1 2 2 1 1\n\n#  male = 2\n#  female = 1\n\nIf you print a factor, the computer just replaces each category designation with its name (2 would be replaced with “male” in this example). These replacements can be done in real time without clogging the memory of your computer as they don’t need to be saved.\nIn some instances a categorical variable might be represented by numbers. For example, grades 9-12 for high school kids. These can be tricky to re-cast.\n\ngrades &lt;- sample( x=9:12, size=10, replace=T )\ngrades\n\n [1]  9 12 10  9  9 10 12  9 11 12\n\ngrades &lt;- as.factor( grades )\ngrades\n\n [1] 9  12 10 9  9  10 12 9  11 12\nLevels: 9 10 11 12\n\nas.numeric( grades )\n\n [1] 1 4 2 1 1 2 4 1 3 4\n\nas.character( grades )\n\n [1] \"9\"  \"12\" \"10\" \"9\"  \"9\"  \"10\" \"12\" \"9\"  \"11\" \"12\"\n\n# proper way to get back to the original numeric vector\nas.numeric( as.character( grades ))\n\n [1]  9 12 10  9  9 10 12  9 11 12\n\n\n\n\nThe very important rule to remember with factors is you can’t move directly from the factor to numeric using the as.numeric() casting function. This will give you the underlying data structure, but will not give you the category names. To get these, you need the as.character casting function.\n\n \nTIP: When reading data from Excel spreadsheets (usually saved in the comma separated value or CSV format), remember to include the following argument to prevent the creation of factors, which can produce some annoying behaviors.\n\ndat &lt;- read.csv( \"filename.csv\", stringsAsFactors=F )",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors and Data Types</span>"
    ]
  },
  {
    "objectID": "3_10-data-types.html#logical-vectors",
    "href": "3_10-data-types.html#logical-vectors",
    "title": "9  Vectors and Data Types",
    "section": "11.7 Logical Vectors",
    "text": "11.7 Logical Vectors\nLogical vectors are collections of a set of TRUE and FALSE statements.\nLogical statements allow us to define groups based upon criteria, then decide whether observations belong to the group. A logical statement is one that contains a logical operator, and returns only TRUE, FALSE, or NA values.\nLogical vectors are important because organizing data into these sets is what drives all of the advanced data analytics (set theory is at the basis of mathematics and computer science).\n\n\n\n\n\n\n\n\n\n\n\nname\nsex\ntreat\nstrength\n\n\n\n\nadam\nmale\ntreatment\n167\n\n\njamal\nmale\ncontrol\n185\n\n\nlinda\nfemale\ntreatment\n119\n\n\nsriti\nfemale\ncontrol\n142\n\n\n\n\n\n\ndat$name == \"sriti\"\n\n[1] FALSE FALSE FALSE  TRUE\n\ndat$sex == \"male\"\n\n[1]  TRUE  TRUE FALSE FALSE\n\ndat$strength &gt; 180\n\n[1] FALSE  TRUE FALSE FALSE\n\n\nWhen defining logical vectors, you can use the abbreviated versions of T for TRUE and F for FALSE.\n\nz1 &lt;- c(T,T,F,T,F,F)\nz1\n\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n\nTypically logical vectors are used in combination with subset operators to identify specific groups in the data.\n\n# isolate data on all of the females in the dataset\ndat[ dat$sex == \"female\" , ]\n\n   name    sex     treat strength\n3 linda female treatment      119\n4 sriti female   control      142\n\n\nSee the next chapter for more details on subsets.",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors and Data Types</span>"
    ]
  },
  {
    "objectID": "3_10-data-types.html#generating-vectors",
    "href": "3_10-data-types.html#generating-vectors",
    "title": "9  Vectors and Data Types",
    "section": "11.8 Generating Vectors",
    "text": "11.8 Generating Vectors\nYou will often need to generate vectors for data transformations or simulations. Here are the most common functions that will be helpful.\n\n11.8.1 Repeated Values\n\ndc_light_exercise_example-03\n\n\n\n11.8.2 Sequence of Values\n\ndc_light_exercise_example-04\n\n\n\n11.8.3 Random Sample\n\ndc_light_exercise_example-05\n\n\n\n11.8.4 Draw From a Normal Distribution\n\ndc_light_exercise_example-06",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors and Data Types</span>"
    ]
  },
  {
    "objectID": "3_10-data-types.html#recycling",
    "href": "3_10-data-types.html#recycling",
    "title": "9  Vectors and Data Types",
    "section": "11.9 Recycling",
    "text": "11.9 Recycling\nWhen we create a new variable from existing variables, it is called a “transformation”. This is very common in data science. Crime is measures by the number of assaults per 100,000 people, for example (crime / pop). A batting average is the number of hits divided by the number of at bats.\nIn R, mathematical operations are vectorized, which means that operations are performed on the entire vector all at once. This makes transformations fast and easy.\n\nx &lt;- 1:10\nx + 5\n\n [1]  6  7  8  9 10 11 12 13 14 15\n\nx * 5\n\n [1]  5 10 15 20 25 30 35 40 45 50\n\n\nR uses a convention called “recycling”, which means that it will re-use elements of a vector if necessary. In the example below the x vector has 10 elements, but the y vector only has 5 elements. When we run out of y, we just start over from the beginning. This is powerful in some instances, but can be dangerous in others if you don’t realize that that it is happening.\n\nx &lt;- 1:10\ny &lt;- 1:5\nx + y\n\n [1]  2  4  6  8 10  7  9 11 13 15\n\nx * y\n\n [1]  1  4  9 16 25  6 14 24 36 50\n\n# the colors are recycled\n\nplot( 1:5, 1:5, col=c(\"red\",\"blue\"), pch=19, cex=3, bty=\"n\" )\n\n\n\n\n\n\n\n\nHere is an example of recycling gone wrong:\n\n\n\n\n\n\n\n\n\n\n\nname\nsex\ntreat\nstrength\n\n\n\n\nadam\nmale\ntreatment\n167\n\n\njamal\nmale\ncontrol\n185\n\n\nlinda\nfemale\ntreatment\n119\n\n\nsriti\nfemale\ncontrol\n142\n\n\n\n\n\n\n# create a subset of data of all female study participants\n\ndat$sex == \"female\"\n\n[1] FALSE FALSE  TRUE  TRUE\n\nthese &lt;- dat$sex == \"female\"\ndat[ these, ]  # correct subset\n\n   name    sex     treat strength\n3 linda female treatment      119\n4 sriti female   control      142\n\n# same thing with a mistake\n# whoops! should be double equal for a logical statement\n# the female element is recycled \n# just wrote over my raw data! \n\ndat$sex = \"female\"    \nthese &lt;- dat$sex == \"female\" \ndat[ these , ]\n\n   name    sex     treat strength\n1  adam female treatment      167\n2 jamal female   control      185\n3 linda female treatment      119\n4 sriti female   control      142",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors and Data Types</span>"
    ]
  },
  {
    "objectID": "3_10-data-types.html#missing-values-nas",
    "href": "3_10-data-types.html#missing-values-nas",
    "title": "9  Vectors and Data Types",
    "section": "11.10 Missing Values: NA’s",
    "text": "11.10 Missing Values: NA’s\nMissing values are coded differently in each data analysis program. SPSS uses a period, for example. In R, missing values are coded as “NA”.\nThe important thing to note is that R wants to make sure you know there are missing values if you are conducting analysis. As a result, it will give you the answer of “NA” when you try to do math with a vector that includes a missing value. You have to ask it explicitly to ignore the missing value.\n\ndc_light_exercise_example-07\n\nYou cannot use the == operator to identify missing values in a dataset. There is a special is.na() function to locate all of the missing values in a vector.\n\ndc_light_exercise_example-08",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Vectors and Data Types</span>"
    ]
  },
  {
    "objectID": "3_20-data-exploration.html",
    "href": "3_20-data-exploration.html",
    "title": "10  Exploring Data",
    "section": "",
    "text": "10.4 Exploratory Data Analysis\nExploratory Data Analysis (EDA) is the implementation of exploratory techniques to better understand new data. Typically, EDA uses visualizing and summarizing functions to detect patterns and anomalies in data beyond initial hypotheses and research questions.\nPractice Data: To demonstrate, we’ll use state park annual attendance from the State of New York’s Office for Parks, Recreation, and Historic Preservation (OPRHP).\nlibrary(readr)\n\nurl &lt;- paste0(\"https://data.ny.gov/api/views/8f3n\",\n              \"-xj78/rows.csv?accessType=DOWNLOAD\")   # Assign URL: \"url\"\n\nparks &lt;- read_csv(url)                                # Read in data: \"parks\"",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring Data</span>"
    ]
  },
  {
    "objectID": "3_20-data-exploration.html#key-concepts",
    "href": "3_20-data-exploration.html#key-concepts",
    "title": "10  Exploring Data",
    "section": "10.1 Key Concepts",
    "text": "10.1 Key Concepts\nIn this chapter, we’ll explore the following key concepts and functions:\n\nExploratory Data Analysis (EDA) & Initial Data Analysis (IDA)\n\nstr()\nnrow()\nncol()\ndim()\nlength()\nrownames()\ncolnames()\nnames()\nclass()\nlevels()\nhead()\ntail()\nsummary()\nView()\n? & help()\nDesc()\nglimpse()\n\nContingency Tables for Frequency & Proportionality\n\ntable()\nprop.table()\nftable()\n\nExploratory Data Visualization (EDV)\n\nhist()\nboxplot()\nplot()\npairs()\npar()\nggpairs() (GGally)\n\nFunctions for Descriptive Stats:\n\nmean()\nmedian()\nmin()\nmax()\nvar()\nquantile()\n\nTabulating Summary Output\nGrouping & Summary Operations with dplyr\n\n%&gt;%\nfilter()\nselect()\ngroup_by()\nsummarize()\nungroup()",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring Data</span>"
    ]
  },
  {
    "objectID": "3_20-data-exploration.html#new-packages",
    "href": "3_20-data-exploration.html#new-packages",
    "title": "10  Exploring Data",
    "section": "10.2 New Packages",
    "text": "10.2 New Packages\nThis chapter uses the following packages (in order of appearance):\n\nDescTools\nGGally\ndplyr",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring Data</span>"
    ]
  },
  {
    "objectID": "3_20-data-exploration.html#key-takeaways",
    "href": "3_20-data-exploration.html#key-takeaways",
    "title": "10  Exploring Data",
    "section": "10.3 Key Takeaways",
    "text": "10.3 Key Takeaways\nToo long; didn’t read? Here’s what you need to know:\n\nInitial and Exploratory Data Analysis (IDA; EDA) are key to exploring new data\n\nKey functions: str() for structure, summary() for descriptive stats\n\nExploratory Data Visualization (EDV) is critical for exploratory analysis\n\nKey functions: plot(), hist(), boxplot(), pairs()\n\nDescriptive statistics functions describe quantitative data\n\nKey functions: mean(), median(), min(), max()\n\nPackage dplyr makes it easy to summarize grouped variables\n\nKey functions: %&gt;% pipes data, group_by(), summarize()\nExample: data %&gt;% group_by(variable) %&gt;% summarize(count = n())\nDon’t forget to use ungroup()",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring Data</span>"
    ]
  },
  {
    "objectID": "3_20-data-exploration.html#exploratory-data-analysis",
    "href": "3_20-data-exploration.html#exploratory-data-analysis",
    "title": "10  Exploring Data",
    "section": "",
    "text": "10.4.1 Common Initial Analysis Techniques\nBase R has a litany of functions commonly used in Initial Data Analysis, or IDA.\n\nIDA is the opening salvo of functions in Exploratory Data Analysis.\nIDA techniques aid in understanding the nuances of your data.\n\n\nData Structure: Function str() is a go-to function for understanding:\n\nThe class of the dataset, e.g. matrix or data.frame\nThe dimensions of a dataset (rows and columns)\nThe class of each variable in the dataset\nThe first several values of each variable\nThe levels in each factor variable\n\n\nstr(parks)\n\nClasses 'spec_tbl_df', 'tbl_df', 'tbl' and 'data.frame':    5007 obs. of  5 variables:\n $ Year        : num  2023 2023 2023 2023 2023 ...\n $ OPRHP Region: chr  \"Allegany\" \"Allegany\" \"Allegany\" \"Allegany\" ...\n $ County      : chr  \"Cattaraugus\" \"Cattaraugus\" \"Chautauqua\" \"Chautauqua\" ...\n $ Facility    : chr  \"Allegany Quaker Area\" \"Allegany Red House Area\" \"Barcelona Lighthouse\" \"Lake Erie St Pk\" ...\n $ Attendance  : num  792323 707529 18019 91967 152695 ...\n - attr(*, \"spec\")=List of 3\n  ..$ cols   :List of 5\n  .. ..$ Year        : list()\n  .. .. ..- attr(*, \"class\")= chr [1:2] \"collector_double\" \"collector\"\n  .. ..$ OPRHP Region: list()\n  .. .. ..- attr(*, \"class\")= chr [1:2] \"collector_character\" \"collector\"\n  .. ..$ County      : list()\n  .. .. ..- attr(*, \"class\")= chr [1:2] \"collector_character\" \"collector\"\n  .. ..$ Facility    : list()\n  .. .. ..- attr(*, \"class\")= chr [1:2] \"collector_character\" \"collector\"\n  .. ..$ Attendance  : list()\n  .. .. ..- attr(*, \"class\")= chr [1:2] \"collector_double\" \"collector\"\n  ..$ default: list()\n  .. ..- attr(*, \"class\")= chr [1:2] \"collector_guess\" \"collector\"\n  ..$ delim  : chr \",\"\n  ..- attr(*, \"class\")= chr \"col_spec\"\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\nDimensions: Like measuring width and height, we can do the same with datasets:\n\nFunction nrow() prints the total number of rows\nFunction ncol() prints the total number of columns\nFunction dim() prints the total number of rows and columns\n\nRecall that in R, dimensions are printed or specified with rows first, then columns.\n\nnrow(parks)     # Print total rows\n\n[1] 5007\n\nncol(parks)     # Print total columns\n\n[1] 5\n\ndim(parks)      # Print rows and columns\n\n[1] 5007    5\n\n\n\nLength: Function length() prints the number of values for a single variable or vector.\n\nlength(parks$Facility)\n\n[1] 5007\n\n\n\nRow & Column Names: Three functions are ideal for printing row and column names:\n\nFunction rownames() prints the names of each row, though rows are rarely named\nFunction colnames() prints the names of each column (i.e. variable)\nFunction names() also prints the names of each variable\nRename variables by assigning new names to their output\n\n\nrownames(parks)[1:5]                              # Print row names 1-5\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\ncolnames(parks)                                   # Print variable names\n\n[1] \"Year\"         \"OPRHP Region\" \"County\"       \"Facility\"     \"Attendance\"  \n\nnames(parks)                                      # Print variable names\n\n[1] \"Year\"         \"OPRHP Region\" \"County\"       \"Facility\"     \"Attendance\"  \n\nnames(parks) &lt;- c(\"Year\", \"Region\", \"County\", \n                  \"Facility\", \"Attendance\")       # Reassign new names\n\nnames(parks)                                      # Print new names\n\n[1] \"Year\"       \"Region\"     \"County\"     \"Facility\"   \"Attendance\"\n\n\n\nClasses: We can determine the class of any object using function class().\n\nDetermine classes of entire datasets\nDetermine classes of individual variables\nDetermine classes of other objects, e.g. models\n\n\nclass(parks)                                      # Dataset class\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\nclass(parks$Year)                                 # Variable class\n\n[1] \"numeric\"\n\nmodel &lt;- lm(Attendance ~ Year + Region, \n            data = parks)                         # Assign linear model\n\nclass(model)                                      # Model class\n\n[1] \"lm\"\n\n\n\nCategorical Levels: Print each category (“level”) of factor variables with levels():\n\nfctr &lt;- as.factor(parks$Region)                   # Coerce to \"factor\"\n\nlevels(fctr)                                      # Print levels\n\n [1] \"Allegany\"         \"Central\"          \"Finger Lakes\"     \"Genesee\"         \n [5] \"Long Island\"      \"New York City\"    \"Niagara\"          \"Palisades\"       \n [9] \"Saratoga\"         \"Saratoga/Capital\" \"Taconic\"          \"Thousand Islands\"\n\n\n\nFirst & Last Observations: Functions head() and tail() print first and last rows:\n\nFunction head() prints the first rows of your data\nFunction tail() prints the last rows of your data\nSpecify the number of rows with argument n =\nBy default, six rows are printed\n\n\nhead(parks, n = 3)      # Print first 3 rows\n\n  Year   Region      County                Facility Attendance\n1 2023 Allegany Cattaraugus    Allegany Quaker Area     792323\n2 2023 Allegany Cattaraugus Allegany Red House Area     707529\n3 2023 Allegany  Chautauqua    Barcelona Lighthouse      18019\n\ntail(parks, n = 3)      # Print last 3 rows\n\n     Year           Region      County                              Facility\n5005 2003 Thousand Islands St Lawrence Robert Moses St Pk - Thousand Islands\n5006 2003 Thousand Islands St Lawrence               St Lawrence Golf Course\n5007 2003 Thousand Islands St Lawrence               Wilson Hill Boat Launch\n     Attendance\n5005     260797\n5006      18226\n5007      15289\n\n\n\nSummaries: Function summary() describes individual variables according to their class:\n\nClass numeric, integer, or double prints descriptive statistics\nClass character includes total values and missing values\nClass factor tallies the total occurences in each level\n\n\nsummary(parks)\n\n      Year         Region             County            Facility        \n Min.   :2003   Length:5007        Length:5007        Length:5007       \n 1st Qu.:2008   Class :character   Class :character   Class :character  \n Median :2013   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2013                                                           \n 3rd Qu.:2018                                                           \n Max.   :2023                                                           \n   Attendance     \n Min.   :      0  \n 1st Qu.:  20177  \n Median :  63934  \n Mean   : 271216  \n 3rd Qu.: 203393  \n Max.   :9596491  \n\n\n\nView Interactively: In RStudio, function View() presents data in an interactive table.\n\nView(parks)\n\n\n\n\n\n\n\nAn interactive table resulting from function View() in RStudio’s IDE.\n\n\n\n\n\n\nDocumentation: If data are from an R package, ? or help() opens documentation.\n\nlibrary(ggplot2)            # Load package containing data\n\n?economics                  # Open documentation with `?`\n\nhelp(economics)             # Open documentation with help()\n\n\n\n\n\n\n\nInteractive documentation in RStudio using ? or help().\n\n\n\n\n\n \n\n\n10.4.2 Techniques for Tallies & Proportions\nMany functions allow tallying frequencies and proportions for character and factor variables.\n\nContingency Tables: Function table() prints total of occurrences for qualitative values.\nThese tables are also called Contingency Tables.\n\ntable(parks$Region)\n\n\n        Allegany          Central     Finger Lakes          Genesee \n             121              544              586              250 \n     Long Island    New York City          Niagara        Palisades \n             626              198              378              702 \n        Saratoga Saratoga/Capital          Taconic Thousand Islands \n             322              162              389              729 \n\n\n\nProportionality: Function prop.table(), with table() output, shows proportionality.\n\nregions &lt;- table(parks$Region)      # Assign `table()` output: \"regions\"\n\nprop.table(regions)                 # Print proportionality\n\n\n        Allegany          Central     Finger Lakes          Genesee \n      0.02416617       0.10864789       0.11703615       0.04993010 \n     Long Island    New York City          Niagara        Palisades \n      0.12502497       0.03954464       0.07549431       0.14020371 \n        Saratoga Saratoga/Capital          Taconic Thousand Islands \n      0.06430997       0.03235470       0.07769123       0.14559617 \n\n\n\nFunctions table() or prop.table() can also weigh variables against eachother.\n\nsubset &lt;- parks[, c(\"Year\", \"Region\")]    # Subset two variables\n\ntable(subset)[, 1:5]                      # Frequency of \"regions\" 1-5\n\n      Region\nYear   Allegany Central Finger Lakes Genesee Long Island\n  2003        5      26           28      12          29\n  2004        5      26           28      12          29\n  2005        5      26           28      12          29\n  2006        5      26           28      12          29\n  2007        5      26           28      12          29\n  2008        5      26           28      12          29\n  2009        5      26           28      12          29\n  2010        5      26           28      12          29\n  2011        5      26           28      12          29\n  2012        5      26           28      12          29\n  2013        5      26           28      12          29\n  2014        5      26           28      12          29\n  2015        6      26           28      12          29\n  2016        6      26           28      12          29\n  2017        7      25           27      11          31\n  2018        7      25           27      11          31\n  2019        7      26           28      12          31\n  2020        7      26           28      12          31\n  2021        7      26           28      12          32\n  2022        7      26           28      12          32\n  2023        7      26           28      12          32\n\noutput &lt;- table(subset)                   # Assign `table()` output\n\nprop.table(output)[, 1:5]                 # Proportionality of \"regions\" 1-5\n\n      Region\nYear      Allegany     Central Finger Lakes     Genesee Long Island\n  2003 0.000998602 0.005192730  0.005592171 0.002396645 0.005791891\n  2004 0.000998602 0.005192730  0.005592171 0.002396645 0.005791891\n  2005 0.000998602 0.005192730  0.005592171 0.002396645 0.005791891\n  2006 0.000998602 0.005192730  0.005592171 0.002396645 0.005791891\n  2007 0.000998602 0.005192730  0.005592171 0.002396645 0.005791891\n  2008 0.000998602 0.005192730  0.005592171 0.002396645 0.005791891\n  2009 0.000998602 0.005192730  0.005592171 0.002396645 0.005791891\n  2010 0.000998602 0.005192730  0.005592171 0.002396645 0.005791891\n  2011 0.000998602 0.005192730  0.005592171 0.002396645 0.005791891\n  2012 0.000998602 0.005192730  0.005592171 0.002396645 0.005791891\n  2013 0.000998602 0.005192730  0.005592171 0.002396645 0.005791891\n  2014 0.000998602 0.005192730  0.005592171 0.002396645 0.005791891\n  2015 0.001198322 0.005192730  0.005592171 0.002396645 0.005791891\n  2016 0.001198322 0.005192730  0.005592171 0.002396645 0.005791891\n  2017 0.001398043 0.004993010  0.005392451 0.002196924 0.006191332\n  2018 0.001398043 0.004993010  0.005392451 0.002196924 0.006191332\n  2019 0.001398043 0.005192730  0.005592171 0.002396645 0.006191332\n  2020 0.001398043 0.005192730  0.005592171 0.002396645 0.006191332\n  2021 0.001398043 0.005192730  0.005592171 0.002396645 0.006391053\n  2022 0.001398043 0.005192730  0.005592171 0.002396645 0.006391053\n  2023 0.001398043 0.005192730  0.005592171 0.002396645 0.006391053\n\n\n \n\n\n10.4.3 Initial Analysis Techniques from Packages\nMany R packages are helpful in Initial Data Analysis, e.g. DescTools and dplyr.\n\nAdvanced Summaries: In DescTools, function Desc() is an enhanced summary().\n\nlibrary(DescTools)\n\nDesc(parks$Year)          # Function `Desc()` on a quantitative variable\n\n------------------------------------------------------------------------------ \nparks$Year (numeric)\n\n    length         n       NAs    unique        0s      mean    meanCI'\n     5'007     5'007         0        21         0  2'013.08  2'012.91\n              100.0%      0.0%                0.0%            2'013.25\n                                                                      \n       .05       .10       .25    median       .75       .90       .95\n  2'004.00  2'005.00  2'008.00  2'013.00  2'018.00  2'021.00  2'022.00\n                                                                      \n     range        sd     vcoef       mad       IQR      skew      kurt\n     20.00      6.08      0.00      7.41     10.00     -0.01     -1.21\n                                                                      \nlowest : 2'003.0 (236), 2'004.0 (236), 2'005.0 (236), 2'006.0 (236), 2'007.0 (236)\nhighest: 2'019.0 (245), 2'020.0 (244), 2'021.0 (246), 2'022.0 (246), 2'023.0 (246)\n\n' 95%-CI (classic)\n\n\n\n\n\n\n\n\nDesc(parks$Region)        # Function `Desc()` on a qualitative variable\n\n------------------------------------------------------------------------------ \nparks$Region (character)\n\n  length      n    NAs unique levels  dupes\n   5'007  5'007      0     12     12      y\n         100.0%   0.0%                     \n\n               level  freq   perc  cumfreq  cumperc\n1   Thousand Islands   729  14.6%      729    14.6%\n2          Palisades   702  14.0%    1'431    28.6%\n3        Long Island   626  12.5%    2'057    41.1%\n4       Finger Lakes   586  11.7%    2'643    52.8%\n5            Central   544  10.9%    3'187    63.7%\n6            Taconic   389   7.8%    3'576    71.4%\n7            Niagara   378   7.5%    3'954    79.0%\n8           Saratoga   322   6.4%    4'276    85.4%\n9            Genesee   250   5.0%    4'526    90.4%\n10     New York City   198   4.0%    4'724    94.3%\n11  Saratoga/Capital   162   3.2%    4'886    97.6%\n12          Allegany   121   2.4%    5'007   100.0%\n\n\n\n\n\n\n\n\n\n\nAdvanced Structures: In dplyr, function glimpse() is a more organized str().\n\nlibrary(dplyr)\n\nglimpse(parks)\n\nRows: 5,007\nColumns: 5\n$ Year       &lt;dbl&gt; 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023,…\n$ Region     &lt;chr&gt; \"Allegany\", \"Allegany\", \"Allegany\", \"Allegany\", \"Allegany\",…\n$ County     &lt;chr&gt; \"Cattaraugus\", \"Cattaraugus\", \"Chautauqua\", \"Chautauqua\", \"…\n$ Facility   &lt;chr&gt; \"Allegany Quaker Area\", \"Allegany Red House Area\", \"Barcelo…\n$ Attendance &lt;dbl&gt; 792323, 707529, 18019, 91967, 152695, 110265, 36725, 235154…",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring Data</span>"
    ]
  },
  {
    "objectID": "3_20-data-exploration.html#exploratory-data-visualization",
    "href": "3_20-data-exploration.html#exploratory-data-visualization",
    "title": "10  Exploring Data",
    "section": "10.5 Exploratory Data Visualization",
    "text": "10.5 Exploratory Data Visualization\nExploratory Data Visualization or EDV is critical to exploratory analyses.\n\nAllows “quick and dirty” visualizations of your new data’s variables\nUsed internally to benefit yourself, collaborators, or specialized audiences\nAssists analysts in decoding and identifying patterns and anomalies in new data\n\n \n\n10.5.1 Common Exploratory Visualization Techniques\nSeveral functions exist for exploring data visually in base R.\n\nHistograms: Quickly view the distribution of quantitative variables with hist().\n\nHistograms are univariate and show the freqency of a range of numeric values\nIncrease their resolution by increasing the number of ranges (breaks =)\n\n\nhist(parks$Attendance,        # Specify a single variable\n     breaks = 100)            # Specify number of breaks and \"bins\"\n\n\n\n\n\n\n\n\n\nBox Plots: View several distributions across categorical variables with boxplot().\n\nThe beginning and end of boxplots represent the first and third quartiles, resp.\nThe width of the box, itself, is the Interquartile Range, or IQR\nThe middle of each boxplot represents the median (50%)\n“Whiskers” are calculated by 1.5 * IQR\nOutliers are demarcated beyond whiskers\nBoth variables are separated with ~\n\n\nboxplot(parks$Attendance ~ parks$Region)\n\n\n\n\n\n\n\n\n\nScatter Plots: View relationships between quantitative variables with plot().\nSince parks only contains one quantitative variable, we use economics from ggplot2.\n\nlibrary(ggplot2)\n\nplot(x = economics$uempmed,     # Median duration of unemployment, in weeks\n     y = economics$unemploy)    # Number of unemployed, in thousands\n\n\n\n\n\n\n\n\n\nPairs Plots: Pairs plots create a matrix of small multiples for each variable.\n\nSmall multiples allow multiple side-by-side comparisons of plots on common axes\nDepending on the class of each variable, different plot methods are used\n\nAgain, for want of class numeric variables, we use economics from ggplot2.\n\nlibrary(ggplot2)\n\npairs(x = economics)\n\n\n\n\n\n\n\n\n\nModel Summaries: Function plot(), used with a model, produces four summary plots.\n\nBy adjusting the global graphics parameters of base R, we can print all four\nIn function par(), specify total rows and columns in function c()\nArgument mfrow = accepts these two values in function par()\n\n\nmodel &lt;- lm(Attendance ~ Year + Region, \n            data = parks)                   # Create linear model: \"model\"\n\npar(mfrow = c(2, 2))                        # Specify dimensions in `par()`\n\nplot(model)                                 # Call `plot()` on model\n\n\n\n\n\n\n\n\n\nAdvanced Pairs Plots: Use package ggplot2 extension GGally and ggpairs().\nAs a more colorful example, we’ll use base R dataset iris.\n\nlibrary(ggplot2)\nlibrary(GGally)                     # Load packages\n\nggpairs(iris,                       # Specify dataset\n        aes(color = Species)) +     # Map colors to variable \"Species\"\n  theme_minimal()                   # Preset theme cleans output",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring Data</span>"
    ]
  },
  {
    "objectID": "3_20-data-exploration.html#descriptive-statistics",
    "href": "3_20-data-exploration.html#descriptive-statistics",
    "title": "10  Exploring Data",
    "section": "10.6 Descriptive Statistics",
    "text": "10.6 Descriptive Statistics\nDescriptive or Summary Statistics concisely describe datasets or individual variables with summary information, e.g. mean, median, mode, minimum value, maxium value, variance, and more.\nWhile descriptive statistics can be the be-all and end-all of a descriptive analysis, they’re also integral to exploratory data analysis.\n \n\n10.6.1 Common Functions for Descriptive Statistics\nAgain, base R has no shortage of functions for descriptive or summary statistics.\n\nMean: The average or mean value of quantitative data is calculated with mean().\n\nmean(parks$Attendance)\n\n[1] 271216\n\n\n\nMedian: Find the value of the 50th percentile, or median, with median().\n\nmedian(parks$Attendance)\n\n[1] 63934\n\n\n\nMinima & Maxima: Find the smallest and largest values with min() and max().\n\nmin(parks$Attendance)   # The smallest value in variable \"Attendance\"\n\n[1] 0\n\nmax(parks$Attendance)   # The largest value in variable \"Attendance\"\n\n[1] 9596491\n\n\n\nVariance: Determine the variance of quantitative values with var().\n\nvar(parks$Attendance)\n\n[1] 670166256405\n\n\n\nQuantiles: Get quantiles, or the value at 0, 25, 50, 75, and 100%, with quantile().\n\nquantile(parks$Attendance)\n\n     0%     25%     50%     75%    100% \n      0   20177   63934  203393 9596491",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring Data</span>"
    ]
  },
  {
    "objectID": "3_20-data-exploration.html#warning-summary-statistics-missing-values",
    "href": "3_20-data-exploration.html#warning-summary-statistics-missing-values",
    "title": "10  Exploring Data",
    "section": "10.7 WARNING: SUMMARY STATISTICS & MISSING VALUES",
    "text": "10.7 WARNING: SUMMARY STATISTICS & MISSING VALUES\n\nIf the quantitative data you intend to summarize contains missing values (NA), the output may not appear as expected.\nTo tell R that missing values exist, and to exclude them from calculation, simply set argument na.rm = to TRUE.",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring Data</span>"
    ]
  },
  {
    "objectID": "3_20-data-exploration.html#group-wise-summaries-in-base-r",
    "href": "3_20-data-exploration.html#group-wise-summaries-in-base-r",
    "title": "10  Exploring Data",
    "section": "10.8 Group-Wise Summaries in Base R",
    "text": "10.8 Group-Wise Summaries in Base R\nSo far, we’ve look at a veriety of ways to explore and summarize datasets and individual variables.\nHowever, you may often seek to summarize and compare subsets of data that are grouped by some common value, category, or label. The following explores how to group and describe data by one or more specified characteristics.\n \n\n10.8.1 Tabulating Contingency Tables\nIn Section 1.1: Exploratory Data Analysis, we learned about contingency tables.\n\nContingency tables tally the frequency of values for each category in your data\n\nCalculated with function table()\n\nProportional contingency tables tally the proportion of each category\n\nCalulated with the output of table() in function prop.table()\n\n\n\nIn order to tabulate contingency tables in their own data frames:\n\nFlatten them with ftable() instead of table()\nConvert the output to a data frame with data.frame()\n\n\nreg_freq &lt;- ftable(parks$Region)    # Assign `ftable()` output: \"reg_freq\"\n\ndata.frame(reg_freq)                # Enter output in `data.frame()`\n\n               Var1 Freq\n1          Allegany  121\n2           Central  544\n3      Finger Lakes  586\n4           Genesee  250\n5       Long Island  626\n6     New York City  198\n7           Niagara  378\n8         Palisades  702\n9          Saratoga  322\n10 Saratoga/Capital  162\n11          Taconic  389\n12 Thousand Islands  729\n\n\n\nLikewise, for proportional contingency tables:\n\nFlatten them with ftable() instead of table()\nCall prop.table() on the output of ftable()\nConvert to a data frame with data.frame()\n\n\nreg_freq &lt;- ftable(parks$Region)    # Assign `ftable()` output: \"reg_freq\"\n\nreg_prop &lt;- prop.table(reg_freq)    # Assign `prop.table()` output: \"reg_prop\"\n\ndata.frame(reg_prop)                # Enter output in `data.frame()`\n\n               Var1       Freq\n1          Allegany 0.02416617\n2           Central 0.10864789\n3      Finger Lakes 0.11703615\n4           Genesee 0.04993010\n5       Long Island 0.12502497\n6     New York City 0.03954464\n7           Niagara 0.07549431\n8         Palisades 0.14020371\n9          Saratoga 0.06430997\n10 Saratoga/Capital 0.03235470\n11          Taconic 0.07769123\n12 Thousand Islands 0.14559617\n\n\n \n\n\n10.8.2 Contingency Tables & Group-Wise Frequencies\nComign soon…\n \n\n\n10.8.3 Apply Functions & Group-Wise Operations\nComign soon…\n \n\n\n10.8.4 Aggregation & Group-Wise Operations\nComign soon…",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring Data</span>"
    ]
  },
  {
    "objectID": "3_20-data-exploration.html#group-wise-operations-with-dplyr",
    "href": "3_20-data-exploration.html#group-wise-operations-with-dplyr",
    "title": "10  Exploring Data",
    "section": "10.9 Group-Wise Operations with dplyr",
    "text": "10.9 Group-Wise Operations with dplyr\nPackage dplyr is a unified framework built explicitly for data manipulation in R, e.g.:\n\nReordering rows based on one or more variables\nPerforming complex filtering and additive joins\nSelecting, reordering, and renaming variables in a data frame\nFiltering rows by specified conditional statements and logical operators\nGrouping rows by one or more specified variables and summarizing their values\n\nWe explore most of theis elsewhere. Here, we focus on group-wise operations.\nBut first, we’ll provide a brief overview of dplyr syntax.\n \n\n10.9.1 Package dplyr Syntax\nPackage dplyr has a somewhat nuanced syntax that is easy to master. Pay attention:\n\nPiping: Package dplyr uses the pipe operator, or %&gt;%, which:\n\nPasses data frames through some new function, emerging as an altered data frame\nBegins with a data frame input in the left hand side\nEnds with a data frame output from the right hand side\n\n\nparks %&gt;%                                         # Specify data frame and pipe\n  filter(Facility == \"Allegany Red House Area\")   # Pass via function `filter()`\n\n# A tibble: 21 × 5\n    Year Region   County      Facility                Attendance\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;                        &lt;dbl&gt;\n 1  2023 Allegany Cattaraugus Allegany Red House Area     707529\n 2  2022 Allegany Cattaraugus Allegany Red House Area     712217\n 3  2021 Allegany Cattaraugus Allegany Red House Area     745792\n 4  2020 Allegany Cattaraugus Allegany Red House Area     665081\n 5  2019 Allegany Cattaraugus Allegany Red House Area     718483\n 6  2018 Allegany Cattaraugus Allegany Red House Area     712217\n 7  2017 Allegany Cattaraugus Allegany Red House Area     730926\n 8  2016 Allegany Cattaraugus Allegany Red House Area     715179\n 9  2015 Allegany Cattaraugus Allegany Red House Area     750751\n10  2014 Allegany Cattaraugus Allegany Red House Area     686068\n# ℹ 11 more rows\n\n\n\nBare Variable Names: Once the dataset is named, you need not type it again.\n\nR recognizes when the data frame has been called\nTherefore, variables need only be named, without the dataset$x notation\n\n\nnames(parks)\n\n[1] \"Year\"       \"Region\"     \"County\"     \"Facility\"   \"Attendance\"\n\nparks %&gt;%                             # Call dataset object name\n  select(Year, Region, Attendance)    # Use bare variable names\n\n# A tibble: 5,007 × 3\n    Year Region   Attendance\n   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1  2023 Allegany     792323\n 2  2023 Allegany     707529\n 3  2023 Allegany      18019\n 4  2023 Allegany      91967\n 5  2023 Allegany     152695\n 6  2023 Allegany     110265\n 7  2023 Allegany      36725\n 8  2023 Central      235154\n 9  2023 Central       67994\n10  2023 Central       60649\n# ℹ 4,997 more rows\n\n\n\nTibbles: When passed through dplyr functions, they become tibbles.\n\nTibbles are truncated printouts of data frames\nTypically, tibbles print the first ten observations\nTibbles also provide both dimensions and variable classes\nAny unprinted observations are summarized underneath the first ten\nWith a large amount of variables, tibbles print only what fits on-screen\nLike unprinted observations, variables that do not fit on-screen are summarized\n\n \n\n\n10.9.2 Grouping by Variables\nIn dplyr, function group_by() accepts the bare names of one or more variables.\nNotably, grouping does nothing by itself. Data must be piped into a new function.\n\nparks %&gt;%\n  group_by(Year)            # Grouping by a single variable: \"Year\"\n\nparks %&gt;%\n  group_by(Region, Year)    # Grouping by two variables: \"Region\", \"Year\"\n\n \n\n\n10.9.3 Group By-Summarize Operations\nAs noted, we must use a function to operate on grouped data.\n\nFunction summarize() allows us to make new variables on grouped data\nWithin summarize(), the basic formula is: new_variable = function(existing_variable)\n\nHere, we create new variable Average from existing variable Attendance:\n\nparks %&gt;%                                 # Invoke \"parks\"\n  group_by(Year) %&gt;%                      # Group by \"Year\"\n  summarize(Average = mean(Attendance))   # Create \"Average\" with `mean()`\n\n# A tibble: 21 × 2\n    Year Average\n   &lt;dbl&gt;   &lt;dbl&gt;\n 1  2003 220094.\n 2  2004 223606.\n 3  2005 235422.\n 4  2006 230406.\n 5  2007 240452.\n 6  2008 231775.\n 7  2009 240104.\n 8  2010 244485.\n 9  2011 242465.\n10  2012 255553.\n# ℹ 11 more rows\n\n\n\nMultiple Summary Variables: Create multiple new variables in one summarize() call.\n\nparks %&gt;%\n  group_by(Year) %&gt;%\n  summarize(Mean = mean(Attendance),\n            Median = median(Attendance),\n            Maximum = max(Attendance),\n            Records = n())                # Create multiple new variables\n\n# A tibble: 21 × 5\n    Year    Mean Median Maximum Records\n   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;int&gt;\n 1  2003 220094. 48199  7390268     236\n 2  2004 223606. 47764  7779887     236\n 3  2005 235422. 50426. 7762999     236\n 4  2006 230406. 50374. 7756133     236\n 5  2007 240452. 61394. 7924083     236\n 6  2008 231775. 57804. 7925725     236\n 7  2009 240104. 60405  8130765     236\n 8  2010 244485. 58895  8274942     236\n 9  2011 242465. 54960  8448914     236\n10  2012 255553. 62370. 8690962     236\n# ℹ 11 more rows\n\n\n\nMultiple Grouping Variables: We can use multiple variables in group_by().\n\nCreates summaries for each permutation of unique values\n\nSuppose we group by one variable with 5 categories and one with 10\nTotal permutations equals 5 * 10, or 50 groups\n\nNote, also, that not grouping on variables will drop ungrouped variables\n\nIn other words, grouping by X, not Y, means Y is then excluded\n\n\n\nparks %&gt;%\n  group_by(Year, Region) %&gt;%              # Group on variables \"Year\", \"Region\"\n  summarize(Mean = mean(Attendance),\n            Median = median(Attendance))\n\n# A tibble: 231 × 4\n# Groups:   Year [21]\n    Year Region           Mean  Median\n   &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;\n 1  2003 Allegany      351875  192079 \n 2  2003 Central        88790.  41128.\n 3  2003 Finger Lakes   93691.  42539 \n 4  2003 Genesee        92704.  15320 \n 5  2003 Long Island   624115. 196520 \n 6  2003 New York City 332013. 118444 \n 7  2003 Niagara       536725.  93284 \n 8  2003 Palisades     198938.  19887 \n 9  2003 Saratoga      100925.  48230 \n10  2003 Taconic       161300.  93131 \n# ℹ 221 more rows\n\n\n\nCreating Summaries of Summaries: Use variables from summarize() in the same call!\n\nparks %&gt;%\n  group_by(Year) %&gt;%\n  summarize(Mean = mean(Attendance),\n            Total = sum(Attendance),                      # Create \"Total\"\n            Proportion = Total / sum(parks$Attendance))   # Use \"Total\" in formula\n\n# A tibble: 21 × 4\n    Year    Mean    Total Proportion\n   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1  2003 220094. 51942100     0.0382\n 2  2004 223606. 52770940     0.0389\n 3  2005 235422. 55559694     0.0409\n 4  2006 230406. 54375864     0.0400\n 5  2007 240452. 56746576     0.0418\n 6  2008 231775. 54698971     0.0403\n 7  2009 240104. 56664442     0.0417\n 8  2010 244485. 57698351     0.0425\n 9  2011 242465. 57221809     0.0421\n10  2012 255553. 60310499     0.0444\n# ℹ 11 more rows\n\n\n\nAssigning Summary Output: Preface summarize() calles with assignment, &lt;-.\n\nmean_att &lt;- parks %&gt;%                 # Assign expression to \"mean_att\"\n  group_by(Year) %&gt;%\n  summarize(Mean = mean(Attendance))\n\nmean_att                              # Autoprint results\n\n# A tibble: 21 × 2\n    Year    Mean\n   &lt;dbl&gt;   &lt;dbl&gt;\n 1  2003 220094.\n 2  2004 223606.\n 3  2005 235422.\n 4  2006 230406.\n 5  2007 240452.\n 6  2008 231775.\n 7  2009 240104.\n 8  2010 244485.\n 9  2011 242465.\n10  2012 255553.\n# ℹ 11 more rows\n\n\n\nUngrouping: As a rule, consider whether to use function ungroup() after group_by().\n\nIf you use grouped summaries for later analysis, they remain grouped under the hood\nThis is particularly annoying to troubleshoot when far downstream in analyses\nDon’t suffer as so many have - use ungroup()\n\n\nmean_att &lt;- parks %&gt;%\n  group_by(Year) %&gt;%\n  summarize(Mean = mean(Attendance)) %&gt;%\n  ungroup()                               # Don't enter a world of pain\n\n \n\nWARNING: SERIOUSLY, DON’T FORGET UNGROUP()\n\nWant to know why your summary data aren’t merging correctly?\nThere’s a litany of possible reasons.\nOften: You didn’t use ungroup().\n\nUse ungroup().",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring Data</span>"
    ]
  },
  {
    "objectID": "3_20-data-exploration.html#further-resources",
    "href": "3_20-data-exploration.html#further-resources",
    "title": "10  Exploring Data",
    "section": "10.10 Further Resources",
    "text": "10.10 Further Resources\nThe following resources may prove helpful to the curious learner.\n\nBryan, J. “Single Table dplyr Functions” STAT 545.\nCrawford, J (2019). “Intro to R: Data Manipulation”.\nRStudio (2015). “Data Wrangling Cheat Sheet”.",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Exploring Data</span>"
    ]
  },
  {
    "objectID": "3_21-summary-stats.html",
    "href": "3_21-summary-stats.html",
    "title": "11  Descriptive Statistics",
    "section": "",
    "text": "11.1 Some Useful Packages\nlibrary( stargazer )  # publication quality tables\nlibrary( skimr )      # quick and comprehensive descriptive stats\nlibrary( dplyr )      # data wrangling\nlibrary( pander )\nDescriptive statistics are hugely important for any analysis, but they can be challenging to produce because different classes of variable require different tables or statistics to be meaningful.\nThe most general core R summary() function prints some basic descriptives about variables in a dataset, reporting statistics based upon data type:\nname\ngroup\ngender\nheight\nweight\nstrength\n\n\n\n\nadam\ntreatment\nmale\n73\n180\n167\n\n\njamal\ncontrol\nmale\n67\n190\n185\n\n\nlinda\ntreatment\nfemale\n62\n130\n119\n\n\nsriti\ncontrol\nfemale\n65\n140\n142\nsummary( dat )\nTable continues below\n\n\n\n\n\n\n\n\n\nname\ngroup\ngender\nheight\nweight\n\n\n\n\nLength:4\ncontrol :2\nfemale:2\nMin. :62.00\nMin. :130.0\n\n\nClass :character\ntreatment:2\nmale :2\n1st Qu.:64.25\n1st Qu.:137.5\n\n\nMode :character\nNA\nNA\nMedian :66.00\nMedian :160.0\n\n\nNA\nNA\nNA\nMean :66.75\nMean :160.0\n\n\nNA\nNA\nNA\n3rd Qu.:68.50\n3rd Qu.:182.5\n\n\nNA\nNA\nNA\nMax. :73.00\nMax. :190.0\n\n\n\n\n\n\n\n\n\nstrength\n\n\n\n\nMin. :119.0\n\n\n1st Qu.:136.2\n\n\nMedian :154.5\n\n\nMean :153.2\n\n\n3rd Qu.:171.5\n\n\nMax. :185.0\nThese are not pretty enough to include in a report. Fortunately there are some functions that produce nice tables for R Markdown reports. We will use the stargazer package extensively for regression results and descriptive statistics.\nIn many instances we will be working with a large dataset with many variables that are non-numeric. For example, the Lahman package contains a People data frame with the demographic information of all Major League baseball players in the League’s 100-year history.\nVariables contained in the People data frame in the Lahman package:\nIn these cases, many of the summary functions will be of limited use. The skimr package was developed for large datasets like these. It will automatically create a set of summary tables for a variety of data types, and the default statistics are reasonable and informative:\nlibrary( skimr )\nlibrary( Lahman )\n\ndata( People )\nskim( People )\n\n\nData summary\n\n\nName\nPeople\n\n\nNumber of rows\n20676\n\n\nNumber of columns\n26\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n14\n\n\nDate\n2\n\n\nfactor\n2\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nplayerID\n0\n1.00\n5\n9\n0\n20676\n0\n\n\nbirthCountry\n59\n1.00\n3\n14\n0\n58\n0\n\n\nbirthState\n540\n0.97\n2\n22\n0\n305\n0\n\n\nbirthCity\n168\n0.99\n3\n25\n0\n4971\n0\n\n\ndeathCountry\n10582\n0.49\n3\n14\n0\n25\n0\n\n\ndeathState\n10638\n0.49\n2\n20\n0\n110\n0\n\n\ndeathCity\n10587\n0.49\n2\n26\n0\n2737\n0\n\n\nnameFirst\n37\n1.00\n2\n14\n0\n2613\n0\n\n\nnameLast\n0\n1.00\n2\n16\n0\n10440\n0\n\n\nnameGiven\n37\n1.00\n2\n43\n0\n13740\n0\n\n\ndebut\n213\n0.99\n10\n10\n0\n10842\n0\n\n\nfinalGame\n213\n0.99\n10\n10\n0\n9731\n0\n\n\nretroID\n49\n1.00\n8\n8\n0\n20627\n0\n\n\nbbrefID\n11\n1.00\n5\n9\n0\n20665\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndeathDate\n10580\n0.49\n1872-03-17\n2023-01-20\n1969-08-13\n9031\n\n\nbirthDate\n420\n0.98\n1820-04-17\n2001-11-19\n1945-08-15\n16528\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nbats\n1178\n0.94\nFALSE\n3\nR: 12823, L: 5419, B: 1256\n\n\nthrows\n977\n0.95\nFALSE\n3\nR: 15728, L: 3970, S: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbirthYear\n109\n0.99\n1936.12\n43.33\n1820\n1897\n1944\n1975\n2001\n▁▅▅▅▇\n\n\nbirthMonth\n278\n0.99\n6.63\n3.46\n1\n4\n7\n10\n12\n▇▅▅▆▇\n\n\nbirthDay\n420\n0.98\n15.62\n8.77\n1\n8\n16\n23\n31\n▇▇▇▇▆\n\n\ndeathYear\n10578\n0.49\n1967.56\n33.61\n1872\n1944\n1969\n1995\n2023\n▁▃▇▇▇\n\n\ndeathMonth\n10579\n0.49\n6.48\n3.54\n1\n3\n6\n10\n12\n▇▅▅▅▇\n\n\ndeathDay\n10580\n0.49\n15.52\n8.79\n1\n8\n15\n23\n31\n▇▇▇▆▆\n\n\nweight\n812\n0.96\n188.21\n22.50\n65\n173\n185\n200\n320\n▁▂▇▁▁\n\n\nheight\n732\n0.96\n72.38\n2.62\n43\n71\n72\n74\n83\n▁▁▁▇▁\nFor more functionality see:\nvignette( \"Using_skimr\", package = \"skimr\" )\nThere are many additional packages and tricks for producing descriptive statistics. Note, though, that most produce a print-out of summary statistics but do not return a useful “tidy” dataset that can be used in subsequent steps. For most data recipes, we will rely on the summarize() function in the dplyr package. It’s utility will become obvious in the next two chapters.",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "3_21-summary-stats.html#some-useful-packages",
    "href": "3_21-summary-stats.html#some-useful-packages",
    "title": "11  Descriptive Statistics",
    "section": "",
    "text": "library( stargazer )\ndat.numeric &lt;- select_if( dat, is.numeric )\nstargazer( dat.numeric, type=\"html\", digits=2, \n           summary.stat = c(\"n\",\"min\",\"median\",\"mean\",\"max\",\"sd\") )\n\n\n\n\n\n\n\nStatistic\n\n\nN\n\n\nMin\n\n\nMedian\n\n\nMean\n\n\nMax\n\n\nSt. Dev.\n\n\n\n\n\n\n\n\nheight\n\n\n4\n\n\n62\n\n\n66\n\n\n66.75\n\n\n73\n\n\n4.65\n\n\n\n\nweight\n\n\n4\n\n\n130\n\n\n160\n\n\n160.00\n\n\n190\n\n\n29.44\n\n\n\n\nstrength\n\n\n4\n\n\n119\n\n\n154.5\n\n\n153.25\n\n\n185\n\n\n28.85\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVARIABLE\nCLASS\nDESCRIPTION\n\n\n\n\nplayerID\nfactor\nA unique code asssigned to each player. The playerID links the data in this file with records on players in the other files.\n\n\nbirthYear\nnumeric\nYear player was born\n\n\nbirthMonth\nnumeric\nMonth player was born\n\n\nbirthDay\nnumeric\nDay player was born\n\n\nbirthCountry\ncharacter\nCountry where player was born\n\n\nbirthState\ncharacter\nState where player was born\n\n\nbirthCity\ncharacter\nCity where player was born\n\n\ndeathYear\nnumeric\nYear player died\n\n\ndeathMonth\nnumeric\nMonth player died\n\n\ndeathDay\nnumeric\nDay player died\n\n\ndeathCountry\ncharacter\nCountry where player died\n\n\ndeathState\ncharacter\nState where player died\n\n\ndeathCity\ncharacter\nCity where player died\n\n\nnameFirst\ncharacter\nPlayer’s first name\n\n\nnameLast\ncharacter\nPlayer’s last name\n\n\nnameGiven\ncharacter\nPlayer’s given name (typically first and middle)\n\n\nweight\nnumeric\nPlayer’s weight in pounds\n\n\nheight\nnumeric\nPlayer’s height in inches\n\n\nbats\nfactor\na factor: Player’s batting hand (left (L), right (R), or both (B))\n\n\nthrows\nfactor\na factor: Player’s throwing hand (left(L) or right(R))\n\n\ndebut\ncharacter\nDate that player made first major league appearance\n\n\nfinalGame\ncharacter\nDate that player made first major league appearance (blank if still active)\n\n\nretroID\ncharacter\nID used by retrosheet, http://www.retrosheet.org/\n\n\nbbrefID\ncharacter\nID used by Baseball Reference website, http://www.baseball-reference.com/\n\n\nbirthDate\ndate\nPlayer’s birthdate, in as.Date format\n\n\ndeathDate\ndate\nPlayer’s deathdate, in as.Date format",
    "crumbs": [
      "The Fundamentals",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "2_40-importing-data.html",
    "href": "2_40-importing-data.html",
    "title": "12  Getting Data into R",
    "section": "",
    "text": "12.3 “Reading In” Data\nImporting data is also called reading in data in R.\nMany base R functions begin with read, like read.csv() and read.table().\nSome R packages have new reading functions, like read_csv() in the “readr” package.\nBe careful with what you read into R!",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Getting Data into R</span>"
    ]
  },
  {
    "objectID": "2_40-importing-data.html#key-concepts",
    "href": "2_40-importing-data.html#key-concepts",
    "title": "12  Getting Data into R",
    "section": "12.1 Key Concepts",
    "text": "12.1 Key Concepts\nIn this chapter, we’ll explore the following key concepts:\n\nFile Formats\nRData Format\nFlat or Text Files\nWrapper Functions",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Getting Data into R</span>"
    ]
  },
  {
    "objectID": "2_40-importing-data.html#key-takeaways",
    "href": "2_40-importing-data.html#key-takeaways",
    "title": "12  Getting Data into R",
    "section": "12.2 Key Takeaways",
    "text": "12.2 Key Takeaways\nToo long; didn’t read? Here’s what you need to know:\n\nImporting data is called “reading in” data in R, and done with read*() functions\nKnow as what format your data are stored to use the appropriate function\nRead in statistical software files with packages “foreign” or “haven”\nRead in JSON files with package “jsonlite”\nRead directly from the web with URLs",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Getting Data into R</span>"
    ]
  },
  {
    "objectID": "2_40-importing-data.html#reading-in-data",
    "href": "2_40-importing-data.html#reading-in-data",
    "title": "12  Getting Data into R",
    "section": "",
    "text": "Source: Perry Bible Fellowship",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Getting Data into R</span>"
    ]
  },
  {
    "objectID": "2_40-importing-data.html#know-thy-data-file-formats",
    "href": "2_40-importing-data.html#know-thy-data-file-formats",
    "title": "12  Getting Data into R",
    "section": "12.4 Know Thy Data: File Formats",
    "text": "12.4 Know Thy Data: File Formats\n\n“If you know the [data] and know yourself,\n\n\nyou need not fear the result of a hundred [analyses].”\n\n\nSun Tzu, The Art of [Data Science]\n\n\nIf you want to get certain data into R, you must know the file format.\n\nFile formats are standardized frameworks for encoding and storing information\nSome file formats are better than others, depending on the information stored\nDetermine a file’s format by its filename extension, e.g. .csv\nThe most common data file formats include .csv, .tsv, and .xlsx\n\n\n\n\n\n\n\nCertain formats are better for storing data.\n\n\n\n\n\nSource: XKCD\n\n \n\n12.4.1 Text or “Flat” Files\n\nflatly /ˈflatli/\n\n\n\nShowing little interest or emotion.\n\n\n\n\nIn a firm and unequivocal manner; absolutely.\n\n\n\n\nIn a smooth and even way.\n\n\n\nOxford English Dictionary\n\n\nThe far most familiar family of file formats includes text data or flat files.\n\nFlat files are simple, standard formats that use plain text\nFlat files are ubiquitous, non-proprietary, and accessible\nConverting a spreadsheet into a text file will “flatten it”, removing all formatting\n\n\n\nPRO TIP:\nFlat files are ideal for sharing data with collaborators.\n\nNot everyone has spreadsheet software, but anyone can use flat files\nFlat files don’t preserve formats, like borders and highlighted cells\nFlat files force users to store data tabularly “flatly”\n\n\n\nComma-separated values or CSV files store data in plain text:\n\nEach line in a CSV file represents one row\nEach value in a CSV file is separated by a comma\nTherefore, each comma designates a separate column\n\n\nA CSV in the Wild: Observe the following public construction records on GitHub:\n\n\n\n\n\nWhat can you decode by observing this raw CSV file?\n\n\n\n\n\nA Wrangled CSV: R (and other software) interpret and tame these formats tabularly.\n\n\n    project       name     ending   zip  ssn      class hours rate  gross\n1  Lakeview Ajay Glass 2015-08-02 14612 3888 Journeyman    40 18.5  740.0\n2  Lakeview Ajay Glass 2015-08-02 13205 2807 Journeyman    32 28.7  918.4\n3  Lakeview Ajay Glass 2015-08-02 14428 7762 Journeyman    40 33.7 1348.0\n4  Lakeview Ajay Glass 2015-08-02 14549 9759 Journeyman    26 33.7  876.2\n5  Lakeview Ajay Glass 2015-08-09 14433 5632 Journeyman    32 17.0  544.0\n6  Lakeview Ajay Glass 2015-08-09 14612 3888 Journeyman    40 18.5  740.0\n7  Lakeview Ajay Glass 2015-08-09 14428 7762 Journeyman    40 33.7 1348.0\n8  Lakeview Ajay Glass 2015-08-09 13118 2838 Journeyman    16 28.7  459.2\n9  Lakeview Ajay Glass 2015-08-16 14433 5632 Journeyman    32 17.0  544.0\n10 Lakeview Ajay Glass 2015-08-16 14612 3888 Journeyman    42 18.5  795.5\n       net  sex race ot pdf_no pdf_pg pg_ob\n1   511.31 &lt;NA&gt; &lt;NA&gt;  0      1      1     1\n2   586.65 &lt;NA&gt; &lt;NA&gt;  0      1      1     2\n3  1025.92 &lt;NA&gt; &lt;NA&gt;  0      1      1     3\n4   891.08 &lt;NA&gt; &lt;NA&gt;  0      1      1     4\n5   384.08 &lt;NA&gt; &lt;NA&gt;  0      1      3     1\n6   657.86 &lt;NA&gt; &lt;NA&gt;  0      1      3     2\n7  1025.92 &lt;NA&gt; &lt;NA&gt;  0      1      3     3\n8   361.98 &lt;NA&gt; &lt;NA&gt;  0      1      3     4\n9   384.08 &lt;NA&gt; &lt;NA&gt;  0      1      5     1\n10  616.28 &lt;NA&gt; &lt;NA&gt;  0      1      5     2\n\n\n \nTab-separated values or TSV files store data in plain text:\n\nLike CSV, each line in a TSV file represents one row\nAlso like CSV, each value in a CSV file is separated by a tab\nEach tab designates a separate column\n\n\nA TSV in the Wild: Observe the same public construction records on GitHub:\n\n\n\n\n\nRaw TSV files are much more intepretable thanks to tabulation.\n\n\n\n\n \n\n\n12.4.2 Excel File Formats\nMicrosoft Excel files are significantly different from text or flat files.\n\nAll Excel file extensions begin with .xls; a workbook is in .xlsx format\nExcel workbooks with multiple sheets must be read in one sheet at a time\nMultiple packages can help R handle Excel files, e.g. readxl, XLConnect, gdata\n\n\n\nWARNING:\nReading Excel files into R will remove all formatting, like:\n\nHighlighted and color-coded cells\nFont formatting, e.g. bold and italics\nBorders, merged cells, formulas, and macros\n\nSome Excel users use formatting to represent information. For example, a user may use red, yellow, and green to represent a categorical variable like “low”, “medium”, and “high”, respectively. In such cases, create a new column to store these data.\n\n \nExcel Files in the Wild: It’s common to see Excel files with heavy formatting:\n\n\n\n\n\nIf R were to import this file, as is, what information would survive?\n\n\n\n\n\nSource: Special District Governments, CNY Vitals Legacy\n\n \n\n\n12.4.3 JSON File Format\nThe JSON format stands for “JavaScript Object Notation”:\n\nJSON files use extension .json\nLike TSV files, JSON files are concise and well-structured\nAlso like TSV, JSON files are generally human-readable\nJSON structure differs across files and are read into R differently\nPackages like jsonlite help R read in .json files\n\n\nJSON Files in the Wild: Check out these Game of Thrones subtitles in JSON format:\n\n\n\n\n\nThis JSON file has nested arrays, or sets of values within sets of values.\n\n\n\n\n \n\n\n12.4.4 Statistical Software Formats\nEach major statistical software has its own (sometimes proprietary) file format, e.g.:\n\nSPSS uses extensions .sav and .por\nSAS uses extensions .sas7bdat and .sas7bcat\nSTATA uses extension .dta\n\n\nDifferent packages exist for importing statistical software files, e.g.:\n\nPackage foreign, created by R’s core team, supports many formats\nPackage haven, created by Hadley Wickham, is faster but for fewer formats\n\n\n\n\n\n\n\n.NORM files do not exist.\n\n\n\n\n\nSource: XKCD\n\n \n\n\n12.4.5 RData Format\nYou may have noticed, when working in R, that you’re asked to save your workspace.\nWhen saving a workspace, .rdata or .rda files are stored in the working directory.\n\nYou can manually save part or all of your workspace with function save()\nYou can then load part or all of your workspace with function load()\n.rdata and .rda files store your session’s objects, command history, etc.\nIf creating an object is computationally expensive, consider saving it!\n\n \n\n\n12.4.6 Conclusions\nThere are a variety of file formats that you can read into R.\nFor virtually every file format, there are functions and packages to import them.\nYou must know the file’s format to decide on the best import function.",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Getting Data into R</span>"
    ]
  },
  {
    "objectID": "2_40-importing-data.html#key-functions-for-importing-data",
    "href": "2_40-importing-data.html#key-functions-for-importing-data",
    "title": "12  Getting Data into R",
    "section": "12.5 Key Functions for Importing Data",
    "text": "12.5 Key Functions for Importing Data\nOnce you’ve identified the extension type, you should consider how to read it into R.\n\nIs it a flat or text file?\nWhat character delimits each value, e.g. ,, ;?\nCan you read it in using base R or do you need a new package?\nWhat other arguments should you specify to ensure success?\n\nWe’ll look at a few common functions for importing common formats.\n \n\n12.5.1 Workhorses & Wrapper Functions:\nThe workhorse of all base R reading functions is read.table().\nThanks to its modifiability, function read.table() is remarkably flexible.\nNot only does it import most file types, it’s used by many wrapper functions.\n\n\nQUESTION\nYou can see whats under a function’s hood by typing it without ().\nLook at the internals of function read.csv(). What function is used?\n\n\nread.csv\n\nfunction (file, header = TRUE, sep = \",\", quote = \"\\\"\", dec = \".\", \n    fill = TRUE, comment.char = \"\", ...) \nread.table(file = file, header = header, sep = sep, quote = quote, \n    dec = dec, fill = fill, comment.char = comment.char, ...)\n&lt;bytecode: 0x00000251f8e13108&gt;\n&lt;environment: namespace:utils&gt;\n\n\n\nA wrapper function is a modified version of a more powerful and versatile function.\nEach is optimized for a specific task, like read.csv() - a very common function.\nWrapper functions are like customized tools. Their toolboxes are packages.",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Getting Data into R</span>"
    ]
  },
  {
    "objectID": "2_40-importing-data.html#base-r-reading-csv-files",
    "href": "2_40-importing-data.html#base-r-reading-csv-files",
    "title": "12  Getting Data into R",
    "section": "12.6 Base R: Reading CSV Files",
    "text": "12.6 Base R: Reading CSV Files\nReading comma-separated values (CSV) files is done with base R’s read.csv() function.\n\nOnly requires one argument: path =\nArgument path = accepts the directory path or a web URL\n\n\nread.csv(file = \"~/dp4ss-textbook/my_data.csv\")\nread.csv(file = \"http://www.ds4ps.com/textbook/my_data.csv\")\n\n\nWe can read in Syracuse, NY lead violations directly from the city’s open data portal.\n\n  #' Put your URL in quotes; here, we name it \"url\"\n  #' Use function paste0() to join long strings of text\n\nurl &lt;- paste0(\"https://opendata.arcgis.com/datasets/\", \n              \"c15a39a8a00e48b1a60c826c8a2cb3e0_0.csv\")\n\nlead_data &lt;- read.csv(file = url,                  # Use object storing URL text\n                      stringsAsFactors = FALSE)    # Always include this argument!\n\n\nNow, we have a locally-stored dataset:\n\nlead_data[1:10, 1:4]      # Bracks specify rows and columns to include\n\n\nConsider the following notable arguments for read.csv():\n\nfile = takes a string of text, either a directory and file name or a URL\nnrow = take a number, setting the limit of rows to read in\nstringsAsFactors = converts text into categories (not cool!)\ncol.names = takes an array made with c() to rename variables\ncolClasses = also takes an array using c() to specify classes\n\n \n\nPRO TIP: READING FROM CLIPBOARDS\nWhy not read the data you’ve copied to your clipboard?\nIn this approach, just change the first argument, file =, to “clipboard”.\n\nread.csv(file = \"clipboard\")",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Getting Data into R</span>"
    ]
  },
  {
    "objectID": "2_40-importing-data.html#base-r-reading-tsv-other-delimited-formats",
    "href": "2_40-importing-data.html#base-r-reading-tsv-other-delimited-formats",
    "title": "12  Getting Data into R",
    "section": "12.7 Base R: Reading TSV & Other Delimited Formats",
    "text": "12.7 Base R: Reading TSV & Other Delimited Formats\nThere are many ways to delimit data in a flat (text) file.\nFor example, tab-delimited, semicolon-delimited, asterisk-delimited - you get it.\nIn these instances, you can use function read.delim() to specify the delimiter.\n\nHere, we specify the delimiter as a comma, ,:\n\nlead &lt;- read.delim(file = url, \n                   stringsAsFactors = FALSE,\n                   sep = ',')                   # Specifies delimiter\n\n\nYou can specify your delimiter as any character you wish:\n\nread.delim(file = my_file, sep = ',')\nread.delim(file = my_file, sep = ';')\nread.delim(file = my_file, sep = '\\t')          # Tab-delimited\n\n\nYou can also read in any flat file, point-and-click, with RStudio’s import wizard:\n\n\n\n\n\nOpen the import wizard in the Environment pane.\n\n\n\n\n\nIn the import wizard, you can specify arguments with a user-friendly interface.\n\n\n\n\n\nClick your way to victory!",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Getting Data into R</span>"
    ]
  },
  {
    "objectID": "2_40-importing-data.html#package-readr-functions",
    "href": "2_40-importing-data.html#package-readr-functions",
    "title": "12  Getting Data into R",
    "section": "12.8 Package “readr” Functions",
    "text": "12.8 Package “readr” Functions\nThe “readr” package improves on several functions in base R’s reading toolkit, e.g.:\n\nBuilt into RStudio\nAll functions begin with read_\nAll functions have more consistent argument names\nAutomatically sets stringsAsFactors = to FALSE\nExpanded, specialized functions for specific data types\nEnhances your data frame by converting it into a “tibble”\n\n\nObserve the output from the “tibble” data frame. What’s different?\n\nlibrary(readr)\n\nurl &lt;- paste0(\"https://opendata.arcgis.com/datasets/\", \n              \"c15a39a8a00e48b1a60c826c8a2cb3e0_0.csv\")\n\nread_csv(url)\n\n\n\nPRO TIP\nConsider getting used to tibbles (enhanced data frames from “readr”).\n\nTibbles print a lot more details about your data, e.g. variable classes\nThey cooperate more easily with packages in the Rstudio ecosystem\nAlso, they automatically truncate output to appear more organized\n\n\n\n\nANOTHER PRO TIP\nYou can change variable classes much more easily in “readr” functions.\nShort string representation allows you to specify classes with one character:\n\n“c” for character\n“d” for double\n“i” for integer\n“l” for logical\n“_” to exclude variable\n\nIn a function, to specify 3 logical, 2 integer, and 4 character variables:\nread_csv(path = path, col_types = c(\"llliicccc\"))",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Getting Data into R</span>"
    ]
  },
  {
    "objectID": "2_40-importing-data.html#importing-excel-files",
    "href": "2_40-importing-data.html#importing-excel-files",
    "title": "12  Getting Data into R",
    "section": "12.9 Importing Excel Files",
    "text": "12.9 Importing Excel Files\nThere are many packages available for reading in Excel’s .xlsx format.\nPackage “readxl” is part of the same package ecosystem as “readr”.\n\nAllows new Excel-related functions, like read_xlsx()\nLists all sheets in a workbook with excel_sheets()\nSpecify certain sheets with argument sheet =\n\n\nlibrary(readxl)\n\nread_xlsx(path = url, \n          sheet = \"FY 2018 Revenue\",    # Specify sheet\n          range = \"B3:G78\",             # Specify range\n          trim_ws = TRUE,               # Remove leading/trailing spaces\n          col_types = \"dcic_cli_cc\")    # Specify variable classes\n\n\nOf course, RStudio has macros set up if you’d prefer:\n\n\n\n\n\nRStudio’s wizard for importing from Excel.\n\n\n\n\n \n\n\n\n\n\nExcel at times.\n\n\n\n\n\nSource: Reddit\n\n \n\nLIFE HACK\nUsing RStudio’s import wizard may seem like such a noob move.\nIt is, but that doesn’t mean you can’t turn it into a clutch move.\n\nEven when using wizards and macros, code will still run in the R console:\n\nCopy that code and throw it into your script. It’s often faster than typing.",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Getting Data into R</span>"
    ]
  },
  {
    "objectID": "2_40-importing-data.html#importing-statistical-software-files",
    "href": "2_40-importing-data.html#importing-statistical-software-files",
    "title": "12  Getting Data into R",
    "section": "12.10 Importing Statistical Software Files",
    "text": "12.10 Importing Statistical Software Files\nTwo packages are commonly used to import files from SPSS, SAS, STATA and other software:\n\nPackage “foreign” was developed by the R core team and highly versatile\nPackage “haven” was developed by RStudio and only reads in SPSS, SAS, and STATA\n\n\nTheir differences are similar to those between base R and “readr” functions.\n\nread.dta() is equivalent to read_dta() in the packages, respectively\nread.spss() is equivalent to read_spss(), respectively\nread.stata() is equavalent to read_stata(), and so on",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Getting Data into R</span>"
    ]
  },
  {
    "objectID": "2_40-importing-data.html#getting-data-off-of-the-web",
    "href": "2_40-importing-data.html#getting-data-off-of-the-web",
    "title": "12  Getting Data into R",
    "section": "12.11 Getting Data Off of the Web",
    "text": "12.11 Getting Data Off of the Web\nThere are a variety of methods to get data out of the web and into R.\nHere are a few popular methods:\n \n\n12.11.1 Retrieve Download URLs\nMany times, it’s a matter of copying URLs from links that download datasets.\nHere, we find the “Download” button on a City Of Pheonix Open Data dataset:\n\n\n\n\n\n\nOpen data portals make it easy to download data straight into R.\n\n\n\n\n\n\n\n\n\n\nRight-click “Download” to copy the URL.\n\n\n\n\n\nYou can save the URL in an object or use the whole string for the path = argument.\n \n\n\n12.11.2 Retrieve Raw Data URLs\nGitHub is riddled with datasets.\nYou can often find a “Download” button for them.\n\n\n\n\n\n\nThe head of a table in GitHub’s data viewer.\n\n\n\n\n\nIn the event that you cannot find a “Download” button, click on “Raw”.\nYou can use the URL provided to read the data into R.\n\n\n\n\n\n\nThe raw CSV file, with the URL for reading into R.",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Getting Data into R</span>"
    ]
  },
  {
    "objectID": "2_40-importing-data.html#further-resources",
    "href": "2_40-importing-data.html#further-resources",
    "title": "12  Getting Data into R",
    "section": "12.12 Further Resources",
    "text": "12.12 Further Resources\nThe following resources should help you further your knowledge on reading in data.\n\nOfficial R Documentation for Function read.table()\nIntroduction to Package “readr”\n“Reading Text Data” (Crawford, 2018)\n“How to Share Data for Collaboration” (Ellis & Leek, 2017)\nRStudio’s “Data Import Cheat Sheet”",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Getting Data into R</span>"
    ]
  },
  {
    "objectID": "2_50-saving-objects.html",
    "href": "2_50-saving-objects.html",
    "title": "13  Saving Data to File",
    "section": "",
    "text": "13.3 Functions for Writing Data\nWe read data into R, or import, with read*() functions.\nSimilarly, we write data from R, or export, with write*() functions.\nWhat is writing data?\nSimply put, it’s the act of storing data in a location and format of your choosing.\nTypically, your data are stored in an object like a matrix or data frame.\nIt’s simply a matter of exporting a data-laden object.",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Saving Data to File</span>"
    ]
  },
  {
    "objectID": "2_50-saving-objects.html#key-concepts",
    "href": "2_50-saving-objects.html#key-concepts",
    "title": "13  Saving Data to File",
    "section": "13.1 Key Concepts",
    "text": "13.1 Key Concepts\nIn this chapter, we’ll explore the following key concepts:\n\nCSV, TSV, & Delimited Files\nR Data Sets (RDS)\nRData Format",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Saving Data to File</span>"
    ]
  },
  {
    "objectID": "2_50-saving-objects.html#key-takeaways",
    "href": "2_50-saving-objects.html#key-takeaways",
    "title": "13  Saving Data to File",
    "section": "13.2 Key Takeaways",
    "text": "13.2 Key Takeaways\nToo long; didn’t read? Here’s everything you need to know:\n\nImporting data is reading data; exporting data is writing data\nKeep your working directories in mind, all files write to them by default\nBase R function write.table() writes most common file types\nBase R function write.csv() writes CSVs, TSVs, and more\nPackage “readr” has write*() functions for each file type\nSave an object with save() and .rds extensions\nSave objects and workspaces using extension .RData\nWrite data to your clipboard with writeClipboard()",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Saving Data to File</span>"
    ]
  },
  {
    "objectID": "2_50-saving-objects.html#functions-for-writing-data",
    "href": "2_50-saving-objects.html#functions-for-writing-data",
    "title": "13  Saving Data to File",
    "section": "",
    "text": "Most base R reading functions have equivalent writing functions\nMost package “readr” functions also have similar writing functions\nPackages for Excel, JSON, and other file formats have writing functions\n\n\n\n\n\n\n\n\n13.3.1 A Brief Note on Working Directories\nUnless otherwise specified, data are written to your working directory by default.\n\nYou can often specify different paths to save your data with argument file =\nPrint your working directory with function getwd()\nChange your working directory with function setwd()\nCreate new directories with function dir.create()\nSee contents of directories with function dir()\n\n \n\n\n13.3.2 Base R’s Workhorse Writing Function: write.table()\nR’s workhorse reading function, on which other functions depend, is read.table().\nR’s workhorse writing function is write.table().\n\nFunction wrappers like write.csv() are powered by write.table() under the hood.\n\nwrite.csv\n\nfunction (...) \n{\n    Call &lt;- match.call(expand.dots = TRUE)\n    for (argname in c(\"append\", \"col.names\", \"sep\", \"dec\", \"qmethod\")) if (!is.null(Call[[argname]])) \n        warning(gettextf(\"attempt to set '%s' ignored\", argname), \n            domain = NA)\n    rn &lt;- eval.parent(Call$row.names)\n    Call$append &lt;- NULL\n    Call$col.names &lt;- if (is.logical(rn) && !rn) \n        TRUE\n    else NA\n    Call$sep &lt;- \",\"\n    Call$dec &lt;- \".\"\n    Call$qmethod &lt;- \"double\"\n    Call[[1L]] &lt;- quote(utils::write.table)\n    eval.parent(Call)\n}\n&lt;bytecode: 0x0000016094b6c958&gt;\n&lt;environment: namespace:utils&gt;\n\n\n\nIf all else fails (and it probably won’t), you can depend on write.table().\n \n\n\n13.3.3 Writing Text Files: write.csv() & write_csv()\nComma-separated values (CSV) files are the most common type of output in R.\nHowever, we can use the same write*() functions to create TSVs and more.\n\nPractice Data: Let’s create a simple data frame to practice writing data.\n\nname &lt;- c(\"Fatimah\", \"Li\", \"Arnold\", \"Fede\", \"Sly\")   # Character vector\nweight &lt;- c(61.4, 68.4, 81.8, 79.9, 90.3)             # Double vector\nage &lt;- c(29L, 31L, 44L, 33L, 27L)                     # Integer vector\n\nclients &lt;- data.frame(name, weight, age,              # Create data frame\n                      stringsAsFactors = FALSE)       # Don't forget this!\n\nclients\n\n     name weight age\n1 Fatimah   61.4  29\n2      Li   68.4  31\n3  Arnold   81.8  44\n4    Fede   79.9  33\n5     Sly   90.3  27\n\n\n\nBase R: Write CSV files using function write.csv().\n\nwrite.csv(x = clients,                # Write object \"clients\"\n          file = \"clients.csv\")       # Write file name and extension\n\nInclude Extensions: When writing a file, include the extension in the file name.\n\nSaving an R script? Include .r\nSaving an Excel sheet? Include .xlsx\nSaving a CSV? Include .csv\n\n\nTSVs & Other Delimiters: You’re not restricted to using commas with write.csv().\n\nwrite.csv(x = clients,\n          file = \"clients.tsv\",       # Use appropriate extension\n          sep = \"\\t\")                 # Save as tab-delimited\n\n\nNotable Arguments: Function write.csv() has some notable parameters, e.g.\n\nx = specifies the name of the object to write\nfile = specifies the output file name; requires quotes and extension\nsep = specifies the delimiter, e.g. commas, tabs, semicolons, etc.\nna = specifies the character(s) to use instead of missing values\n\n\nPackage “readr”: Function write_csv() is the same as write.csv() except:\n\nSignificantly faster at writing data\nDoes not write row names automatically\nCannot write files with non-comma delimiters\nMore consistent argument names; file = is now path =\n\n\nWriting with “readr”: Observe write_csv() in action:\n\nlibrary(readr)\n\nwrite_csv(x = clients,\n          path = \"clients.csv\")\n\n\nTSV Files with “readr”: Bummer! Can’t write TSV files with write_csv().\nHark! Package “readr” has function write_tsv() for precisely that!\n\nlibrary(readr)\n\nwrite_tsv(x = clients,\n          path = \"clients.tsv\")     # Right tool for the job\n\n\nIn fact, package “readr” has writing functions optimized for many file types.\nWe encourage you to check out each one, e.g. help(write_delim):\n\nwrite_csv()\nwrite_csv2()\nwrite_delim()\nwrite_excel_csv()\nwrite_file()\nwrite_rds()\nwrite_tsv()\n\n \n\n\n\n\n\nMind blown.",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Saving Data to File</span>"
    ]
  },
  {
    "objectID": "2_50-saving-objects.html#saving-your-work-r-datasets-rds",
    "href": "2_50-saving-objects.html#saving-your-work-r-datasets-rds",
    "title": "13  Saving Data to File",
    "section": "13.4 Saving Your Work: R Datasets (RDS)",
    "text": "13.4 Saving Your Work: R Datasets (RDS)\nManually restoring your workspace to your former session’s glory is a pain.\nHence, base R has functions save() and load() to save objects locally.\n\nSaving an Object: Save your original object, clients, as a .rds file:\n\nsave(clients, \n     file = \"clients_object.rds\")\n\n\nSaving More than One Object: List each object first and save as .RData:\n\nsave(age, name, weight, clients,\n     file = \"clients_all.RData\")\n\n\nSaving your Workspace: Save your history and all objects with save.image():\n\nsave.image(file = \"my_workspace.RData\")\n\n\nLoad Objects & Workspaces: Simply input the file name into function load():\n\nload(\"clients_object.rds\")      # Load a single object file\nload(\"clients_all.RData\")       # Load multiple objects\nload(\"my_workspace.RData\")      # Laod entire workspace\n\n\nWhy save objects and workspaces?\n\nPerfectly reproduce your environment for collaborators\nLoad typical header information for scripts, like authors and versions\nSave and load objects that take a lot of time or computing power to create\nWe haven’t learned many object types, but some are useful when reproduced often",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Saving Data to File</span>"
    ]
  },
  {
    "objectID": "2_50-saving-objects.html#writing-data-to-statistical-software-files",
    "href": "2_50-saving-objects.html#writing-data-to-statistical-software-files",
    "title": "13  Saving Data to File",
    "section": "13.5 Writing Data to Statistical Software Files",
    "text": "13.5 Writing Data to Statistical Software Files\nThe “foreign” and “haven” packages help read and write files used in SAS, SPSS, etc.\n\nPackage “foreign” uses write.foreign() as a catch-all writing function.\nArgument package = accepts “SPSS”, “SAS”, and other software names.\nIt also has wrapper functions like write_dta() for Sata files, e.g.\n\nlibrary(foreign)\n\nwrite.foreign(df = clients, \n              datafile = \"clients.sas\", \n              package = \"SAS\")\n\n\nPackage “haven” only has four specific functions rather than a single workhorse:\n\nlibrary(haven)\n\nwrite_dta(clients, \"clients.dta\")   # Stata\nwrite_sas(clients, \"clients.sas\")   # SAS\nwrite_sav(clients, \"clients.sav\")   # SPSS\nwrite_xpt(clients, \"clients.xpt\")   # SAS, too",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Saving Data to File</span>"
    ]
  },
  {
    "objectID": "2_50-saving-objects.html#copying-data-to-your-clipboard",
    "href": "2_50-saving-objects.html#copying-data-to-your-clipboard",
    "title": "13  Saving Data to File",
    "section": "13.6 Copying Data to Your Clipboard",
    "text": "13.6 Copying Data to Your Clipboard\nRead data from the clipboard with readClipboard(); write with writeClipboard().\n\nThe concept, briefly:\n\nWhen copying text, it goes to your clipboard - the same as writeClipboard()\nWhen pasting text, it comes from your clipboard - the same as readClipboard()\n\n\n\n\n\n\n\nYou can write character data to your clipboard from R.\n\n\n\n\n\nCharacter Data Only: Here, we’ll copy two objects to the clipboard.\n\nNote that only the character data will copy to your clipboard\nNon-characyet data must be coerced with function as.character()\n\n\ntxt &lt;- \"This is a sentences comprised of text.\"\nnum &lt;- 2.718\n\nwriteClipboard(txt)                 # Accepts character data\nwriteClipboard(num)                 # Will not accept numeric data\nwriteClipboard(as.character(num))   # Accepts when coerced to character",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Saving Data to File</span>"
    ]
  },
  {
    "objectID": "2_50-saving-objects.html#copying-data-to-excel",
    "href": "2_50-saving-objects.html#copying-data-to-excel",
    "title": "13  Saving Data to File",
    "section": "13.7 Copying Data to Excel",
    "text": "13.7 Copying Data to Excel\nThere’s a “quick and dirty” method to copying and pasting data from R into Excel.\n\nOpen the tabular data object with function View()\nHighlight each cell, starting at lower-right, ending at upper-left\nRight click or Ctrl + C to copy the RStudio Viewer data\nRight click or Ctrl + V to paste into Excel\n\n\n\n\n\n\n\nCopy and paste right from your RStudio viewer.",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Saving Data to File</span>"
    ]
  },
  {
    "objectID": "2_50-saving-objects.html#further-resources",
    "href": "2_50-saving-objects.html#further-resources",
    "title": "13  Saving Data to File",
    "section": "13.8 Further Resources",
    "text": "13.8 Further Resources\n\nExporting Data (Quick-R)\nR Data Import/Export (CRAN)",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Saving Data to File</span>"
    ]
  },
  {
    "objectID": "2_50-saving-objects.html#works-cited",
    "href": "2_50-saving-objects.html#works-cited",
    "title": "13  Saving Data to File",
    "section": "13.9 Works Cited",
    "text": "13.9 Works Cited\n\nSpongBob SquarePants\nTim & Eric’s Awesome Show\nGravity Falls",
    "crumbs": [
      "Data IO",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Saving Data to File</span>"
    ]
  },
  {
    "objectID": "4_10-data-verbs.html",
    "href": "4_10-data-verbs.html",
    "title": "14  Data Verbs",
    "section": "",
    "text": "14.1 Packages Used in This Chapter\nlibrary( dplyr )\nlibrary( pander )",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Verbs</span>"
    ]
  },
  {
    "objectID": "4_10-data-verbs.html#key-concepts",
    "href": "4_10-data-verbs.html#key-concepts",
    "title": "14  Data Verbs",
    "section": "14.2 Key Concepts",
    "text": "14.2 Key Concepts\n\n\n\n\n\nData verbs are functions that require a dataframe as the primary argument, perform some transformation on the data, then return a new dataframe.\n\n\n\n\nFor a nice overview of all of the dataset verbs in dplyr check out The dplyr Cheatsheet.",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Verbs</span>"
    ]
  },
  {
    "objectID": "4_10-data-verbs.html#the-dplyr-package",
    "href": "4_10-data-verbs.html#the-dplyr-package",
    "title": "14  Data Verbs",
    "section": "14.3 The dplyr Package",
    "text": "14.3 The dplyr Package\nThis chapter will demonstrate a few basic dataset functions contained within the dplyr package. There are a few things to note as you get started:\n\ndplyr functions are all data verbs that accept a dataset as the argument, transform the data, and return a new dataset.\nThe first argument is always the dataset name, and variables (columns) can be referenced directly by name without quotation marks.\ndplyr functions will return data as a “tibble” (tbl_df class), which is a regular data frame wrapped in a nice print method that includes metadata in the printout.\n\nFor example, here is the regular data frame preview:\n\n\n   weight group\n1    4.17  ctrl\n2    5.58  ctrl\n3    5.18  ctrl\n4    6.11  ctrl\n5    4.50  ctrl\n6    4.61  ctrl\n7    5.17  ctrl\n8    4.53  ctrl\n9    5.33  ctrl\n10   5.14  ctrl\n\n\nThe tibble will print the first few rows and columns of a dataset, and includes dataset dimensions and vector classes:\n\n\n# A tibble: 30 × 2\n   weight group\n    &lt;dbl&gt; &lt;fct&gt;\n 1   4.17 ctrl \n 2   5.58 ctrl \n 3   5.18 ctrl \n 4   6.11 ctrl \n 5   4.5  ctrl \n 6   4.61 ctrl \n 7   5.17 ctrl \n 8   4.53 ctrl \n 9   5.33 ctrl \n10   5.14 ctrl \n# ℹ 20 more rows\n\n\nWe will cover the following dplyr functions in this section of the textbook:\n\n\n\n\n\n\n\nDATA VERB\nACTION\n\n\n\n\nfilter()\nSelect rows\n\n\nselect()\nSelect columns\n\n\narrange()\nSort the dataset by one or more columns\n\n\nmutate()\nCreate a new variable by transforming an existing variable or variables\n\n\nsummarize()\nCreate summary statistics for specified variables\n\n\ngroup_by()\nSplit the dataset (implicitly) into a separate dataset for each group\n\n\n\n\n14.3.1 Use filter() to Subset Rows\n\n\n\n\n\n\n\n\n\nIn the last chapter we learned how to use operators to translate from plain English questions to data queries.\nAs an example, a city manager might want to know the average amount owed on a delinquent property tax.\n\n\n\n\n\n\n\n\n\ntax.id\namount.owed\n\n\n\n\n1\n$0\n\n\n2\n$5,549\n\n\n3\n$0\n\n\n4\n$1,709\n\n\n5\n$0\n\n\n6\n$634\n\n\n7\n$0\n\n\n8\n$0\n\n\n9\n$0\n\n\n10\n$9,353\n\n\n\n\n\nWe could write the query as follows:\n\nDefine the group.\nSelect the data that belongs to the group.\nAnalyze the group subset.\n\n\nthese.late &lt;- taxdat$amount.owed &gt; 0                 # 1. Define group\noverdue.amounts &lt;- taxdat$amount.owed[ these.late ]  # 2. Select data\noverdue.amounts\n\n[1] 5549 1709  634 9353 1366\n\nmean( overdue.amounts )                              # 3. Analyze data\n\n[1] 3722.2\n\n\nThe filter() function in the dplyr package is a slightly more elegant verb for selecting the group and subsetting the data by rows.\n\nfilter( dataset name , logical expression )\n\n\nfilter( taxdat, amount.owed &gt; 0 )\n\n  tax.id amount.owed\n1      2        5549\n2      4        1709\n3      6         634\n4     10        9353\n5     12        1366\n\n\nNote that we do not need to reference the dat$ references inside dplyr functions.\n\n\n14.3.2 select() Columns\n\n\n\n\n\n\n\n\n\nIn the core R operators, we select colums from a dataset using the subset function:\n\ndata( USArrests ) # historic data on crime rates in the US\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nMurder\nAssault\nUrbanPop\nRape\n\n\n\n\nAlabama\n13.2\n236\n58\n21.2\n\n\nAlaska\n10\n263\n48\n44.5\n\n\nArizona\n8.1\n294\n80\n31\n\n\nArkansas\n8.8\n190\n50\n19.5\n\n\nCalifornia\n9\n276\n91\n40.6\n\n\nColorado\n7.9\n204\n78\n38.7\n\n\n\n\n\n\nUSArrests[ , c(\"Murder\",\"Assault\") ]\n\n\n\n\n\n\n\n\n\n\n\n \nMurder\nAssault\n\n\n\n\nAlabama\n13.2\n236\n\n\nAlaska\n10\n263\n\n\nArizona\n8.1\n294\n\n\nArkansas\n8.8\n190\n\n\nCalifornia\n9\n276\n\n\nColorado\n7.9\n204\n\n\n\n\n\nThe select() function converts this operation into a data verb:\n\nselect( USArrests, Murder, Assault )\n\n\n\n\n\n\n\n\n\n\n\n \nMurder\nAssault\n\n\n\n\nAlabama\n13.2\n236\n\n\nAlaska\n10\n263\n\n\nArizona\n8.1\n294\n\n\nArkansas\n8.8\n190\n\n\nCalifornia\n9\n276\n\n\nColorado\n7.9\n204\n\n\n\n\n\nThe select() function adds a lot of additional arguments that make it easy to quickly identify and keep only the necessary variables. We can demonstrate a few using the built-in iris dataset in R.\n\nhead( iris ) %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\nExclude a column with the negative sign:\n\nselect( iris, -Species ) \n\n\n\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\n\n\n\n\n5.1\n3.5\n1.4\n0.2\n\n\n4.9\n3\n1.4\n0.2\n\n\n4.7\n3.2\n1.3\n0.2\n\n\n4.6\n3.1\n1.5\n0.2\n\n\n5\n3.6\n1.4\n0.2\n\n\n5.4\n3.9\n1.7\n0.4\n\n\n\n\n\nSelect by variable names:\n\nselect( iris, ends_with( \"Length\" ) ) \n\n\n\n\n\n\n\n\n\n\nSepal.Length\nPetal.Length\n\n\n\n\n5.1\n1.4\n\n\n4.9\n1.4\n\n\n4.7\n1.3\n\n\n4.6\n1.5\n\n\n5\n1.4\n\n\n5.4\n1.7\n\n\n\n\n\n\nselect( iris, starts_with( \"Petal\" ) ) \n\n\n\n\n\n\n\n\n\n\nPetal.Length\nPetal.Width\n\n\n\n\n1.4\n0.2\n\n\n1.4\n0.2\n\n\n1.3\n0.2\n\n\n1.5\n0.2\n\n\n1.4\n0.2\n\n\n1.7\n0.4\n\n\n\n\n\n\nselect( iris, matches(\"pal\") ) \n\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\n\n\n\n\n5.1\n3.5\n\n\n4.9\n3\n\n\n4.7\n3.2\n\n\n4.6\n3.1\n\n\n5\n3.6\n\n\n5.4\n3.9\n\n\n\n\n\nOr we can select by a range of variables by placing a colon between the first and last:\n\nselect( iris, Sepal.Length:Petal.Width ) \n\n\n\n\n\n\n\n\n\n\n\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n1.4\n0.2\nsetosa\n\n\n1.4\n0.2\nsetosa\n\n\n1.3\n0.2\nsetosa\n\n\n1.5\n0.2\nsetosa\n\n\n1.4\n0.2\nsetosa\n\n\n1.7\n0.4\nsetosa\n\n\n\n\n\n\n\n14.3.3 arrange() Sorts Data\n\n\n\n\n\n\n\n\n\nThe arrange() function sorts a dataset by one or more columns. By default, it sorts from smallest to largest.\n\narrange( PlantGrowth, weight ) \n\n\n\n\n\n\n\n\n\n\nweight\ngroup\n\n\n\n\n3.59\ntrt1\n\n\n3.83\ntrt1\n\n\n4.17\nctrl\n\n\n4.17\ntrt1\n\n\n4.32\ntrt1\n\n\n\n\n\nIf we prefer the dataset be sorted from largest to smallest, we can apply the descending function desc() to the sort variable.\n\narrange( PlantGrowth, desc(weight) ) \n\n\n\n\n\n\n\n\n\n\nweight\ngroup\n\n\n\n\n6.31\ntrt2\n\n\n6.15\ntrt2\n\n\n6.11\nctrl\n\n\n6.03\ntrt1\n\n\n5.87\ntrt1\n\n\n\n\n\nOr alternatively we can use the shortcut syntax of adding a negative sign in front of the variable:\n\narrange( PlantGrowth, -weight ) \n\nWe can also sort by multiple columns at once:\n\narrange( PlantGrowth, group, weight ) \n\n\n\n\n\n\n\n\n\n\ngroup\nweight\n\n\n\n\nctrl\n5.17\n\n\nctrl\n5.18\n\n\nctrl\n5.33\n\n\nctrl\n5.58\n\n\nctrl\n6.11\n\n\ntrt1\n4.69\n\n\ntrt1\n4.81\n\n\ntrt1\n4.89\n\n\ntrt1\n5.87\n\n\ntrt1\n6.03\n\n\n\n\n\nNOTE, the equivalent core R functions would use subset[] and order() functions together. You might see examples on Stack Overflow written like this:\n\nPlantGrowth[ order(PlantGrowth$weight, decreasing=TRUE) , ]\n\n\n\n\n\n\n\n\n\n\n\n \nweight\ngroup\n\n\n\n\n21\n6.31\ntrt2\n\n\n28\n6.15\ntrt2\n\n\n4\n6.11\nctrl\n\n\n17\n6.03\ntrt1\n\n\n15\n5.87\ntrt1\n\n\n29\n5.8\ntrt2\n\n\n\n\n\nAs you can see, the dplyr versions are typically more intuitive and concise!\n\n\n14.3.4 Variable Transforms with mutate()\nOne of the most common operations in data analysis is to create a new variable from one or more existing variables, a “variable transformation”. Some examples include:\n\nx_squared &lt;-  x * x\n\ncelsius &lt;-  ( fahrenheit - 32 ) * ( 5/9 )\n\nbody.mass.index &lt;-  kg / meters^2\n\nper.capita.income &lt;-  income / population\n\nThe mutate() function creates a new transformed variable from the formula you specify and adds it to the original dataset.\nAs an example, perhaps we have data on the number of nonprofits located in each US city. If we look at the raw count of nonprofits, it makes it look as though the large cities have the most vibrant nonprofit sectors:\n\n\n\n\n\n\n\n\n\ncity\nnonprofits\n\n\n\n\nNEW YORK\n26503\n\n\nLOS ANGELES\n17417\n\n\nWASHINGTON\n15701\n\n\nSAN FRANCISCO\n12149\n\n\nBOSTON\n10536\n\n\nCHICAGO\n10247\n\n\nPHILADELPHIA\n8538\n\n\nDALLAS\n6008\n\n\nSEATTLE\n5830\n\n\nATLANTA\n5438\n\n\n\n\n\nBut these numbers may be misleading. Once we account for the population size through a new nonprofit density metric (nonprofits per 1,000 residents), we can see that some smaller cities have higher densities per capita.\n\ndat.npos &lt;- mutate( dat.npos, density = nonprofits / (pop/1000) )\n\n\n\n\n\n\n\n\n\n\ncity\ndensity\n\n\n\n\nPORTLAND\n2.65\n\n\nMADISON\n2.415\n\n\nANCHORAGE\n2.156\n\n\nSANTA BARBARA\n1.928\n\n\nLINCOLN\n1.886\n\n\nWASHINGTON\n1.866\n\n\nASHEVILLE\n1.805\n\n\nTALLAHASSEE\n1.785\n\n\nBOSTON\n1.692\n\n\nALBANY\n1.685\n\n\n\n\n\n\n\n14.3.5 rename() Variables\nMore often than not you will read in a dataset that has strange or meaningless variable names:\n\nx1 &lt;- c(\"male\",\"male\",\"female\",\"female\")\nx2 &lt;- c(\"treatment\",\"control\",\"treatment\",\"control\")\ndat &lt;- data.frame( x1, x2 )\n\n\n\n\n\n\n\n\n\n\nx1\nx2\n\n\n\n\nmale\ntreatment\n\n\nmale\ncontrol\n\n\nfemale\ntreatment\n\n\nfemale\ncontrol\n\n\n\n\n\nThe core R functions make it a little awkward to rename these variables.\n\nnames( dat ) &lt;- c(\"gender\",\"study.group\")\n\n\n\n\n\n\n\n\n\n\ngender\nstudy.group\n\n\n\n\nmale\ntreatment\n\n\nmale\ncontrol\n\n\nfemale\ntreatment\n\n\nfemale\ncontrol\n\n\n\n\n\nThe rename() function provides a more intuitive syntax:\n\nrename( dat, gender=x1, study.group=x2 )\n\n\n\n14.3.6 summarize() Variables\nThe next chapter will cover descriptive statistics in more depth, including some useful packages and functions for generating statistics for a variety of variable types and reporting nice tables.\nMost descriptive functions, however, are not data verbs in the sense that they accept a data frame as the input and return a transformed data frame or tibble. The dplyr function summarize() is the primary function that will be used in data recipes (see the next chapter).\nLike other data verbs, the first argument will be the input dataset. In this case, there is no pre-determined set of descriptive statistics. The user needs to specify the desired metrics.\n\n\n\n\n\n\n\n\n\n\ngroup\ngender\nstrength\n\n\n\n\ntreatment\nfemale\n162\n\n\ncontrol\nfemale\n128\n\n\ntreatment\nfemale\n119\n\n\ncontrol\nfemale\n100\n\n\ntreatment\nfemale\n123\n\n\ntreatment\nmale\n94\n\n\n\n\n\n\nsummarize( dat, n=n(), min=min(strength), mean=mean(strength), max=max(strength) )\n\n    n min  mean max\n1 100  42 99.11 162\n\n\nSimilarly, the native table() function is useful, but returns a table object. The dplyr count() function will function almost identically, but it will return a data frame.\n\ndplyr::count( dat, group, gender )\n\n      group gender  n\n1   control female 18\n2   control   male 25\n3 treatment female 26\n4 treatment   male 31\n\n\nThe real power of this function is the ability to use it with the group_by() function to analyze outcomes for many data subsets at once.\n\ngrouped.dat &lt;- group_by( dat, group, gender )\ndplyr::summarize( grouped.dat, \n                  n=n(), \n                  min=min(strength), \n                  mean=round( mean(strength), 1 ), \n                  max=max(strength) ) \n\n# A tibble: 4 × 6\n# Groups:   group [2]\n  group     gender     n   min  mean   max\n  &lt;fct&gt;     &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 control   female    18    79 110.    149\n2 control   male      25    42  74.2    98\n3 treatment female    26   104 125.    162\n4 treatment male      31    56  91.4   125\n\n\nWe will discuss this functionality in-depth two chapters from now.",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Verbs</span>"
    ]
  },
  {
    "objectID": "4_20-data-recipes.html",
    "href": "4_20-data-recipes.html",
    "title": "15  Data Recipes",
    "section": "",
    "text": "15.1 Packages Used in This Chapter\nlibrary( dplyr )\nlibrary( pander )\nlibrary( ggvis )",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Recipes</span>"
    ]
  },
  {
    "objectID": "4_20-data-recipes.html#key-concepts",
    "href": "4_20-data-recipes.html#key-concepts",
    "title": "15  Data Recipes",
    "section": "15.2 Key Concepts",
    "text": "15.2 Key Concepts\n\n\n\n\n\nData recipes are written using a series of data steps. We can simplify this process using pipes\n\n\n\n\n\n\n\n\n\n\nRecall, data verbs use data frames as the primary input and the output value.\n\n\n\n\n\n\n\n\n\n\nThe pipe operator passes a data frame forward through a chain of data verbs. We only reference the dataset name once, and all other times it’s implicitly called through piping.",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Recipes</span>"
    ]
  },
  {
    "objectID": "4_20-data-recipes.html#the-pipe-operator",
    "href": "4_20-data-recipes.html#the-pipe-operator",
    "title": "15  Data Recipes",
    "section": "15.3 The Pipe Operator %>%",
    "text": "15.3 The Pipe Operator %&gt;%\nThe idea of functions() was first introduced using a metaphor of a cookie recipe that has ingredients (data and arguments) and requires that each step of the process building on the results of the previous step.\nThe pipe operator allows us to follow this same model to build “data recipes”, a stylized way of writing a program as a series of data verbs chained together to wrangle and analyze the data. The pipe operator passes the data from one verb to the next without having to name it directly.\n\n\n\n\n\nThe pipe operator allows us to pass a transformed dataset forward in the recipe.",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Recipes</span>"
    ]
  },
  {
    "objectID": "4_20-data-recipes.html#building-data-recipes",
    "href": "4_20-data-recipes.html#building-data-recipes",
    "title": "15  Data Recipes",
    "section": "15.4 Building Data Recipes",
    "text": "15.4 Building Data Recipes\nData recipes are simple scripts that follow a series of steps, just like a recipe.\nThis chapter demonstrates how data verbs and pipe operators can be used to write recipes to generate interesting insights.\nTo demonstrate the idea, we will use a dataset of US Baby Names released by the Social Security Administration. This version was downloaded by Ryan Burge and posted on Kaggle. I’ve re-posted it on GitHub so it can be read directly into R easily:\n\nURL &lt;- \"https://github.com/DS4PS/Data-Science-Class/blob/master/DATA/BabyNames.rds?raw=true\"\nnames &lt;- readRDS( gzcon( url( URL )))\nnames %&gt;% head() %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\nId\nName\nYear\nGender\nCount\n\n\n\n\n1\nMary\n1880\nF\n7065\n\n\n2\nAnna\n1880\nF\n2604\n\n\n3\nEmma\n1880\nF\n2003\n\n\n4\nElizabeth\n1880\nF\n1939\n\n\n5\nMinnie\n1880\nF\n1746\n\n\n6\nMargaret\n1880\nF\n1578\n\n\n\n\n\nLet’s start by building a recipe to identify the top 10 male names for Baby Boomers.\n\nCreate a subset of data for men born between 1946 and 1964.\nSort by the annual count of each name in the subset.\nKeep only the most popular year for each name.\nIdentify the top 10 most popular during this period.\nPrint the results in a nice table that includes name and peak year data.\n\nThe recipe will look something like this:\n\nnames %&gt;% \n  filter( Gender ==\"M\" & Year &gt;= 1946 & Year &lt;= 1964 ) %&gt;%\n  arrange( desc( Count ) ) %&gt;%\n  distinct( Name, .keep_all=T ) %&gt;%\n  top_n( 10, Count ) %&gt;%\n  select( Name, Year, Count ) %&gt;%\n  pander()\n\n\n\n\n\n\n\n\n\nName\nYear\nCount\n\n\n\n\nJames\n1947\n94755\n\n\nMichael\n1957\n92709\n\n\nRobert\n1947\n91642\n\n\nJohn\n1947\n88318\n\n\nDavid\n1955\n86191\n\n\nWilliam\n1947\n66969\n\n\nRichard\n1946\n58859\n\n\nMark\n1960\n58735\n\n\nThomas\n1952\n48617\n\n\nCharles\n1947\n40773\n\n\n\n\n\nThere are many ways to construct a data recipe. We could have alternatively taken this approach:\n\nCreate a subset of data for men born between 1946 and 1964.\nCount the total numer of men given each name during the period.\nFind the top 10 most popular names.\n\n\nnames %&gt;% \n  filter( Gender ==\"M\" & Year &gt;= 1946 & Year &lt;= 1964 ) %&gt;%\n  group_by( Name ) %&gt;%\n  dplyr::summarize( total=sum(Count) ) %&gt;%\n  dplyr::arrange( desc(total) ) %&gt;%\n  slice( 1:10 ) %&gt;%\n  pander()\n\n\n\n\n\n\n\n\nName\ntotal\n\n\n\n\nJames\n1570607\n\n\nRobert\n1530527\n\n\nJohn\n1524619\n\n\nMichael\n1463911\n\n\nDavid\n1395499\n\n\nWilliam\n1072303\n\n\nRichard\n959321\n\n\nThomas\n810160\n\n\nMark\n684159\n\n\nCharles\n657780\n\n\n\n\n\nWe can see that these two approaches to answering our question give us slightly different results, but are pretty close.\nLet’s try to identify when specific female names have peaked.\n\nCreate a subset of data for women.\nGroup the data by “Name” so we can analyze each name separately.\nFind the year with the highest count for each name.\nStore this data as “peak.years”.\n\nEach name will occur once in this dataset in the year that it experienced it’s peak popularity.\n\npeak.years &lt;- \n  names %&gt;%\n  filter( Gender == \"F\" ) %&gt;%\n  group_by( Name ) %&gt;%\n  top_n( 1, Count ) %&gt;% \n  ungroup() \n\npeak.years %&gt;% head( 5 ) %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\nId\nName\nYear\nGender\nCount\n\n\n\n\n568\nManerva\n1880\nF\n10\n\n\n720\nNeppie\n1880\nF\n7\n\n\n2621\nZilpah\n1881\nF\n9\n\n\n4625\nCrete\n1882\nF\n8\n\n\n4750\nAlwina\n1882\nF\n6\n\n\n\n\n\nWe can then filter by years to see which names peaked in a given period.\n\nfilter( peak.years, Year == 1950 ) %&gt;% \n  arrange( desc( Count ) ) %&gt;%\n  slice( 1:5 ) %&gt;%\n  pander()\n\n\n\n\n\n\n\n\n\n\n\nId\nName\nYear\nGender\nCount\n\n\n\n\n462006\nConstance\n1950\nF\n4442\n\n\n462008\nGlenda\n1950\nF\n4213\n\n\n462103\nBonita\n1950\nF\n1527\n\n\n462301\nIlene\n1950\nF\n453\n\n\n462305\nMarta\n1950\nF\n445\n\n\n\n\n\n\n# library( ggvis )\nnames %&gt;%\n  filter( Name == \"Constance\" & Gender == \"F\" ) %&gt;%\n  select (Name, Year, Count) %&gt;%\n  ggvis( ~Year, ~Count, stroke = ~Name ) %&gt;%\n  layer_lines()\n\n\n\n\n\n\n\n\nRenderer: \nSVG\n | \nCanvas\n\n\nDownload\n\n\n\n\n\n\n\n\n\ntop.five.1920 &lt;- \n  filter( peak.years, Year == 1920 ) %&gt;% \n  top_n( 5, Count ) \n\ntop.five.1920\n\n# A tibble: 5 × 5\n      Id Name     Year Gender Count\n   &lt;int&gt; &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;  &lt;int&gt;\n1 169464 Ruth     1920 F      26100\n2 169465 Mildred  1920 F      18058\n3 169472 Marie    1920 F      12745\n4 169477 Lillian  1920 F      10050\n5 169481 Gladys   1920 F       8819\n\n\n\nnames %&gt;%\n  filter( Name %in% top.five.1920$Name & Gender ==\"F\" ) %&gt;%\n  select (Name, Year, Count) %&gt;%\n  ggvis( ~Year, ~Count, stroke = ~Name ) %&gt;%\n  layer_lines()\n\n\n\n\n\n\n\n\nRenderer: \nSVG\n | \nCanvas\n\n\nDownload\n\n\n\n\n\n\n\n\n\ntop.five.1975 &lt;- \n  filter( peak.years, Year == 1975 ) %&gt;% \n  top_n( 5, Count ) \n\nnames %&gt;%\n  filter( Name %in% top.five.1975$Name & Gender ==\"F\" ) %&gt;%\n  select (Name, Year, Count) %&gt;%\n  ggvis( ~Year, ~Count, stroke = ~Name ) %&gt;%\n  layer_lines()\n\n\n\n\n\n\n\n\nRenderer: \nSVG\n | \nCanvas\n\n\nDownload\n\n\n\n\n\n\n\n\n\ntop.five.2000 &lt;- \n  filter( peak.years, Year == 2000 ) %&gt;% \n  top_n( 5, Count ) \n\nnames %&gt;%\n  filter( Name %in% top.five.2000$Name & Gender ==\"F\" ) %&gt;%\n  select (Name, Year, Count) %&gt;%\n  ggvis( ~Year, ~Count, stroke = ~Name ) %&gt;%\n  layer_lines()\n\n\n\n\n\n\n\n\nRenderer: \nSVG\n | \nCanvas\n\n\nDownload\n\n\n\n\n\n\n\n\nRyan Burge posted a fun project on Kaggle about how to find hipster names using this historical data. He defines hipster names as those meeting the following criteria:\n\nThey were popular when your grandmother was young.\nThey were unpopular when your parents were young.\nThey have recently become popular again.\n\nLet’s stick with women’s names.\n\ndf1 &lt;- filter( names, Gender == \"F\" & Year &gt;= 1915 & Year &lt;= 1935 & Count &gt; 3000 )\ndf2 &lt;- filter( names, Gender == \"F\" & Year == 1980 & Count &lt;= 1000 )\ndf3 &lt;- filter( names, Gender == \"F\" & Year &gt;= 2010 & Count &gt; 2000)\n\nhipster.names &lt;- \n  names %&gt;%\n  filter( Name %in% df1$Name & Name %in% df2$Name & Name %in% df3$Name ) %&gt;%\n  group_by( Name ) %&gt;%\n  dplyr::summarize( total=sum(Count), peak=max(Count) ) %&gt;%\n  arrange( desc( peak ) ) \n\nHere are the top 6 female hipster names:\n\ntop.hipster.names &lt;- \n  c(  \"Emma\",  \"Evelyn\",  \"Alice\",\n      \"Grace\", \"Lillian\", \"Charlotte\"  )\n\nnames %&gt;%\n  filter( Name %in% top.hipster.names & Gender ==\"F\" ) %&gt;%\n  select (Name, Year, Count) %&gt;%\n  ggvis( ~Year, ~Count, stroke = ~Name ) %&gt;%\n  layer_lines()\n\n\n\n\n\n\n\n\nRenderer: \nSVG\n | \nCanvas\n\n\nDownload\n\n\n\n\n\n\n\n\nAnd the full list:\n\nhipster.names %&gt;% pander()\n\n\n\n\n\n\n\n\n\nName\ntotal\npeak\n\n\n\n\nEmma\n595546\n22701\n\n\nEvelyn\n534502\n14279\n\n\nGrace\n469034\n12770\n\n\nAlice\n551034\n11956\n\n\nLillian\n421392\n10050\n\n\nCharlotte\n312022\n10048\n\n\nElla\n273663\n9868\n\n\nJosephine\n297064\n8683\n\n\nEleanor\n268153\n8499\n\n\nRuby\n340143\n8407\n\n\nHazel\n244069\n7615\n\n\nClara\n268980\n5779\n\n\nEva\n252465\n4564\n\n\nLucy\n185064\n4257\n\n\nStella\n155080\n4165\n\n\nViolet\n122984\n4156\n\n\nVivian\n198012\n4128",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Recipes</span>"
    ]
  },
  {
    "objectID": "4_20-data-recipes.html#conclusion",
    "href": "4_20-data-recipes.html#conclusion",
    "title": "15  Data Recipes",
    "section": "15.5 Conclusion",
    "text": "15.5 Conclusion\nThe pipe operator is a little confusing when you first encounter it, but you will find that using data verbs contained in the dplyr package and the pipe operator will speed up your analysis and make your code more readable.\nIn the next chapter we focus more on the use of groups in data science, and the applications of the group_by() function to make your job easier.",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Recipes</span>"
    ]
  },
  {
    "objectID": "4_30-data-joins-core.html",
    "href": "4_30-data-joins-core.html",
    "title": "16  Data Joins",
    "section": "",
    "text": "16.1 Packages Used in This Chapter\nlibrary( pander )\nlibrary( dplyr )\nlibrary( maps )",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Joins</span>"
    ]
  },
  {
    "objectID": "4_30-data-joins-core.html#relational-databases",
    "href": "4_30-data-joins-core.html#relational-databases",
    "title": "16  Data Joins",
    "section": "16.2 Relational Databases",
    "text": "16.2 Relational Databases\nModern databases are huge - think about the amount of information stored at Amazon in the history of each transation, the database where Google logs every single search from every person around the world, or Twitter’s database of all of the tweets (millions each day).\nWhen databases become large, flat spreadsheet style formats are not useful because they create a lot of redundant information, are large to store, and are not efficient to search. Large datasets are instead stored in relational databases - sets of tables that contain unique IDs that allow them to be joined when necessary.\nFor example, consider a simple customer database. We don’t want to store customer info with our transactions because we would be repeating their name and street address every time they make a new purchase. As a result, we store customer information and transaction information separately.\nCustomer Database\n\n\n\n\n\n\n\n\n\n\n\n\nCUSTOMER.ID\nFIRST.NAME\nLAST.NAME\nADDRESS\nZIP.CODE\n\n\n\n\n178\nAlvaro\nJaurez\n123 Park Ave\n57701\n\n\n934\nJanette\nJohnson\n456 Candy Ln\n57701\n\n\n269\nLatisha\nShane\n1600 Penn Ave\n20500\n\n\n\n\n\nTransactions Database\n\n\n\n\n\n\n\n\n\n\nCUSTOMER.ID\nPRODUCT\nPRICE\n\n\n\n\n178\nvideo\n5.38\n\n\n178\nshovel\n12\n\n\n269\nbook\n3.99\n\n\n269\npurse\n8\n\n\n934\nmirror\n7.64\n\n\n\n\n\nIf we want to make the information actionable then we need to combine these datasets. For example, perhaps we want to know the average purchase amount from an individual in the 57701 zip code. We cannot answer that question with either dataset since the zip code is in one dataset, and the price is in another. We need to merge the data.\n\nmerge( customer.info, purchases )   \n\n  CUSTOMER.ID FIRST.NAME LAST.NAME       ADDRESS ZIP.CODE PRODUCT PRICE\n1         178     Alvaro    Jaurez  123 Park Ave    57701   video  5.38\n2         178     Alvaro    Jaurez  123 Park Ave    57701  shovel 12.00\n3         269    Latisha     Shane 1600 Penn Ave    20500    book  3.99\n4         269    Latisha     Shane 1600 Penn Ave    20500   purse  8.00\n5         934    Janette   Johnson  456 Candy Ln    57701  mirror  7.64\n\nfull.dat &lt;- merge( customer.info, purchases ) \n\nfull.dat$PRICE[ full.dat$ZIP.CODE == \"57701\" ]\n\n[1]  5.38 12.00  7.64\n\nmean( full.dat$PRICE[ full.dat$ZIP.CODE == \"57701\" ] )\n\n[1] 8.34\n\n\nIn reality, each purchase would have a purchase ID that is linked to shipping addresses, customer complaints, seller ratings, etc. Each seller would have their own data table with info. Each purchase would be tied to a payment type, which has its own data table. The system gets quite complex, which is why it is important to pay attention to the details of putting the data back together again.\n\n\n\n\n\nExample of a relational database schema\n\n\n\n\nWe will cover a few details of data merges that will help you avoid common and very subtle mistakes that can lead to incorrect inferences.",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Joins</span>"
    ]
  },
  {
    "objectID": "4_30-data-joins-core.html#set-theory",
    "href": "4_30-data-joins-core.html#set-theory",
    "title": "16  Data Joins",
    "section": "16.3 Set Theory",
    "text": "16.3 Set Theory\nIn order to merge data correctly you need to understand some very basic principles of set theory.\n\n16.3.1 Set Theory Functions\nLet’s assume we have two sets: set1=[A,B], set2=[B,C]. Each element in this set represents a group of observations that occurs in the dataset. So B represents people that occur in both datasets, A represents people that occur only in the first dataset, and C represents people that only occur in the second dataset.\nWe can then describe membership through three operations:\n\n\n\n\n\nMembership defined by two sets\n\n\n\n\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nunion: X OR Y\nThe universe of all elements across all both sets: [A,B,C]\n\n\nintersection: X & Y\nThe elements shared by both sets: [B]\n\n\ndifference: X & ! Y\nThe elements in my first set, not in my second [A] or [C]\n\n\n\nLet’s see how this might work in practice with an example of members of a study:\n\n\n\n\n\n\n\n\n\n\nname\ngroup\ngender\n\n\n\n\nfrank\ntreat\nmale\n\n\nwanda\ntreat\nfemale\n\n\nsanjay\ncontrol\nmale\n\n\nnancy\ncontrol\nfemale\n\n\n\n\n\nFor this example let’s define set 1 as the treatment group, and set 2 as all women in the study. Note that set membership is always defined as binary (you are in the set or out), but it can include multiple criteria (the set of animals can contains cats, dogs, and mice).\n\ntreated &lt;- name[ group == \"treat\" ]\n\ntreated \n\n[1] \"frank\" \"wanda\"\n\nfemales &lt;- name[ gender == \"female\" ]\n\nfemales \n\n[1] \"wanda\" \"nancy\"\n\n\nNow we can specify group belonging using some convenient set theory functions: union(), setdiff(), and intersect().\n\nunion( treated, females )\n\n[1] \"frank\" \"wanda\" \"nancy\"\n\nintersect( treated, females )\n\n[1] \"wanda\"\n\nsetdiff( treated, females )\n\n[1] \"frank\"\n\nsetdiff( females, treated )\n\n[1] \"nancy\"\n\n\nIt is very important to note that union() and intersect() are symmetric functions, meaning intersect(x,y) will give you the same result as intersect(y,x). The setdiff() function is not symmetric, however.\n\n\n16.3.2 Set Theory Using Logical Operators\nTypically you will define your groups using logical operators, which perform the exact same funciton as set theory functions but are a little more expressive and flexible.\nLet’s use the same example above where x=“treatment” and y=“female”, then consider these cases:\n\nWho belongs in each group?\n\n\n\n\n\n\n\n\n\n\nname\ngroup\ngender\n\n\n\n\nfrank\ntreat\nmale\n\n\nwanda\ntreat\nfemale\n\n\nsanjay\ncontrol\nmale\n\n\nnancy\ncontrol\nfemale\n\n\n\n\n\n\n#   x\n\nname[ group == \"treat\" ]\n\n[1] \"frank\" \"wanda\"\n\n#   x & y\n\nname[ group == \"treat\" & gender == \"female\" ]\n\n[1] \"wanda\"\n\n#   x & ! y\n\nname[ group == \"treat\" & gender != \"female\" ]\n\n[1] \"frank\"\n\n#  x | y\n\nname[ group == \"treat\" | gender == \"female\" ]\n\n[1] \"frank\" \"wanda\" \"nancy\"\n\n\nWho belongs in these groups?\n\n!x & !y\nx & ! ( x & y )\n( x | y ) & ! ( x & y )",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Joins</span>"
    ]
  },
  {
    "objectID": "4_30-data-joins-core.html#merging-data",
    "href": "4_30-data-joins-core.html#merging-data",
    "title": "16  Data Joins",
    "section": "16.4 Merging Data",
    "text": "16.4 Merging Data\nThe Merge Function\nThe merge function joins two datasets. The function requires two datasets as the arguments, and they need to share a unique ID variable. Recall the example from above:\n\nmerge( customer.info, purchases )\n\n  CUSTOMER.ID FIRST.NAME LAST.NAME       ADDRESS ZIP.CODE PRODUCT PRICE\n1         178     Alvaro    Jaurez  123 Park Ave    57701   video  5.38\n2         178     Alvaro    Jaurez  123 Park Ave    57701  shovel 12.00\n3         269    Latisha     Shane 1600 Penn Ave    20500    book  3.99\n4         269    Latisha     Shane 1600 Penn Ave    20500   purse  8.00\n5         934    Janette   Johnson  456 Candy Ln    57701  mirror  7.64\n\n\nThe important thing to keep in mind is that the default merge operation uses the intersection of the two datasets. It will drop all elements that don’t occur in both datasets. We may want to fine-tune this as to not lose valuable data and potentially bias our analysis. As an example, no illegal immigrants will have social security numbers, so if you are merging using the SSN, you will drop this group from the data, which could impact your results.\n\nWith a little help from the set theory examples above, we can think about which portions of the data we wish to drop and which portions we wish to keep.\n\n\n\n\n\n\n\nArgument\nUsage\n\n\n\n\nall=F\nDEFAULT - new dataset contains intersection of X and Y (B only)\n\n\nall=T\nNew dataset contains union of X and Y (A, B & C)\n\n\nall.x=T\nNew dataset contains A and B, not C\n\n\nall.y=T\nNew dataset contains B and C, not A\n\n\n\nHere is some demonstrations with examples adapted from the R help file.\n\nauthors   \n\n      surname nationality deceased\n1       Tukey          US      yes\n2     Tierney          US       no\n3      Ripley          UK       no\n4      McNeil   Australia       no\n5 Shakespeare     England      yes\n\nbooks    \n\n         name                     title\n1       Tukey Exploratory Data Analysis\n2    Venables Modern Applied Statistics\n3      Ripley        Spatial Statistics\n4      Ripley     Stochastic Simulation\n5      McNeil Interactive Data Analysis\n6 R Core Team      An Introduction to R\n\n# adding books to the author bios dataset  ( set B only )\n\nmerge(authors, books, by.x = \"surname\", by.y = \"name\")    \n\n  surname nationality deceased                     title\n1  McNeil   Australia       no Interactive Data Analysis\n2  Ripley          UK       no        Spatial Statistics\n3  Ripley          UK       no     Stochastic Simulation\n4   Tukey          US      yes Exploratory Data Analysis\n\n# adding author bios to the books dataset  ( set B only )\n\nmerge(books, authors, by.x = \"name\", by.y = \"surname\")    \n\n    name                     title nationality deceased\n1 McNeil Interactive Data Analysis   Australia       no\n2 Ripley        Spatial Statistics          UK       no\n3 Ripley     Stochastic Simulation          UK       no\n4  Tukey Exploratory Data Analysis          US      yes\n\n# keep books without author bios, lose authors without books  ( sets A and B )\n\nmerge( books, authors, by.x = \"name\", by.y = \"surname\", all.x=T )     \n\n         name                     title nationality deceased\n1      McNeil Interactive Data Analysis   Australia       no\n2 R Core Team      An Introduction to R        &lt;NA&gt;     &lt;NA&gt;\n3      Ripley        Spatial Statistics          UK       no\n4      Ripley     Stochastic Simulation          UK       no\n5       Tukey Exploratory Data Analysis          US      yes\n6    Venables Modern Applied Statistics        &lt;NA&gt;     &lt;NA&gt;\n\n# keep authors without book listed, lose books without author bios   ( sets B and C )\n\nmerge( books, authors, by.x = \"name\", by.y = \"surname\", all.y=T )    \n\n         name                     title nationality deceased\n1      McNeil Interactive Data Analysis   Australia       no\n2      Ripley        Spatial Statistics          UK       no\n3      Ripley     Stochastic Simulation          UK       no\n4 Shakespeare                      &lt;NA&gt;     England      yes\n5     Tierney                      &lt;NA&gt;          US       no\n6       Tukey Exploratory Data Analysis          US      yes\n\n# dont' throw out any data   ( sets A and B and C )\n\nmerge( books, authors, by.x = \"name\", by.y = \"surname\", all=T )   \n\n         name                     title nationality deceased\n1      McNeil Interactive Data Analysis   Australia       no\n2 R Core Team      An Introduction to R        &lt;NA&gt;     &lt;NA&gt;\n3      Ripley        Spatial Statistics          UK       no\n4      Ripley     Stochastic Simulation          UK       no\n5 Shakespeare                      &lt;NA&gt;     England      yes\n6     Tierney                      &lt;NA&gt;          US       no\n7       Tukey Exploratory Data Analysis          US      yes\n8    Venables Modern Applied Statistics        &lt;NA&gt;     &lt;NA&gt;\n\n\nAlso note that the order of your datasets in the argument list will impact the inclusion or exclusion of elements.\nmerge( x, y, all=F ) EQUALS merge( y, x, all=F )\nmerge( x, y, all.x=T ) DOES NOT EQUAL merge( y, x, all.x=T )\n\n16.4.1 The by.x and by.y Arguments\nWhen you use the default merge() function without specifying the variables to merge upon, the function will check for common variable names across the two datasets. If there are multiple, it will join the shared variables to create a new unique key. This might be problematic if that was not the intent.\nTake the example of combining fielding and salary data in the Lahman package. If we are not explicit about the merge variable, we may get odd results. Note that they two datasets share four ID variables.\n\nlibrary( Lahman )\ndata( Fielding )\ndata( Salaries )\n\n\nintersect( names(Fielding), names(Salaries) )\n\n[1] \"playerID\" \"yearID\"   \"teamID\"   \"lgID\"    \n\n# merge id\n\nint &lt;- intersect( names(Fielding), names(Salaries) )\n\npaste( int[1],int[2],int[3],int[4], sep=\".\" )\n\n[1] \"playerID.yearID.teamID.lgID\"\n\n\nTo avoid problems, be explicit using the by.x and by.x arguments to control which variable is used for the merge.\n\nhead( merge( Salaries, Fielding ) )\n\n  yearID teamID lgID  playerID salary stint POS  G GS InnOuts  PO  A E DP PB WP\n1   1985    ATL   NL barkele01 870000     1   P 20 18     221   2  9 1  0 NA NA\n2   1985    ATL   NL bedrost01 550000     1   P 37 37     620  13 23 4  3 NA NA\n3   1985    ATL   NL benedbr01 545000     1   C 70 67    1698 314 35 4  1  1 NA\n4   1985    ATL   NL  campri01 633333     1   P 66  2     383   7 13 4  3 NA NA\n5   1985    ATL   NL ceronri01 625000     1   C 91 76    2097 384 48 6  4  6 NA\n6   1985    ATL   NL chambch01 800000     1  1B 39 27     814 299 25 1 31 NA NA\n  SB CS ZR\n1 NA NA NA\n2 NA NA NA\n3 65 24 NA\n4 NA NA NA\n5 69 29 NA\n6 NA NA NA\n\nhead( merge( Salaries, Fielding, by.x=\"playerID\", by.y=\"playerID\" ) )\n\n   playerID yearID.x teamID.x lgID.x  salary yearID.y stint teamID.y lgID.y POS\n1 aardsda01     2010      SEA     AL 2750000     2009     1      SEA     AL   P\n2 aardsda01     2010      SEA     AL 2750000     2007     1      CHA     AL   P\n3 aardsda01     2010      SEA     AL 2750000     2015     1      ATL     NL   P\n4 aardsda01     2010      SEA     AL 2750000     2008     1      BOS     AL   P\n5 aardsda01     2010      SEA     AL 2750000     2012     1      NYA     AL   P\n6 aardsda01     2010      SEA     AL 2750000     2004     1      SFN     NL   P\n   G GS InnOuts PO A E DP PB WP SB CS ZR\n1 73  0     214  2 5 0  1 NA NA NA NA NA\n2 25  0      97  2 4 1  0 NA NA NA NA NA\n3 33  0      92  0 1 1  0 NA NA NA NA NA\n4 47  0     146  3 6 0  0 NA NA NA NA NA\n5  1  0       3  0 0 0  0 NA NA NA NA NA\n6 11  0      32  0 0 0  0 NA NA NA NA NA",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Joins</span>"
    ]
  },
  {
    "objectID": "4_30-data-joins-core.html#non-unique-observations-in-id-variables",
    "href": "4_30-data-joins-core.html#non-unique-observations-in-id-variables",
    "title": "16  Data Joins",
    "section": "16.5 Non-Unique Observations in ID Variables",
    "text": "16.5 Non-Unique Observations in ID Variables\nIn some rare instances, you will need to merge to datasets that have non-singular elements in the unique key ID variables, meaning each observation / individual appears more than one time in the data. Note that in this case, for each occurance of an observation / individual in your X dataset, you will merge once with each occurance of the same observation / individual in the Y dataset. The result will be a multiplicative expansion of the size of your dataset.\nFor example, if John appears on four separate rows of X, and three seperate rows of Y, the new dataset will contain 12 rows of John (4 x 3 = 12).\ndataset X contains four separate instances of an individual [ X1, X2, X3, X4 ]\ndataset Y contains three separate instances of an individual [ Y1, Y2, Y3 ]\nAfter the merge we have one row for each pair:\nX1-Y1\nX1-Y2\nX1-Y3\nX2-Y1\nX2-Y2\nX2-Y3\nX3-Y1\nX3-Y2\nX3-Y3\nX4-Y1\nX4-Y2\nX4-Y3\nFor example, perhaps a sales company has a database that keeps track of biographical data, and sales performance. Perhaps we want to see if there is peak age for sales performance. We need to merge these datasets.\n\nbio &lt;- data.frame( name=c(\"John\",\"John\",\"John\"),\n                   year=c(2000,2001,2002),\n                   age=c(43,44,45) )\n\nperformance &lt;- data.frame( name=c(\"John\",\"John\",\"John\"),\n                           year=c(2000,2001,2002),\n                           sales=c(\"15k\",\"20k\",\"17k\") )\n\n# correct merge\n\nmerge( bio, performance, by.x=c(\"name\",\"year\"), by.y=c(\"name\",\"year\") ) \n\n  name year age sales\n1 John 2000  43   15k\n2 John 2001  44   20k\n3 John 2002  45   17k\n\n# incorrect merge\n\nmerge( bio, performance, by.x=c(\"name\"), by.y=c(\"name\") )  \n\n  name year.x age year.y sales\n1 John   2000  43   2000   15k\n2 John   2000  43   2001   20k\n3 John   2000  43   2002   17k\n4 John   2001  44   2000   15k\n5 John   2001  44   2001   20k\n6 John   2001  44   2002   17k\n7 John   2002  45   2000   15k\n8 John   2002  45   2001   20k\n9 John   2002  45   2002   17k\n\n\nIt is good practice to check the size (number of rows) of your dataset before and after a merge. If it has expanded, chances are you either used the wrong unique IDs, or your dataset contains duplicates.\n\n16.5.1 Example of Incorrect Merge\nHere is a tangible example using the Lahman baseball dataset. Perhaps we want to examine the relationship between fielding position and salary. The Fielding dataset contains fielding position information, and the Salaries dataset contains salary information. We can merge these two datasets using the playerID field.\nIf we are not thoughtful about this, however, we will end up causing problems. Let’s look at an example using Kirby Pucket.\n\nkirby.fielding &lt;- Fielding[ Fielding$playerID == \"puckeki01\" , ]\n\nhead( kirby.fielding )\n\n       playerID yearID stint teamID lgID POS   G  GS InnOuts  PO  A E DP PB WP\n83868 puckeki01   1984     1    MIN   AL  OF 128 128    3377 438 16 3  4 NA NA\n85177 puckeki01   1985     1    MIN   AL  OF 161 160    4213 465 19 8  5 NA NA\n86509 puckeki01   1986     1    MIN   AL  OF 160 157    4155 429  8 6  3 NA NA\n87916 puckeki01   1987     1    MIN   AL  OF 147 147    3820 341  8 5  2 NA NA\n89284 puckeki01   1988     1    MIN   AL  OF 158 157    4049 450 12 3  4 NA NA\n90705 puckeki01   1989     1    MIN   AL  OF 157 154    3985 438 13 4  3 NA NA\n      SB CS ZR\n83868 NA NA NA\n85177 NA NA NA\n86509 NA NA NA\n87916 NA NA NA\n89284 NA NA NA\n90705 NA NA NA\n\nnrow( kirby.fielding )\n\n[1] 21\n\nkirby.salary &lt;- Salaries[ Salaries$playerID == \"puckeki01\" , ]\n\nhead( kirby.salary )\n\n     yearID teamID lgID  playerID  salary\n280    1985    MIN   AL puckeki01  130000\n917    1986    MIN   AL puckeki01  255000\n1610   1987    MIN   AL puckeki01  465000\n2244   1988    MIN   AL puckeki01 1090000\n2922   1989    MIN   AL puckeki01 2000000\n3717   1990    MIN   AL puckeki01 2816667\n\nnrow( kirby.salary )\n\n[1] 13\n\nkirby.field.salary &lt;- merge( kirby.fielding, kirby.salary, by.x=\"playerID\", by.y=\"playerID\" )\n\nhead( select( kirby.field.salary, yearID.x, yearID.y,   POS,    G,  GS, salary ) )\n\n  yearID.x yearID.y POS   G  GS  salary\n1     1984     1985  OF 128 128  130000\n2     1984     1986  OF 128 128  255000\n3     1984     1987  OF 128 128  465000\n4     1984     1988  OF 128 128 1090000\n5     1984     1989  OF 128 128 2000000\n6     1984     1990  OF 128 128 2816667\n\nnrow( kirby.field.salary )\n\n[1] 273\n\n21*13\n\n[1] 273\n\n\nWhat we have done here is taken each year of fielding data, and matched it to every year of salary data. We can see that we have 21 fielding observations and 13 years of salary data, so our resulting dataset is 273 observation pairs.\nThis merge also makes it difficult to answer the question of the relationship between fielding position and salary if players change positions over time.\nThe correct merge in this case would be a merge on a playerID-yearID pair. We can create a unique key by combining playerID and yearID using paste():\n\nhead( paste( kirby.fielding$playerID, kirby.fielding$yearID, sep=\".\") )\n\n[1] \"puckeki01.1984\" \"puckeki01.1985\" \"puckeki01.1986\" \"puckeki01.1987\"\n[5] \"puckeki01.1988\" \"puckeki01.1989\"\n\n\nBut there is a simple solution as the merge function also allows for multiple variables to be used for a merge() command.\n\nkirby.field.salary &lt;- merge( kirby.fielding, kirby.salary, \n                            by.x=c(\"playerID\",\"yearID\"), \n                            by.y=c(\"playerID\",\"yearID\")   )\n\nnrow( kirby.field.salary )\n\n[1] 20",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Joins</span>"
    ]
  },
  {
    "objectID": "4_30-data-joins-core.html#the-in-function",
    "href": "4_30-data-joins-core.html#the-in-function",
    "title": "16  Data Joins",
    "section": "16.6 The %in% function",
    "text": "16.6 The %in% function\nSince we are talking about intersections and matches, I want to briefly introduce the %in% function. It is a combination of the two.\nThe intersect() function returns a list of unique matches between two vectors.\n\ndata(Salaries)\ndata(Fielding)\nintersect( names(Salaries), names(Fielding) )\n\n[1] \"yearID\"   \"teamID\"   \"lgID\"     \"playerID\"\n\n\nThe match() function returns the position of matched elements.\n\nx &lt;- c(\"A\",\"B\",\"C\",\"B\")\n\ny &lt;- c(\"B\",\"D\",\"A\",\"F\")\n\nmatch( x, y )\n\n[1]  3  1 NA  1\n\n\nThe %in% function returns a logical vector, where TRUE signifies that the element in y also occurs in x. In other words, does a specific element in y belong to the intersection of x,y.\nThis is very useful for creating subsets of data that belong to both sets.\n\nx &lt;- c(\"A\",\"B\",\"C\")\n\ny &lt;- c(\"B\",\"D\",\"A\",\"B\",\"F\",\"B\")\n\ny %in% x # does each element of y occur anywhere in x?\n\n[1]  TRUE FALSE  TRUE  TRUE FALSE  TRUE\n\ny[ y %in% x] # keep only data that occurs in both\n\n[1] \"B\" \"A\" \"B\" \"B\"",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Joins</span>"
    ]
  },
  {
    "objectID": "4_30-data-joins-core.html#the-match-function",
    "href": "4_30-data-joins-core.html#the-match-function",
    "title": "16  Data Joins",
    "section": "16.7 The Match Function",
    "text": "16.7 The Match Function\nOften times we do not need to merge data, we may just need sort data in one dataset so that it matches the order of another dataset. This is accomplished using the match() function.\nNote that we can rearrange the order of a dataset by referencing the desired position.\n\nx &lt;- c(\"Second\",\"Third\",\"First\")\n\nx\n\n[1] \"Second\" \"Third\"  \"First\" \n\nx[ c(3,1,2) ]\n\n[1] \"First\"  \"Second\" \"Third\" \n\n\nThe match() function returns the positions of matches of its first vector to the second vector listed in the arguments. Or in other words, the order that vector 2 would need to follow to match vector 1.\n\nx &lt;- c(\"A\",\"B\",\"C\")\n\ny &lt;- c(\"B\",\"D\",\"A\")\n\ncbind( x, y )\n\n     x   y  \n[1,] \"A\" \"B\"\n[2,] \"B\" \"D\"\n[3,] \"C\" \"A\"\n\nmatch( x, y )\n\n[1]  3  1 NA\n\nmatch( y, x) # not a symmetric operation!\n\n[1]  2 NA  1\n\n# In the y vector:\n#\n#  [3]=A\n#  [1]=B\n# [NA]=D (no match)\n\norder.y &lt;- match( x, y )\n\ny[ order.y ]\n\n[1] \"A\" \"B\" NA \n\n\nWe can see that match() returns the correct order to put y in so that it matches the order of x. In the re-ordered vector, the first element is the original third element A, the second element is the original first element B, and there is no third element because D did not match anything in x.\nNote the order of arguments in the function:\n\nmatch( data I want to match to , data I need to re-order )\n\nWe can use this position information to re-order y as follows:\n\nx &lt;- sample( LETTERS[1:15], size=10 )\n\ny &lt;- sample( LETTERS[1:15], size=10 )\n\ncbind( x, y )\n\n      x   y  \n [1,] \"K\" \"J\"\n [2,] \"F\" \"G\"\n [3,] \"G\" \"I\"\n [4,] \"E\" \"B\"\n [5,] \"A\" \"E\"\n [6,] \"I\" \"L\"\n [7,] \"B\" \"H\"\n [8,] \"O\" \"O\"\n [9,] \"J\" \"M\"\n[10,] \"N\" \"N\"\n\norder.y &lt;- match( x, y )\n\ny.new &lt;- y[ order.y ]\n\ncbind( x, y.new )\n\n      x   y.new\n [1,] \"K\" NA   \n [2,] \"F\" NA   \n [3,] \"G\" \"G\"  \n [4,] \"E\" \"E\"  \n [5,] \"A\" NA   \n [6,] \"I\" \"I\"  \n [7,] \"B\" \"B\"  \n [8,] \"O\" \"O\"  \n [9,] \"J\" \"J\"  \n[10,] \"N\" \"N\"  \n\n# Note the result if you confuse the order or arguments\n\norder.y &lt;- match( y, x )\n\ny.new &lt;- y[ order.y ]\n\ncbind( x, y.new )\n\n      x   y.new\n [1,] \"K\" \"M\"  \n [2,] \"F\" \"I\"  \n [3,] \"G\" \"L\"  \n [4,] \"E\" \"H\"  \n [5,] \"A\" \"B\"  \n [6,] \"I\" NA   \n [7,] \"B\" NA   \n [8,] \"O\" \"O\"  \n [9,] \"J\" NA   \n[10,] \"N\" \"N\"  \n\n\nThis comes in handy when we are matching information between two tables. For example, in GIS the map regions follow a specific order but your data does not. Create a color scheme for levels of your data, and then re-order the colors so they match the correct region on the map. In this example, we will look at unemployment levels by county.\n\nlibrary( maps )\ndata( county.fips )\ndata( unemp )\n\nmap( database=\"county\" )\n\n\n\n\n\n\n\n# assign a color to each level of unemployment, red = high, gray = medium, blue = low\n\ncolor.function &lt;- colorRampPalette( c(\"steelblue\", \"gray70\", \"firebrick\") )\n\n\ncolor.vector &lt;- cut( rank(unemp$unemp), breaks=7, labels=color.function( 7 ) )\n\ncolor.vector &lt;- as.character( color.vector )\n\nhead( color.vector )\n\n[1] \"#B28282\" \"#B28282\" \"#B22222\" \"#B25252\" \"#B28282\" \"#B22222\"\n\n# doesn't look quite right\n\nmap( database=\"county\", col=color.vector, fill=T, lty=0 )\n\n\n\n\n\n\n\n# what went wrong here? \n\n# our unemployment data (and thus the color vector) follows a different order\n\ncbind( map.id=county.fips$fips, data.id=unemp$fips, color.vector )[ 2500:2510 , ]\n\n      map.id  data.id color.vector\n [1,] \"48011\" \"47149\" \"#B28282\"   \n [2,] \"48013\" \"47151\" \"#B22222\"   \n [3,] \"48015\" \"47153\" \"#B22222\"   \n [4,] \"48017\" \"47155\" \"#B28282\"   \n [5,] \"48019\" \"47157\" \"#B28282\"   \n [6,] \"48021\" \"47159\" \"#B22222\"   \n [7,] \"48023\" \"47161\" \"#B25252\"   \n [8,] \"48025\" \"47163\" \"#B3B3B3\"   \n [9,] \"48027\" \"47165\" \"#B28282\"   \n[10,] \"48029\" \"47167\" \"#B25252\"   \n[11,] \"48031\" \"47169\" \"#B25252\"   \n\n# place the color vector in the correct order\n\nthis.order &lt;- match( county.fips$fips, unemp$fips )\n\ncolor.vec.ordered &lt;- color.vector[ this.order ]\n\n# colors now match their correct counties\n\nmap( database=\"county\", col=color.vec.ordered, fill=T, lty=0 )\ntitle( main=\"Unemployment Levels by County in 2009\")\n\n\n\n\n\n\n\n\nNote that elements can be recycled from your y vector:\n\nx &lt;- c(\"A\",\"B\",\"C\",\"B\")\n\ny &lt;- c(\"B\",\"D\",\"A\",\"F\")\n\ncbind( x, y )\n\n     x   y  \n[1,] \"A\" \"B\"\n[2,] \"B\" \"D\"\n[3,] \"C\" \"A\"\n[4,] \"B\" \"F\"\n\nmatch( x, y )\n\n[1]  3  1 NA  1\n\norder.y &lt;- match( x, y )\n\ny.new &lt;- y[ order.y ]\n\ncbind( x, y.new )\n\n     x   y.new\n[1,] \"A\" \"A\"  \n[2,] \"B\" \"B\"  \n[3,] \"C\" NA   \n[4,] \"B\" \"B\"",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Joins</span>"
    ]
  },
  {
    "objectID": "4_40-data-joins-dplyr.html",
    "href": "4_40-data-joins-dplyr.html",
    "title": "18  Data Joins With dplyr",
    "section": "",
    "text": "19 Overview of Joins",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Joins With dplyr</span>"
    ]
  },
  {
    "objectID": "4_40-data-joins-dplyr.html#inner-outer-left-and-right",
    "href": "4_40-data-joins-dplyr.html#inner-outer-left-and-right",
    "title": "18  Data Joins With dplyr",
    "section": "19.1 Inner, Outer, Left and Right",
    "text": "19.1 Inner, Outer, Left and Right\nThere are two things to remember when conducting joins. The first is that when you merge two datasets it is rare for them both to contain the exact same observations. As a result, you need to make decisions about which data to keep and which data to drop post-merge. There are four options:\n\nKeep everything: “Outer Join”\nKeep only observations in both datasets: “Inner Join”\nKeep all observations from dat1: “Left Join”\nKeep all observations from dat2: “Right Join”\n\n\n\n\n\n\n\n\n\n\n\n\nThese options are specified through the all=, all.x=, and all.y= arguments in the merge() function, where the arguments all.x stands for observations in dat1, and all.y stands for observations in dat2 in this example.",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Joins With dplyr</span>"
    ]
  },
  {
    "objectID": "4_40-data-joins-dplyr.html#compound-ids",
    "href": "4_40-data-joins-dplyr.html#compound-ids",
    "title": "18  Data Joins With dplyr",
    "section": "19.2 Compound IDs",
    "text": "19.2 Compound IDs\nIt is often the case where a single ID does not uniquely specify observations needed to join data. For example, if you are looking at the relationship between employee eye color and their height, these are both variables that can only have one value per employee, so the employee ID would be sufficient to merge the eye color and height datasets.\nIf we want to look at the relationship between employee sick days in a given year and their performance, we now might have multiple years of data for each employee. To merge these two datasets we need to use both ID and YEAR. This is an example of a compound ID - two or more variables are needed to create a unique key for the join.\n\n\n\n\n\n\n\n\n\nConsider an example where sales representatives get a bonus if they sell over $100,000 in subscriptions each year. We have one database generated by the sales department, and another generated by HR. We want to merge them to ensure bonuses have been properly issued. The data tables are structured as follows:\n\n\n\n\n\n\n\n\n\n\nid\nyear\nsales\n\n\n\n\nA\n2001\n54000\n\n\nA\n2002\n119000\n\n\nB\n2001\n141000\n\n\nB\n2002\n102000\n\n\nC\n2001\n66000\n\n\nC\n2002\n68000\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nyear\nbonus\n\n\n\n\nA\n2001\nFALSE\n\n\nA\n2002\nTRUE\n\n\nB\n2001\nTRUE\n\n\nB\n2002\nTRUE\n\n\nC\n2001\nFALSE\n\n\nC\n2002\nFALSE\n\n\n\n\n\nThe RIGHT way to merge these tables is to specify the set of IDs that allow you to identify unique observations. The employee ID is not sufficient in this case because there are separate observations for each year. As a result:\n\nmerge( dat1, dat2, by=c(\"id\",\"year\") )         %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\nid\nyear\nsales\nbonus\n\n\n\n\nA\n2001\n54000\nFALSE\n\n\nA\n2002\n119000\nTRUE\n\n\nB\n2001\n141000\nTRUE\n\n\nB\n2002\n102000\nTRUE\n\n\nC\n2001\n66000\nFALSE\n\n\nC\n2002\n68000\nFALSE\n\n\n\n\n\nThe WRONG way to merge these two datasets is to use only the employee ID. In this case, since the rows are now no longer unique, the only choice the merge() function has is to join EVERY instance on the right to each instance on the left. It has the effect of blowing up the size of the database (notice we have increased from 6 to 12 rows), as well as duplicating fields and incorrectly aligning data.\nRow number 2, for example, reports that employee A received a bonus on $54,000 in sales.\n\nmerge( dat1, dat2, by=\"id\" )         %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\nid\nyear.x\nsales\nyear.y\nbonus\n\n\n\n\nA\n2001\n54000\n2001\nFALSE\n\n\nA\n2001\n54000\n2002\nTRUE\n\n\nA\n2002\n119000\n2001\nFALSE\n\n\nA\n2002\n119000\n2002\nTRUE\n\n\nB\n2001\n141000\n2001\nTRUE\n\n\nB\n2001\n141000\n2002\nTRUE\n\n\nB\n2002\n102000\n2001\nTRUE\n\n\nB\n2002\n102000\n2002\nTRUE\n\n\nC\n2001\n66000\n2001\nFALSE\n\n\nC\n2001\n66000\n2002\nFALSE\n\n\nC\n2002\n68000\n2001\nFALSE\n\n\nC\n2002\n68000\n2002\nFALSE\n\n\n\n\n\nThe solution is to ensure you are using the combination of keys that ensures each observation is correctly specified.",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Joins With dplyr</span>"
    ]
  },
  {
    "objectID": "4_40-data-joins-dplyr.html#merge-keys",
    "href": "4_40-data-joins-dplyr.html#merge-keys",
    "title": "18  Data Joins With dplyr",
    "section": "19.3 Merge Keys",
    "text": "19.3 Merge Keys\nYour “keys” are the shared IDs across your datasets used for the join. You can check for shared variable names by looking at the intersection of names in both:\n\nintersect( names(dat1), names(dat2) )\n\n[1] \"id\"   \"year\"\n\n\nThis works when the datasets were generated from a common system that uses standardized and identical ID names. In other cases, the same key may be spelled differently (‘year’ vs. ‘YEAR’) or have completely different names.\nThe check above at least allows you to catch instances where variable names would be repeated, and thus duplicated in the merged file. When this happens, like the ‘year’ variable in the example above, the merge operation will add an ‘x’ and ‘y’ to the end of the variable names to specify which dataset they originated from (‘year.x’ and ‘year.y’ in the example above).\nIn instances where variable names differ, you can specify the names directly using “by.x=” and “by.y=”:\n\nmerge( dat1, dat2, by.x=\"fips\", by.y=\"FIPS\" )",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Joins With dplyr</span>"
    ]
  },
  {
    "objectID": "4_40-data-joins-dplyr.html#in-summary",
    "href": "4_40-data-joins-dplyr.html#in-summary",
    "title": "18  Data Joins With dplyr",
    "section": "19.4 In Summary",
    "text": "19.4 In Summary\nYou will be using the merge() function in this lab to join datasets. You need to specify two arguments in each merge call.\n\nmerge( dat1, dat2, by=\"\", all=\"\" )\n\nYour IDs used to join dataset:\n\nUse “by=” when variable names are identical\nUse “by.x=” and “by.y=” when the variable names are spelled differently\nRemember the c() when you have a compound key: by=c(“FIPS”,“YEAR”)\n\nSpecify an outer, inner, left, or right join:\n\nall=TRUE creates an outer join\nall=FALSE creates an inner join\nall.x=TRUE creates a left join\nall.y=TRUE creates a right join",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Data Joins With dplyr</span>"
    ]
  },
  {
    "objectID": "5_10-constructing-groups.html",
    "href": "5_10-constructing-groups.html",
    "title": "18  Constructing Groups",
    "section": "",
    "text": "18.1 Key Concepts\nWe translate questions from plain English into computer code using logical statements.\nLogical statements generate logical vectors or “selector vectors” where:\nWe use these vectors to:",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Constructing Groups</span>"
    ]
  },
  {
    "objectID": "5_10-constructing-groups.html#key-concepts",
    "href": "5_10-constructing-groups.html#key-concepts",
    "title": "18  Constructing Groups",
    "section": "",
    "text": "TRUE signifies an observation belongs to our defined group\nFALSE signifies an observation does not\n\n\n\ncount group membership\ndetermine the proportion of the population belonging to the group\ncreate subsets of data belonging to the group",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Constructing Groups</span>"
    ]
  },
  {
    "objectID": "5_10-constructing-groups.html#logical-operators",
    "href": "5_10-constructing-groups.html#logical-operators",
    "title": "18  Constructing Groups",
    "section": "18.2 Logical Operators",
    "text": "18.2 Logical Operators\nSimilar to mathematical operators, logical operators are a basic building block of data programming. Most often when working with data we are not creating complicated statistical models. We are identifying members of a group (print all of the females from the study), and describing a subset of the data (compare the average price of houses with a pool to houses without a pool).\nIn order to accomplish these simple tasks we need to use logic statements. A logic statement answers the question, does an observation belong to our group?\nMany times groups are simple: identify all professions that have average salaries over $100k a year, for example.\nGroups can be complex: identify the African American children from a specific zip code in Chicago that live in households with single mothers.\nIn this chapter we will use nine basic logical operators:\n\n\n\nOperator\nDescription\n\n\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to\n\n\nx | y\nx OR y\n\n\nx & y\nx AND y\n\n\n!\nopposite of\n\n\n[ ]\nsubset\n\n\n\nLogical operators create logical vectors, a vector that contains only TRUE or FALSE. The TRUE means that the observation belongs to a group, FALSE means it does not.\n\nx1 &lt;- c( 7, 9, 1, 2 )\nx1 &gt; 5\n\n\n\nTRUE, TRUE, FALSE and FALSE\n\n\n\ngender &lt;- c(\"male\",\"male\",\"female\",\"female\")\ngender == \"female\"\n\n\n\nFALSE, FALSE, TRUE and TRUE\n\n\n \n\nTry it yourself:\n\n\ndc_light_exercise_ex-01\n\nNote that the logical statement for “equals” is written with two equal signs. This is important to remember, because using a single equal sign can introduce subtle errors into your analysis.\n\nx1 &lt;- c( 7, 9, 1, 2 )\nx1 == 9\n\n[1] FALSE  TRUE FALSE FALSE\n\nx1 = 9  # single equal overwrites your data\nx1\n\n[1] 9\n\n\nWe can write compound logical statements using the AND & and OR | operators:\n\n\n\n\n\n\n\n\n\n\nid\ngender\nstudy.group\n\n\n\n\n1\nmale\ntreatment\n\n\n2\nmale\ncontrol\n\n\n3\nfemale\ntreatment\n\n\n4\nfemale\ncontrol\n\n\n\n\n\n\ngender == \"female\"  &  study.group == \"treatment\"\n\n\n\nFALSE, FALSE, TRUE and FALSE\n\n\n\ngender == \"female\"  |  study.group == \"treatment\"\n\n\n\nTRUE, FALSE, TRUE and TRUE",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Constructing Groups</span>"
    ]
  },
  {
    "objectID": "5_10-constructing-groups.html#selector-vectors",
    "href": "5_10-constructing-groups.html#selector-vectors",
    "title": "18  Constructing Groups",
    "section": "18.3 Selector Vectors",
    "text": "18.3 Selector Vectors\nNote that we use operators to create logical vectors where TRUE designates observations that belong to the defined group, and FALSE designates observations outside the group. The term “selector vector” is a useful way to remember this purpose.\nAfter you have defined a group by composing a logical statement, then the vector can be used to count group members and take subsets of other variables to calculate group statistics.\n\n\n\n\n\n\n\n\n\n\n\nname\ngender\ngroup\nstrength\n\n\n\n\nfrank\nmale\ntreat\n27\n\n\nwanda\nfemale\ntreat\n43\n\n\nsanjay\nmale\ncontrol\n19\n\n\nnancy\nfemale\ncontrol\n58\n\n\n\n\n\n\nthese.female &lt;- dat$gender == \"female\"\n\nsum( these.female )                   # number of women in the study\n\n[1] 2\n\nmean( these.female )                  # proportion of the study that is women\n\n[1] 0.5\n\ndat[ these.female , ]                 # all data belonging to women\n\n   name gender   group strength\n2 wanda female   treat       43\n4 nancy female control       58\n\nmean( dat$strength[ these.female ] )  # average outcome for women in the study\n\n[1] 50.5\n\n\nI will consistently name my logical vectors “these.GROUP” throughout the chapters, where GROUP represents the group label. For example, I selected women above, so the selector vector is called “these.female”.",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Constructing Groups</span>"
    ]
  },
  {
    "objectID": "5_10-constructing-groups.html#usefulness-of-selector-vectors",
    "href": "5_10-constructing-groups.html#usefulness-of-selector-vectors",
    "title": "18  Constructing Groups",
    "section": "18.4 Usefulness of Selector Vectors",
    "text": "18.4 Usefulness of Selector Vectors\nSelector vectors, i.e. logical vectors that were created by defining a group, have three main uses in our analysis.\nONE: Logical vectors give us an easy way to count things within defined groups.\nWe can apply a sum() function to a logical vector, and the result will be a tally of all of the TRUE cases. The mean() function will give us the proportion of the sample that belongs to our defined group.\n\n# how many females do we have in our study?\n\nsum( gender == \"female\" )\n\n[1] 2\n\n# how many females do we have in our treatment group?\n\nsum( gender == \"female\" & group == \"treat\" )\n\n[1] 1\n\n# what proportion of our study are men? \n\nmean( gender == \"male\" )\n\n[1] 0.5\n\n\nTWO: We can create a selector variable that is used for subsets. A selector vector used in a subset operator will drop all observations that are FALSE, isolating data belonging to the group:\n\nthese.female &lt;- gender == \"female\"\nname[ these.female ]\n\n[1] \"wanda\" \"nancy\"\n\nstrength[ these.female ]\n\n[1] 43 58\n\n\nOr we can create a subset of the full dataset:\n\ndat[ these.female , ]\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nname\ngender\ngroup\nstrength\n\n\n\n\n2\nwanda\nfemale\ntreat\n43\n\n\n4\nnancy\nfemale\ncontrol\n58",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Constructing Groups</span>"
    ]
  },
  {
    "objectID": "5_10-constructing-groups.html#compound-logical-statements",
    "href": "5_10-constructing-groups.html#compound-logical-statements",
    "title": "18  Constructing Groups",
    "section": "19.1 Compound Logical Statements",
    "text": "19.1 Compound Logical Statements\nWe can combine multiple logical statements using the AND, OR, and NOT operators ( &, |, ! ). This functionality gives us an incredible ability to specify very granular groups within our analysis. This will be important as we begin to construct analysis in a way that we search for apples to apples comparisons within our data in order to make inferences about program effectiveness.\nThese statements require some precision, however. Use care when applying the AND, OR, and NOT operators as to not include unintended data in your sample.",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Constructing Groups</span>"
    ]
  },
  {
    "objectID": "5_10-constructing-groups.html#the-opposite-of-operator",
    "href": "5_10-constructing-groups.html#the-opposite-of-operator",
    "title": "18  Constructing Groups",
    "section": "19.2 The Opposite-Of Operator",
    "text": "19.2 The Opposite-Of Operator\nThe ! operator is a special case, where it is not used to define a new logical vector, but rather it swaps the values of an existing logical vector.\n\n! TRUE\n\n[1] FALSE\n\n! FALSE\n\n[1] TRUE\n\n\n\nx1 &lt;- c(7,9,1,2)\nthese &lt;- x1 &gt; 5\nthese\n\n[1]  TRUE  TRUE FALSE FALSE\n\n! these\n\n[1] FALSE FALSE  TRUE  TRUE\n\n\nBe careful with the order of operations though! If we are working with this data, for example:\n\n\n\n\n\n\n\n\n\ngroup\ngender\n\n\n\n\ntreat\nmale\n\n\ntreat\nfemale\n\n\ncontrol\nmale\n\n\ncontrol\nfemale\n\n\n\n\n\nAnd we define men in the treatment group:\n\nthese &lt;- group == \"treat\" & gender == \"male\" \ndat[ these , ] %&gt;% pander\n\n\n\n\n\n\n\n\ngroup\ngender\n\n\n\n\ntreat\nmale\n\n\n\n\n\nNote that the opposite of men in the treatment group is NOT women in the control group:\n\nthese &lt;- ! ( group == \"treat\" & gender == \"male\" )\ndat[ these , ] %&gt;% pander\n\n\n\n\n\n\n\n\n\n \ngroup\ngender\n\n\n\n\n2\ntreat\nfemale\n\n\n3\ncontrol\nmale\n\n\n4\ncontrol\nfemale\n\n\n\n\n\nWomen in the control group would be:\n\n# not treatment group & not men\nthese &lt;- ! ( group == \"treat\" ) &  ! ( gender == \"male\" ) \ndat[ these , ]\n\n    group gender\n4 control female",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Constructing Groups</span>"
    ]
  },
  {
    "objectID": "5_10-constructing-groups.html#find-and-replace",
    "href": "5_10-constructing-groups.html#find-and-replace",
    "title": "18  Constructing Groups",
    "section": "21.1 Find and Replace",
    "text": "21.1 Find and Replace\nWe can use selector variables to replace observations with new values using the assignment operator. This is similar to a find and replace operation.\n\nanimals &lt;- c( \"mole\", \"mouse\", \"shrew\", \"mouse\", \"rat\", \"shrew\" )\n\n# the lab assistant incorrectly identified the shrews\n\nanimals\n\n[1] \"mole\"  \"mouse\" \"shrew\" \"mouse\" \"rat\"   \"shrew\"\n\nanimals[ animals == \"shrew\" ] &lt;- \"possum\"\n\nanimals\n\n[1] \"mole\"   \"mouse\"  \"possum\" \"mouse\"  \"rat\"    \"possum\"\n\n\nWe don’t know if linda received the treatment:\n\n\n\n\n\n\n\n\n\nname\nstudy.group\n\n\n\n\nadam\ntreatment\n\n\njamal\ncontrol\n\n\nlinda\ntreatment\n\n\nsriti\ncontrol\n\n\n\n\n\n\n# replace \nstudy.group[ name == \"linda\" ] &lt;- NA\n\nstudy.group\n\n[1] \"treatment\" \"control\"   NA          \"control\"",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Constructing Groups</span>"
    ]
  },
  {
    "objectID": "5_10-constructing-groups.html#nas-in-logical-statements",
    "href": "5_10-constructing-groups.html#nas-in-logical-statements",
    "title": "18  Constructing Groups",
    "section": "21.2 NAs in Logical Statements",
    "text": "21.2 NAs in Logical Statements\nRecall that missing values are an extremely important concept in statistics. If one-third of our survey sample reports that they never smoked pot, one-third reports they have smoked pot, and one-third did not answer the question, then what do we report for the proportion of the population that has smoked pot?\nWe might prefer to be cautious and count only the people that have confirmed they have smoked pot, resulting in an estimate of 33.3%.\nIf we throw out the missing data, then 50% of respondents have smoked pot.\nIf we assume those that refuse to answer have likely smoked pot, our estimate might be 66.6% of the sample.\nThese different results are a function of how we treat the missing data in our survey, so it is important that we can keep track of missing values, especially during subset operations.\nNote how NAs effect compound logical statements:\n\nTRUE & TRUE\n\n[1] TRUE\n\nTRUE & FALSE\n\n[1] FALSE\n\nTRUE & NA\n\n[1] NA\n\nFALSE & NA\n\n[1] FALSE\n\n\nTo make sense of these rules consider the following:\nIf one condition is already FALSE, the missing value does not matter because under the & condition BOTH must be TRUE for the observation to belong to our defined group. After we know that one of the conditions is FALSE the missing value is irrelevant. For example, if we want to select all women in the treatment group, and we have a man with an unclear treatment group status, he is still excluded from the group because he is a man.\nOn the other hand, if one condition is TRUE, and another is NA, R does not want to throw out the data because the state of the missing value is unclear. As a result, it will preserve the observation, but it will replace all of the data with missing values to signal the lack of certainty associated with that observation.\n\n\n\n\n\n\n\n\n\ngroup\ngender\n\n\n\n\ntreat\nmale\n\n\ntreat\nfemale\n\n\ncontrol\nmale\n\n\ncontrol\nfemale\n\n\n\n\n\n\nkeep.these &lt;- c(T,F,NA,F)\ndat[ keep.these , ]\n\n   group gender\n1  treat   male\nNA  &lt;NA&gt;   &lt;NA&gt;\n\n\nTo remove the rows with missing values in your selector vector, replace all NAs with FALSE:\n\nkeep.these[ is.na(keep.these) ] &lt;- FALSE\ndat[ keep.these , ]\n\n  group gender\n1 treat   male",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Constructing Groups</span>"
    ]
  },
  {
    "objectID": "5_20-group-structure.html",
    "href": "5_20-group-structure.html",
    "title": "19  Efficient Use of Group Structure",
    "section": "",
    "text": "19.1 Packages Used in this Chapter\nlibrary( pander )\nlibrary( dplyr )\nlibrary( tidyr )\nlibrary( reshape2 )\nlibrary( scales )\nlibrary( ggplot2 )\nlibrary( Lahman )",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Efficient Use of Group Structure</span>"
    ]
  },
  {
    "objectID": "5_20-group-structure.html#hypothetical-experimental-data",
    "href": "5_20-group-structure.html#hypothetical-experimental-data",
    "title": "19  Efficient Use of Group Structure",
    "section": "19.2 Hypothetical Experimental Data",
    "text": "19.2 Hypothetical Experimental Data\nWe will demonstrate some functions using this hypothetical dataset:\n\nhead( d ) %&gt;% pander\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nrace\nblood.type\ngender\nage\nstudy.group\nspeed\n\n\n\n\n1\nblack\nB\nfemale\n46\ntreatment\n643.4\n\n\n2\nasian\nA\nfemale\n29\ntreatment\n661.9\n\n\n3\nblack\nB\nfemale\n35\ntreatment\n677.2\n\n\n4\nasian\nA\nmale\n38\ntreatment\n536.2\n\n\n5\nblack\nB\nfemale\n42\ntreatment\n741.6\n\n\n6\nwhite\nA\nmale\n30\ntreatment\n494.6",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Efficient Use of Group Structure</span>"
    ]
  },
  {
    "objectID": "5_20-group-structure.html#group-structure",
    "href": "5_20-group-structure.html#group-structure",
    "title": "19  Efficient Use of Group Structure",
    "section": "19.3 Group Structure",
    "text": "19.3 Group Structure\nThe two most important skills as you first learn a data programming language are:\n\nTranslating English phrases into computer code using logical statements\n\nOrganizing your data into groups\n\nThis lecture focuses on efficiently splitting your data into groups, and then analyzing your data by group.\n\n19.3.1 What Are Groups?\nA group represents a set of elements with identical characteristics - mice all belong to one group and elephants belong to another. Easy enough, right?\nIn data analysis, it is a little more complicated because a group is defined by a set of features. Each group still represents a set of elements with identical characteristics, but when we have multiple features there is a unique group for each combination of features.\nThe simple way to think about this is that the cross-tab of features generates a grid (table), and each cell represents a unique group:\n\n\n\n\n\n\n\n\n\nWe might be interested in simple groups (treatment cases versus control cases) or complex groups (does the treatment effect women and men differently?).\nIn previous lectures you have learned to identify a group with a logical statement and analyze that group discretely.\n\nmean( speed[ study.group == \"treatment\" & gender==\"female\" ] )\n\n[1] 617.3832\n\n\nIn this lecture you will learn to define a group structure, then analyze all of your data using that structure.\n\ntapply( speed, INDEX = list( study.group, gender ), FUN = mean )\n\n\n\n\n\n\n\n\n\n\n\n \nfemale\nmale\n\n\n\n\ncontrol\n469.2\n363.8\n\n\ntreatment\n617.4\n512\n\n\n\n\n\n\n\n19.3.2 Main Take-Away\nR has been designed to do efficient data analysis by defining a group structure, then quickly applying a function to all unique members.\n\nThe base R packages do this with a set of functions in the apply() family. The tapply() function allows you to specify an outcome to analyze and a group, then ask for results from a function.\n\ntapply( X=speed, INDEX=list( study.group, gender ), FUN=mean ) \n\n\n\n\n\n\n\n\n\n\n\n \nfemale\nmale\n\n\n\n\ncontrol\n469.2\n363.8\n\n\ntreatment\n617.4\n512\n\n\n\n\n\nThe dplyr package makes this process easier using some simple verbs and the “pipe” operator.\n\ndat  %&gt;%  group_by( study.group, gender )  %&gt;%  summarize( ave.speed = mean(speed) )\n\n\n\n\n\n\n\n\n\n\n\nstudy.group\ngender\nave.speed\n\n\n\n\ncontrol\nmale\n363.8\n\n\ncontrol\nfemale\n469.2\n\n\ntreatment\nmale\n512\n\n\ntreatment\nfemale\n617.4\n\n\n\n\n\n\n\n19.3.3 Example\nLet’s think about a study looking at reading speed. The treatment is a workshop that teaches some speed-reading techniques. In this study we have data on:\n\ngender (male, female)\nrace (black, white, asian)\nblood.type (A, B)\nage (from 18 to 93)\n\nExamining descriptive statistics we can see that reading speed varies by gender and the treatment group, but not by race or blood type:\n\n\n\n\n\n\n\n\n\nThe question is, how many unique groups can we create with these four factors?\nEach individual factor contains a small number of levels (only 2 or 3 in this case), which makes the group structure look deceptively simple at first glance. When we start to examine combinations of factors we see that group structure can get complicated pretty quickly.\nIf we look at gender alone, we have two levels: male and female. So we have two groups. If we look at our study groups alone we have two groups: treatment and control.\nIf we look at gender and the study groups together, we now have a 2 x 2 grid, or four unique groups.\nIf the race factor has three levels, how many unique groups will we have considering the study design, gender, and race together?\n\n\n\n\n\n\n\n\n\nWe can calculate the size of the grid by multiplying number of levels for each factor. We see here we have 12 unique groups:\n\nnlevels( gender ) * nlevels( study.group ) * nlevels( race )\n\n[1] 12\n\n\nIf we add blood type, a factor with two levels (A and B), we now have 24 unique groups:\n\np + facet_grid( race + study.group ~ gender + blood.type) \n\n\n\n\n\n\n\n\nWhat about age? It is a continuous variable, so it’s a little more tricky.\nWe can certainly analyze the relationship between age and speed using correlation tools.\n\nplot( age, speed, bty=\"n\", main=\"Age\" )\n\n\n\n\n\n\n\n\nBut we can also incorporate this independent variable into a group structure. We can treat each distinct age as a separate group. The ages in this study range from 18 to 93, so we have 65 distinct ages represented.\n\nplot( factor(age), speed, las=2, frame.plot=F, outline=F, main=\"Age\", xaxt=\"n\"  )\n\n\n\n\n\n\n\n\nIf we think about the overall group structure, then, we have unique groups defined by gender, race, blood type, and study design, and another 65 age groups. So in total we now have 24 x 65 = 1,560 groups! That is getting complicated.\nThis group design is problematic for two reasons. From a pragmatic standpoint, we can’t report results from 1,500 groups in a table. From a more substantive perspective, although we have 1,500 distinct cells in our grid, many may not include observations that represent the unique combination of all factors. So this group design is not very practical.\nA similar problem arises if our data includes time. If our data includes the time of events recorded by hours, days of the week, months, and years, we can have complicated group structures if we try to analyze every unique combination.\nWe can simplify our analysis by thinking about age ranges instead of ages, or in other words by binning our continuous data. If we split it into five-year ranges, for example, we have gone from 65 distinct ages to 12 distinct age groups.\n\nage.group &lt;- cut( age, \n                  breaks=seq(from=20,to=80,by=5),\n                  labels=paste( seq(from=20,to=75,by=5), \n                                \"to\", seq(from=25,to=80,by=5) ) )\n\ngroup.structure &lt;- formula( speed ~ age.group )\n\nboxplot( group.structure, las=2, frame.plot=F, outline=F, main=\"Age Group\"  )\n\n\n\n\n\n\n\n\nWe have now simplified our analysis from 1,560 to 288 possible groups. Combinations of groups will also be easier:\n\ngroup.structure &lt;- formula( speed ~ gender * age.group )\n\nboxplot( group.structure, \n         las=2, frame.plot=F, outline=F, main=\"Age Group by Gender\",\n         col=c(\"firebrick\",\"steelblue\"), xaxt=\"n\" )",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Efficient Use of Group Structure</span>"
    ]
  },
  {
    "objectID": "5_20-group-structure.html#analysis-by-group",
    "href": "5_20-group-structure.html#analysis-by-group",
    "title": "19  Efficient Use of Group Structure",
    "section": "19.4 Analysis by Group",
    "text": "19.4 Analysis by Group\nLet’s demonstrate some analysis of groups using the Lahman package and some dplyr verbs. Let’s do some analysis of player salaries (Salaries dataset), and start with a simple group structure - teams in the National League and time.\n\nWhich team has the highest average player salary?\nWhich team has the most players paid over $5 million a season?\nWhich team has raised it’s pay the most over the past decade?\n\nLet’s start by thinking about group structure. We have teams, and we have seasons. Teams is stored as a factor, and seasons as a numeric value, so we can consider group for each by counting levels and unique values:\n\nnlevels( Salaries$teamID )\n\n[1] 35\n\nlength( unique( Salaries$yearID ) )\n\n[1] 32\n\n\nSo we can potentially calculate 32 x 46 = 1,472 average player salaries.\n\n19.4.1 Highest Average Player Salary\nFor our first question, we will select only teams from the National League. Let’s use the most recent year of data to calculate average pay.\n\nSalaries %&gt;% filter( lgID == \"NL\", yearID == 2016 ) %&gt;% \n             group_by( teamID) %&gt;% \n             summarize( Ave_Salary = mean(salary) )\n\n# A tibble: 15 × 2\n   teamID Ave_Salary\n   &lt;fct&gt;       &lt;dbl&gt;\n 1 ARI      3363041.\n 2 ATL      2362010.\n 3 CHN      5312678.\n 4 CIN      3066899.\n 5 COL      3413487 \n 6 LAN      6322525.\n 7 MIA      2761222.\n 8 MIL      2292508.\n 9 NYN      4958857.\n10 PHI      2033793.\n11 PIT      3706387.\n12 SDN      3756475.\n13 SFN      6890151.\n14 SLN      4614629.\n15 WAS      5448179.\n\n\nSince the salaries are large, they are a little hard to read. Let’s clean up the table a bit.\n\nSalaries  %&gt;% \n             filter( lgID == \"NL\", yearID == 2016 ) %&gt;% \n             group_by( teamID ) %&gt;% \n             summarize( Ave_Salary=dollar( mean(salary,na.rm=T) ) ) %&gt;%\n             arrange( desc(Ave_Salary) ) %&gt;%\n             pander()\n\n\n\n\n\n\n\n\nteamID\nAve_Salary\n\n\n\n\nSFN\n$6,890,151\n\n\nLAN\n$6,322,525\n\n\nWAS\n$5,448,179\n\n\nCHN\n$5,312,678\n\n\nNYN\n$4,958,857\n\n\nSLN\n$4,614,629\n\n\nSDN\n$3,756,475\n\n\nPIT\n$3,706,387\n\n\nCOL\n$3,413,487\n\n\nARI\n$3,363,041\n\n\nCIN\n$3,066,899\n\n\nMIA\n$2,761,222\n\n\nATL\n$2,362,010\n\n\nMIL\n$2,292,508\n\n\nPHI\n$2,033,793\n\n\n\n\n\n\n\n19.4.2 Most Players Paid Over $5 Million\nThis question requires you to utilize a logical statement in order to translate from the question to code. We need to inspect each salary, determine whether it is over the $5m threshold, then count all of the cases. The operation will look something like this:\n\nsum( Salaries$salary &gt; 5000000 )\n\n[1] 3175\n\n\nIt gets a little trickier when we want to do the operation simultaneously across groups. Our team group structure is already defined, so let’s define our logical vector and count cases that match:\n\ndat.NL &lt;- filter( Salaries, yearID == 2010 & lgID == \"NL\" ) %&gt;% droplevels()\n\ngt.5m &lt;- dat.NL$salary &gt; 5000000\n\ntable( dat.NL$teamID, gt.5m )\n\n     gt.5m\n      FALSE TRUE\n  ARI    23    3\n  ATL    21    6\n  CHN    19    8\n  CIN    21    5\n  COL    23    6\n  FLO    23    4\n  HOU    24    4\n  LAN    20    7\n  MIL    25    4\n  NYN    19    9\n  PHI    18   10\n  PIT    27    0\n  SDN    25    1\n  SFN    21    7\n  SLN    19    6\n  WAS    26    4\n\n\nThis solution works, but the table provides too much information. We can use dplyr to simplify and format the table nicely for our report:\n\nSalaries %&gt;% \n   filter( yearID == 2010 & lgID == \"NL\" ) %&gt;%\n   group_by( teamID ) %&gt;% \n   summarise( gt_five_million = sum( salary &gt; 5000000 ) ) %&gt;% \n   arrange( desc(gt_five_million) ) %&gt;%\n   pander\n\n\n\n\n\n\n\n\nteamID\ngt_five_million\n\n\n\n\nPHI\n10\n\n\nNYN\n9\n\n\nCHN\n8\n\n\nLAN\n7\n\n\nSFN\n7\n\n\nATL\n6\n\n\nCOL\n6\n\n\nSLN\n6\n\n\nCIN\n5\n\n\nFLO\n4\n\n\nHOU\n4\n\n\nMIL\n4\n\n\nWAS\n4\n\n\nARI\n3\n\n\nSDN\n1\n\n\nPIT\n0\n\n\n\n\n\n\n\n19.4.3 Fielding Positions\nWhich fielding position is the highest paid?\n\nmerge( Salaries, Fielding ) %&gt;%\n  filter( yearID == 2016 ) %&gt;%\n  group_by( POS ) %&gt;%\n  summarize( Mean_Salary = dollar( mean(salary) ) ) %&gt;%\n  pander\n\n\n\n\n\n\n\n\nPOS\nMean_Salary\n\n\n\n\n1B\n$5,680,014\n\n\n2B\n$3,092,188\n\n\n3B\n$3,812,355\n\n\nC\n$3,167,962\n\n\nOF\n$4,317,165\n\n\nP\n$4,002,667\n\n\nSS\n$2,574,626\n\n\n\n\n\n\n\n19.4.4 Country of Birth\nWhich country has produced the highest paid baseball players?\n\nmerge( Salaries, People ) %&gt;%\n  filter( yearID == 2016 ) %&gt;%\n  group_by( birthCountry ) %&gt;%\n  summarize( Mean_Salary = dollar( mean(salary) ) ) %&gt;% \n  pander\n\n\n\n\n\n\n\n\nbirthCountry\nMean_Salary\n\n\n\n\nAruba\n$650,000\n\n\nAustralia\n$523,400\n\n\nBrazil\n$1,548,792\n\n\nCAN\n$7,854,167\n\n\nColombia\n$3,125,289\n\n\nCuba\n$5,787,593\n\n\nCuracao\n$5,724,167\n\n\nD.R.\n$5,045,623\n\n\nGermany\n$511,500\n\n\nJapan\n$8,247,012\n\n\nMexico\n$5,333,671\n\n\nNetherlands\n$2,425,000\n\n\nNicaragua\n$2,375,000\n\n\nP.R.\n$3,241,378\n\n\nPanama\n$2,946,550\n\n\nSaudi Arabia\n$522,500\n\n\nSouth Korea\n$5,326,190\n\n\nTaiwan\n$6,750,000\n\n\nUSA\n$4,222,249\n\n\nV.I.\n$507,500\n\n\nVenezuela\n$4,521,051\n\n\n\n\n\n\n\n19.4.5 Pay Raises\nTo examine pay raises, we will now use more than one year of data. Since the question asks about pay raises over the past decade, we will filter the last ten years of data.\nAnd since we are looking at patterns over teams and over time, we need to define a group structure with two variables:\n\nSalaries %&gt;% filter( yearID &gt; 2006 & lgID == \"NL\" ) %&gt;% \n             group_by( teamID, yearID ) %&gt;% \n             summarize( mean= dollar(mean(salary)) ) %&gt;%\n             head( 20 ) %&gt;% pander\n\n\n\n\n\n\n\n\n\nteamID\nyearID\nmean\n\n\n\n\nARI\n2007\n$1,859,555\n\n\nARI\n2008\n$2,364,383\n\n\nARI\n2009\n$2,812,141\n\n\nARI\n2010\n$2,335,314\n\n\nARI\n2011\n$1,986,660\n\n\nARI\n2012\n$2,733,512\n\n\nARI\n2013\n$3,004,400\n\n\nARI\n2014\n$3,763,904\n\n\nARI\n2015\n$2,132,207\n\n\nARI\n2016\n$3,363,041\n\n\nATL\n2007\n$3,117,530\n\n\nATL\n2008\n$3,412,189\n\n\nATL\n2009\n$3,335,385\n\n\nATL\n2010\n$3,126,802\n\n\nATL\n2011\n$3,346,257\n\n\nATL\n2012\n$2,856,205\n\n\nATL\n2013\n$3,254,501\n\n\nATL\n2014\n$4,067,042\n\n\nATL\n2015\n$2,990,885\n\n\nATL\n2016\n$2,362,010\n\n\n\n\n\nThis might seem like an odd format. We might expect something that looks more like our grid structure:\n\ndat.NL &lt;- filter( Salaries, yearID &gt; 2010 & lgID == \"NL\" ) %&gt;% droplevels()\n\ntapply( dat.NL$salary, \n        INDEX=list(dat.NL$teamID, dat.NL$yearID), \n        FUN=mean, na.rm=T ) %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n2011\n2012\n2013\n2014\n2015\n2016\n\n\n\n\nARI\n1986660\n2733512\n3004400\n3763904\n2132207\n3363041\n\n\nATL\n3346257\n2856205\n3254501\n4067042\n2990885\n2362010\n\n\nCHN\n5001893\n3392194\n3867989\n2426759\n4138547\n5312678\n\n\nCIN\n2531571\n2935843\n4256178\n3864911\n4187862\n3066899\n\n\nCOL\n3390310\n2692054\n2976363\n3180117\n3827544\n3413487\n\n\nFLO\n2190154\nNA\nNA\nNA\nNA\nNA\n\n\nHOU\n2437724\n2332731\nNA\nNA\nNA\nNA\n\n\nLAN\n3472967\n3171453\n6980069\n6781706\n7441103\n6322525\n\n\nMIA\nNA\n4373259\n1400079\n1549515\n2835688\n2761222\n\n\nMIL\n2849911\n3755921\n3077881\n3748778\n3477586\n2292508\n\n\nNYN\n4401752\n3457555\n1648278\n3168777\n3870667\n4958857\n\n\nPHI\n5765879\n5817965\n6533200\n5654530\n4295885\n2033793\n\n\nPIT\n1553345\n2248286\n2752214\n2756357\n3065259\n3706387\n\n\nSDN\n1479650\n1973025\n2342339\n2703061\n4555435\n3756475\n\n\nSFN\n4377716\n3920689\n5006441\n5839649\n6100056\n6890151\n\n\nSLN\n3904947\n3939317\n3295004\n4310464\n4586212\n4614629\n\n\nWAS\n2201963\n2695171\n4548131\n4399456\n5365085\n5448179\n\n\n\n\n\nLater on we will look at the benefits of “tidy data”, but the basic idea is that you can “facet” your analysis easily when your groups are represented as factors instead of arranged as a table. For example, here is a time series graph that is faceted by teams:\n\nSalaries %&gt;% filter( yearID &gt; 2000 & lgID == \"AL\" ) %&gt;% \n             group_by( teamID, yearID ) %&gt;% \n             summarize( Mean_Player_Salary=mean(salary) ) -&gt; t1\n\nqplot( data=t1, x=yearID, y=Mean_Player_Salary, \n       geom=c(\"point\", \"smooth\")  ) + \n       facet_wrap( ~ teamID, ncol=5 )\n\n\n\n\n\n\n\n\nNow you can quickly see that Detroit is the team that has raised salaries most aggressively.\nIf we need to, we can easily convert a tidy dataset into something that looks like a table using the spread() function:\n\nSalaries %&gt;% filter( yearID &gt; 2006 & lgID == \"NL\" ) %&gt;% \n             group_by( teamID, yearID ) %&gt;% \n             summarize( mean = dollar(mean(salary)) ) %&gt;%\n             spread( key=yearID, value=mean, sep=\"_\" ) %&gt;% \n             select( 1:6 ) %&gt;% na.omit() %&gt;%\n             pander\n\n\n\n\n\n\n\n\n\n\n\n\nteamID\nyearID_2007\nyearID_2008\nyearID_2009\nyearID_2010\nyearID_2011\n\n\n\n\nARI\n$1,859,555\n$2,364,383\n$2,812,141\n$2,335,314\n$1,986,660\n\n\nATL\n$3,117,530\n$3,412,189\n$3,335,385\n$3,126,802\n$3,346,257\n\n\nCHN\n$3,691,494\n$4,383,179\n$5,392,360\n$5,429,963\n$5,001,893\n\n\nCIN\n$2,210,483\n$2,647,061\n$3,198,196\n$2,760,059\n$2,531,571\n\n\nCOL\n$2,078,500\n$2,640,596\n$2,785,222\n$2,904,379\n$3,390,310\n\n\nFLO\n$984,097\n$660,955\n$1,315,500\n$2,112,212\n$2,190,154\n\n\nHOU\n$3,250,333\n$3,293,719\n$3,814,682\n$3,298,411\n$2,437,724\n\n\nLAN\n$3,739,811\n$4,089,260\n$4,016,584\n$3,531,778\n$3,472,967\n\n\nMIL\n$2,629,130\n$2,790,948\n$3,083,942\n$2,796,837\n$2,849,911\n\n\nNYN\n$3,841,055\n$4,593,113\n$5,334,785\n$4,800,819\n$4,401,752\n\n\nPHI\n$2,980,940\n$3,495,710\n$4,185,335\n$5,068,871\n$5,765,879\n\n\nPIT\n$1,427,327\n$1,872,684\n$1,872,808\n$1,294,185\n$1,553,345\n\n\nSDN\n$2,235,022\n$2,376,697\n$1,604,952\n$1,453,819\n$1,479,650\n\n\nSFN\n$3,469,964\n$2,641,190\n$2,965,230\n$3,522,905\n$4,377,716\n\n\nSLN\n$3,224,529\n$3,018,923\n$3,278,830\n$3,741,630\n$3,904,947\n\n\nWAS\n$1,319,554\n$1,895,207\n$2,140,286\n$2,046,667\n$2,201,963\n\n\n\n\nSalaries %&gt;% filter( yearID &gt; 2006 & lgID == \"NL\" ) %&gt;% \n             group_by( teamID, yearID ) %&gt;% \n             summarize( mean = dollar(mean(salary)) ) %&gt;%\n             spread( key=teamID, value=mean, sep=\"_\" ) %&gt;% \n             select( 1:6 ) %&gt;% \n             pander\n\n\n\n\n\n\n\n\n\n\n\n\nyearID\nteamID_ARI\nteamID_ATL\nteamID_CHN\nteamID_CIN\nteamID_COL\n\n\n\n\n2007\n$1,859,555\n$3,117,530\n$3,691,494\n$2,210,483\n$2,078,500\n\n\n2008\n$2,364,383\n$3,412,189\n$4,383,179\n$2,647,061\n$2,640,596\n\n\n2009\n$2,812,141\n$3,335,385\n$5,392,360\n$3,198,196\n$2,785,222\n\n\n2010\n$2,335,314\n$3,126,802\n$5,429,963\n$2,760,059\n$2,904,379\n\n\n2011\n$1,986,660\n$3,346,257\n$5,001,893\n$2,531,571\n$3,390,310\n\n\n2012\n$2,733,512\n$2,856,205\n$3,392,194\n$2,935,843\n$2,692,054\n\n\n2013\n$3,004,400\n$3,254,501\n$3,867,989\n$4,256,178\n$2,976,363\n\n\n2014\n$3,763,904\n$4,067,042\n$2,426,759\n$3,864,911\n$3,180,117\n\n\n2015\n$2,132,207\n$2,990,885\n$4,138,547\n$4,187,862\n$3,827,544\n\n\n2016\n$3,363,041\n$2,362,010\n$5,312,678\n$3,066,899\n$3,413,487",
    "crumbs": [
      "Data Wrangling",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Efficient Use of Group Structure</span>"
    ]
  },
  {
    "objectID": "6_10-intro-to-data-viz.html",
    "href": "6_10-intro-to-data-viz.html",
    "title": "Intro to Data Viz",
    "section": "",
    "text": "Visualization can be a powerful way to generate insights from data. Creating impactful graphics is not a trivial undertaking, however. There is a science to how the brain consumes visual information. And there is an art to combining elements of graphics in ways that make the data both aesthetically pleasing and informative. It takes practice to develop these skills.\nThis section of the textbook is not about the design of a specific visualization or graphic, but rather the implementation. Once you have an idea for your graphic in your mind, you need a few basic R functions to create the visualization. We will cover these nuts and bolts of building custom graphics, some popular R packages for visualization, and some tricks to take your data viz game to the next level using dynamic graphics and animations.\n\n\n\n\n\nThe core graphics package allows you to control every element of a visualization.\n\n\n\n\nIf you would like to start building your data viz muscles, we recommend the following resources as good jumping-off points.\nPopular blogs that demonstrate the step-by-step process of making a mediocre graphic into a compelling graphic:\n\nMakeover Mondays\nFlowing Data\nJunk Charts\nNYT Graphics Blog\nHelp Me Viz\n\nUseful introductory textbooks on data visualization:\n\nSchwabish, J. A. (2014). An economist’s guide to visualizing data. Journal of Economic Perspectives, 28(1), 209-34.\nT. Chiasson, D. Gregory, & Contributors (2013). Data + Design: A simple introduction to preparing and visualizing information.\nTableau: Which Chart Goes with What Data\n\nInspiration and help with R graphics:\n\nR Graph Gallery\nR Graph Catalog\nR Graph Compendium\nggplot2 Geoms Gallery",
    "crumbs": [
      "Intro to Data Viz"
    ]
  },
  {
    "objectID": "6_20-plot-basics.html",
    "href": "6_20-plot-basics.html",
    "title": "20  The plot() Function",
    "section": "",
    "text": "20.0.1 Key Concepts\nWe can create highly-customized scatterplots by mastering a few arguments:\nThese are some of the most useful arguments for the plot() function, but only a small sample of settings that you can change.\nTry help( “par” ) for a more extensive list.",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The plot() Function</span>"
    ]
  },
  {
    "objectID": "6_20-plot-basics.html#plot-arguments",
    "href": "6_20-plot-basics.html#plot-arguments",
    "title": "20  The plot() Function",
    "section": "20.1 plot() Arguments",
    "text": "20.1 plot() Arguments\nThis lecture is a brief introduction to the plot() function in R, the work horse of the graphics package. We will introduce the flexibility of the fully-customizable graphics engine in R through the demonstration of some useful arguments.\nTo demonstrate these arguments we will use a simple dataset from a hypothetical farming experiment that examines the relationship between levels of new fertilizer under development and the height of the corn. To identify the optimal dosage of fertilizer to use, the experiment applies different levels to separate fields of corn, then measures the average final corn height at each dosage. The fields are scattered across three farms, and “moisture” represents the average Volumetric Water Content of the soil in each field.\nYou can load it as follows:\n\nsource( \"https://raw.githubusercontent.com/DS4PS/Data-Science-Class/master/DATA/corn_stalks.R\" )\n\n\n\n\n\n\n\n\n\n\n\n\nfertilizer\ncorn.height\nmoisture\nfarm\n\n\n\n\n92\n252\n0.45\nC\n\n\n39\n300\n0.52\nA\n\n\n73\n278\n0.3\nC\n\n\n46\n279\n0.67\nB\n\n\n44\n295\n0.89\nC\n\n\n87\n277\n0.31\nA\n\n\n59\n324\n0.3\nA\n\n\n67\n307\n0.96\nA\n\n\n22\n297\n0.6\nA\n\n\n49\n303\n0.7\nC\n\n\n\n\n\n \n\nChange plot() arguments to see how they impact the graph.\nIn your R console type colors() to get a list of color names that R will recognize, or type demo(\"colors\") to get a tour of some options.\n\n\ndc_light_exercise_ex-01",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The plot() Function</span>"
    ]
  },
  {
    "objectID": "6_20-plot-basics.html#the-default-scatterplot",
    "href": "6_20-plot-basics.html#the-default-scatterplot",
    "title": "20  The plot() Function",
    "section": "20.2 The Default Scatterplot",
    "text": "20.2 The Default Scatterplot\nThe default plot() function requires an x-variable and y-variable and will create a scatterplot, adding axes and a title:\n\nplot( x=fertilizer, y=corn.height )\n\n\n\n\n\n\n\n\nOk, so let’s improve upon this a bit. You can use the following arguments to customize the plot:",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The plot() Function</span>"
    ]
  },
  {
    "objectID": "6_20-plot-basics.html#titles",
    "href": "6_20-plot-basics.html#titles",
    "title": "20  The plot() Function",
    "section": "20.3 Titles",
    "text": "20.3 Titles\nWe can add better labels and a title with xlab=, ylab=, and main=.\n\nplot( \n      x=fertilizer, y=corn.height,\n      xlab=\"Fertilizer (mg)\",\n      ylab=\"Corn Height (cm)\",\n      main=\"Relationship Between Fertilizer Intensity and Corn Growth\"    \n    )\n\n\n\n\n\n\n\n\nWe can also change their size with cex.lab= to control the size of the axes labels, and cex.main= to control the size of the title.\nNote that all of the cex arguments are aspect ratios, meaning that the default value of 1 represents 100% and all other argument values are in relation to this default. A value of 2 means to increase the title to 200% of the size, an argument of 0.5 shrinks the title to half the original size.\n\nplot( \n      x=fertilizer, y=corn.height,\n      xlab=\"Fertilizer (mg)\",\n      ylab=\"Corn Height (cm)\",\n      main=\"cex.lab=2\", \n      cex.lab=2,               # double the size of the axis labels\n      col.lab=\"steelblue\"      # change color of axis labels\n    )",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The plot() Function</span>"
    ]
  },
  {
    "objectID": "6_20-plot-basics.html#type-of-plot",
    "href": "6_20-plot-basics.html#type-of-plot",
    "title": "20  The plot() Function",
    "section": "20.4 Type of Plot",
    "text": "20.4 Type of Plot\nWe can plot points, lines, or some combination of lines and points using the type= argument:\n\n“l” for lines\n“p” for points\n“b” for both points and lines\n“o” plots lines over points\n“n” for no lines or points\n\n\nplot( \n      x=fertilizer, y=corn.height,\n      type=\"p\",\n      main='type=\"p\"',\n      cex.main=2,\n      xlab=\"\",\n      ylab=\"\",\n      col.axis=\"gray60\",\n      frame.plot=F\n    )",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The plot() Function</span>"
    ]
  },
  {
    "objectID": "6_20-plot-basics.html#shape-of-points",
    "href": "6_20-plot-basics.html#shape-of-points",
    "title": "20  The plot() Function",
    "section": "20.5 Shape of Points",
    "text": "20.5 Shape of Points\nThe argument pch determines the shape of the plot points. The numeric values 0 to 25 represent different default shapes. We can also use any number, letter, or symbol as a plotting shape.\n\n\n\n\n\n\n\n\n\nNote that shapes 0 to 14 are hollow, 15 to 20 are solid, and 21 to 25 can also plot a background color specified by the bg= argument.\n\nplot( \n      x=fertilizer, y=corn.height,\n      frame.plot=FALSE,\n      xlab=\"Fertilizer (mg)\",\n      ylab=\"Corn Height (cm)\",\n      main=\"pch=23\", cex.main=1.5,\n      pch=23, col=\"red\", bg=\"green\"\n    )",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The plot() Function</span>"
    ]
  },
  {
    "objectID": "6_20-plot-basics.html#size-of-points",
    "href": "6_20-plot-basics.html#size-of-points",
    "title": "20  The plot() Function",
    "section": "20.6 Size of Points",
    "text": "20.6 Size of Points\nWe change the size of points using the cex= argument (pronounced “chex”). Similar to the title cex, it is an aspect ratio so cex=2 increases the size of the plotting points to 200% of the original, and cex=0.5 scales the size down to half of the original size.\n\nplot( \n      x=fertilizer, y=corn.height,\n      col=\"darkgoldenrod2\", \n      pch=19, \n      cex=2,                        # scale points to 200% normal size\n      xlab=\"\", ylab=\"\", las=1, \n      main=\"cex=2\", cex.main=2,\n      frame.plot=FALSE\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe cex= argument is also useful for incorporating a third numeric variable into the analysis. For example, perhaps we want to include the average moisture levels of the soil for each field. When we use a numeric vector like this with the cex= argument, instead of a single constant, the plot will adjust the size of observation based upon its measured moisture level. Since moisture values are between 0 and 1, I have scaled them by 3 to ensure the points are large enough to see.\n\nplot( \n      x=fertilizer, y=corn.height,\n      col=\"darkgoldenrod2\", \n      cex=3*moisture,\n      pch=19, frame.plot=F,\n      xlab=\"Fertilizer (mg)\",\n      ylab=\"Corn Height (cm)\",\n      main=\"Relationship Between Fertilizer Intensity and Corn Growth\"\n    )",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The plot() Function</span>"
    ]
  },
  {
    "objectID": "6_20-plot-basics.html#colors",
    "href": "6_20-plot-basics.html#colors",
    "title": "20  The plot() Function",
    "section": "20.7 Colors",
    "text": "20.7 Colors\nThe argument col= determines the color of plot points. To see a list of preset options check out:\nList of default named colors in R\n\nplot( \n      x=fertilizer, y=corn.height,\n      col=\"darkgoldenrod2\", pch=19, cex=2,\n      xlab=\"Fertilizer (mg)\",\n      ylab=\"Corn Height (cm)\",\n      main=\"Relationship Between Fertilizer Intensity and Corn Growth\",\n      frame.plot=FALSE\n    )\n\n\n\n\n\n\n\n\nIn the example above we specified a single color for all of our corn heights. If we want to incorporate a third categorical variable in our analysis, we can use a factor in our dataset as the value we pass to the col= argument. For example, perhaps we want to indicate which farm each field belongs to in the graph.\n\nplot( \n      x=fertilizer, y=corn.height,\n      pch=19, cex=2,\n      col=farm, \n      xlab=\"Fertilizer (mg)\",\n      ylab=\"Corn Height (cm)\",\n      main=\"Relationship Between Fertilizer Intensity and Corn Growth\",\n      frame.plot=FALSE\n    )\n\n\n\n\n\n\n\n\nNote that “farms” has to be a factor in order to use it in the col= argument. In this example, the farms have labels of “A” to “C”.\n\nlevels( farm )\n\n[1] \"A\" \"B\" \"C\"\n\n\nYou might be curious how R selected the colors for the three farms. The palette() function will print the default values that R uses for categorical variables:\n\npalette()\n\n[1] \"black\"   \"#DF536B\" \"#61D04F\" \"#2297E6\" \"#28E2E5\" \"#CD0BBC\" \"#F5C710\"\n[8] \"gray62\" \n\n\nYou can see that the first three are the colors used in the graph above. Since there are only 8 default values, if your categorical variable has more than 8 levels it will start to recycle colors.\nPerhaps you don’t like the default values. You can select your own by passing color names to the palette() function as follows:\n\npalette( c(\"forestgreen\",\"darkorange1\",\"darkorchid\") )\n\nplot( \n      x=fertilizer, y=corn.height,\n      pch=18, cex=3,\n      col=farm, \n      xlab=\"Fertilizer (mg)\",\n      ylab=\"Corn Height (cm)\",\n      main=\"Relationship Between Fertilizer Intensity and Corn Growth\",\n      frame.plot=FALSE\n    )",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The plot() Function</span>"
    ]
  },
  {
    "objectID": "6_20-plot-basics.html#looking-ahead",
    "href": "6_20-plot-basics.html#looking-ahead",
    "title": "20  The plot() Function",
    "section": "20.8 Looking Ahead",
    "text": "20.8 Looking Ahead\nIn the next section, we will add some lines, points, and text to the plot.\nWe can add lines to highlight trends (a regression is just the average of Y at each level of X).\n\nplot( \n      x=fertilizer, y=corn.height,\n      xlab=\"Fertilizer (mg)\",\n      ylab=\"Corn Height (cm)\",\n      main=\"Relationship Between Fertilizer Intensity and Corn Growth\",\n      pch=19, \n      col=\"gray\",\n      cex=2,\n      bty=\"n\"\n    )\n\nlines( lowess( fertilizer, corn.height ), col=\"darkgoldenrod2\", lwd=4 )\n\n\n\n\n\n\n\n\nIn order to add narrative to your graphs, you can add points and text.\nThe points() function operates with basically the same parameters as the plot() function. The text() function uses the same X and Y coordinates, but you also have to add an argument for the text that you want added to the plot.\nLet’s highlight the tallest corn stalk as an example.\n\ntallest.x &lt;- fertilizer[ which.max( corn.height ) ]\ntallest.y &lt;- corn.height[ which.max( corn.height ) ]\n\npoints( x=tallest.x, y=tallest.y, cex=3, lwd=1.5, col=\"firebrick4\" )\n\ntext( x=tallest.x, y=tallest.y, \n      labels=\"Tallest Stalk\", \n      pos=3, offset=1, col=\"firebrick4\" )",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>The plot() Function</span>"
    ]
  },
  {
    "objectID": "6_30-customized-graphics.html",
    "href": "6_30-customized-graphics.html",
    "title": "21  Customizing Plots",
    "section": "",
    "text": "21.1 Key Concepts\nWe can create almost any customized visualization using a small number of functions from the Core R graphics engine.\nCreating a new canvas:\nAdding data:\nAnnotating data:\nAnnotating the canvas:",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Customizing Plots</span>"
    ]
  },
  {
    "objectID": "6_30-customized-graphics.html#key-concepts",
    "href": "6_30-customized-graphics.html#key-concepts",
    "title": "21  Customizing Plots",
    "section": "",
    "text": "plot.new()\n\nplot.window( xlim, ylim )\n\n\n\npoints()\n\nlines()\n\nsegments()\n\nablines()\n\n\n\ntext()\n\n\n\ntitle()\n\naxis()\nbox()",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Customizing Plots</span>"
    ]
  },
  {
    "objectID": "6_30-customized-graphics.html#packages-used-in-this-chapter",
    "href": "6_30-customized-graphics.html#packages-used-in-this-chapter",
    "title": "21  Customizing Plots",
    "section": "21.2 Packages Used in This Chapter",
    "text": "21.2 Packages Used in This Chapter\nWe will use some data from the Lahman baseball data package for examples in this chapter.\nThe People data frame contains information about professional baseball players. We will focus on the relationship between height and weight of the players using the Body Mass Index (bmi) measure.\nThe data is interesting because it is easy to detect an abrubt and significant increase in baseball player size starting in the 1980’s when the the abuse of steroids began.\n\n\nMark McGuire before and after alleged steroid use.",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Customizing Plots</span>"
    ]
  },
  {
    "objectID": "6_30-customized-graphics.html#core-graphic-functions",
    "href": "6_30-customized-graphics.html#core-graphic-functions",
    "title": "21  Customizing Plots",
    "section": "21.3 Core Graphic Functions",
    "text": "21.3 Core Graphic Functions\n\npoints(\n  x=x, y=y,              # plots points at the x,y positions \n  pch=19,                # the type of point to plot\n  cex=2,                 # aspect ratio of point size\n  col=\"red\",             # color of points\n  bg=\"green\"             # fill color for open symbols\n )\n\n\ntext(\n  x=x, y=y,              # draws a line by connecting points \n  labels=some.text,      # vector of labels to plot on the graph\n  pos=3,                 # position: 1=below, 2=left, 3=above, 4=right\n  cex=2,                 # aspect ratio of text size\n  col=\"red\"              # color of text\n )\n\n\nlines(\n  x=x, y=y,              # draws a line by connecting points \n  lty=\"l\",               # type of lines, same as above\n  lwd=0.5,               # line thickness\n)\n\n\nsegments(\n  x0=x0, y0=y0,          # starting points of the segments (usually a vector)\n  x1=x1, y1=y1,          # end points of the segments (usually a vector)\n  ...                    # other arguments from lines()\n )\n\n\ntitle(\n  main=\"Plot Title\",     # text for the plot title\n  xlab=\"x variable\",     # text for the x-axis label\n  ylab=\"y variable\",     # text for the y-axis label\n  line= -1               # move the title closer / further\n)\n \n\naxis(\n  side=1                   # 1=below, 2=left, 3=above, 4=right\n  at=c(10,20,30),          # position of tick marks\n  labels=c(\"S\",\"M\",\"L\")    # labels for tick marks \n)",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Customizing Plots</span>"
    ]
  },
  {
    "objectID": "6_30-customized-graphics.html#layering-approach",
    "href": "6_30-customized-graphics.html#layering-approach",
    "title": "21  Customizing Plots",
    "section": "21.4 Layering Approach",
    "text": "21.4 Layering Approach\nGraphs in R are created by layering elements on top of each other. The easiest way to understand this is to build a plot from piece by piece.\nThe following graph is comprised of six components:\n\nplot box\npoints\nx-axis\ny-axis\nx-label\ny-label\n\nMany other elements can be added to the plot, but you will see below the flexibility that is gained by mastering even the basic plotting functions.\n\n21.4.1 Load Player Data\nThe Lahman package is an example of a data package in R - one that does not include new functions or analytical tools. Rather, it contains player and team statistics stored as a collection of data frames to make it easy to share and deploy for any analytics involving player or team performance.\nDatasets are loaded from R packages using the data() function.\n\n# install.packages( \"dplyr\" )\n# install.packages( \"Lahman\" )\nlibrary( dplyr )   # data wrangling\nlibrary( Lahman )  # baseball data\ndata( People )\n\n\n\n\n\n\n\n\n\n\n\n\n\nnameFirst\nnameLast\nheight\nweight\nbirthYear\n\n\n\n\nDavid\nAardsma\n75\n215\n1981\n\n\nHank\nAaron\n72\n180\n1934\n\n\nTommie\nAaron\n75\n190\n1939\n\n\nDon\nAase\n75\n190\n1954\n\n\nAndy\nAbad\n73\n184\n1972\n\n\nFernando\nAbad\n74\n235\n1985\n\n\n\n\n\nRemove players that are missing height or weight measures:\n\nPeople &lt;- select( People, height, weight, birthState )\nPeople &lt;- na.omit( People )\n\n\n\n21.4.2 Building the Canvas\nCustom graphics in R allow you to create a blank canvas upon which you will add your data and your data narrative.\nThe canvas is the Cartesian coordinate system upon which you will place the data. You need to define the coordinate system before plotting any data using the xlim= and ylim= arguments.\nThe plot.new() function launches a new graphics window. The plot.window() function sets the coordinates.\n\nplot.new()\nplot.window( xlim=c( 1, 10 ), ylim=c( 1, 5 ) )\n\n\n\n\n\n\n\n\n\n\n\nNew canvas with the grid defined by xlim and ylim values.\n\nNote that data can only be plotted on the canvas, but graphs also contain margins around the canvas used for titles, text, and axes.\n\n\n\n\n\n\n\n\n\nWe need to pick a grid that suits our data. Since we will examine the relationship between height and weight we will use the min and max values of these variables to create the bounding box for our canvas.\n\n# find max and min values for each variable\nxmin &lt;- min( People$height )\nxmax &lt;- max( People$height )\nymin &lt;- min( People$weight )\nymax &lt;- max( People$weight )\n\n# empty plot\nplot.new()\nplot.window( xlim=c(xmin,xmax), ylim=c(ymin,ymax) )\nbox( col=\"black\" )\nbox( \"outer\", col=\"gray\" )\ntext( 64, 200, \"(Empty Canvas)\", cex=2 )\n\n\n\n\n\n\n\n\n\n\n21.4.3 Add Points\nWe can now add some data to the canvas:\n\nplot.new()\nplot.window( xlim=c(xmin,xmax), ylim=c(ymin,ymax) )\nbox( col=\"black\" )\nbox( \"outer\", col=\"gray\" )\npoints( People$height, People$weight, pch=19 )\n\n\n\n\n\n\n\n\n\n\n21.4.4 Add axes\n\n# add an x-axis and y-axis\nplot.new()\nplot.window( xlim=c(xmin,xmax), ylim=c(ymin,ymax) )\nbox( \"outer\", col=\"gray\" )\npoints( People$height, People$weight, pch=19 )\naxis( side=1 )\naxis( side=2, las=1 )   # las turns the y-axis tick mark numbers \n\n\n\n\n\n\n\n\n\n\n21.4.5 Add Axis Labels\n\n# add axis labels\nplot.new()\nplot.window( xlim=c(xmin,xmax), ylim=c(ymin,ymax) )\npoints( People$height, People$weight, pch=19 )\naxis( side=1 )\naxis( side=2, las=1 )\ntitle( xlab=\"Height (inches)\", ylab=\"Weight (lbs)\" )\ntitle( main=\"Relationship Between Height and Weight of MLB Players\" )\n\n\n\n\n\n\n\n\n\n\n21.4.6 Improving Aesthetics\nNow let’s see if we can improve the look and feel of the graph to make the narrative pop.\nThis is pretty dense data, so let’s see if we can use some color transparency to get a sense of typical players versus outliers.\nWe can adjust the xlim and ylim arguments so we don’t show the empty bottom left quadrant, and let’s use the gray() color function to add some transparency to the over-plotted data points.\nThe first argument of the gray() function is a value between 0 and 1 specifying how dark you want the gray (0 being white, 1 being black), and the second argument is another value between 0 and 1 specifying the transparency of the points (0 being invisible and 1 being no transparency).\nWe can also jitter them a bit (a tiny amount of random error to each data point) so they are not all plotted on top of each other since height and weight only take integer values.\n\n# rnorm() adds random noise to a data point\nheight.jitter &lt;- People$height + rnorm( nrow(People) )\nweight.jitter &lt;- People$weight + rnorm( nrow(People) )\n\nplot.new()\nplot.window( xlim=c(62,xmax), ylim=c(110,ymax) )\npoints( height.jitter, weight.jitter, \n        pch=19, cex=1.5, col=gray(0.5,0.1) )\naxis( side=1 )\naxis( side=2 )\ntitle( xlab=\"Height (inches)\", ylab=\"Weight (lbs)\",\n       main=\"Relationship Between Height and Weight of MLB Players\" )\n\n\n\n\n\n\n\n\n\n\n21.4.7 Highlighting Groups\nThis is helpful, but maybe we want to highlight a group within the data.\nPerhaps we have a theory that corn-fed players that grew up in Iowa are taller and weigh more.\n\nplot.new()\nplot.window( xlim=c(60,xmax), ylim=c(110,ymax) )\naxis( side=1 )\naxis( side=2 )\ntitle( xlab=\"Height (inches)\", ylab=\"Weight (lbs)\",\n       main=\"Relationship Between Height and Weight of MLB Players\" )\n\npoints( height.jitter, weight.jitter, \n        pch=19, cex=2, col=gray(0.5,0.02) )\n\n# highlight players from Iowa\n\nthese.iowa &lt;- People$birthState == \"IA\"\n\n# use our selection vector to subset the data used in overplotting\n\npoints( height.jitter[ these.iowa ], \n        weight.jitter[ these.iowa ], \n        pch=19, cex=0.5, col=\"firebrick3\" )\n\npoints( 63, 250, pch=19, col=\"firebrick\" )\ntext( 63, 250, \"Players form Iowa\", \n      pos=4, cex=1.5, col=\"darkgray\" )\n\n\n\n\n\n\n\n\nLet’s add another layer with only the data that falls within the 25th to 75th percentiles of BMI, what we might consider “averaged” sized players relative to their height.\n\nplot.new()\nplot.window( xlim=c(60,xmax), ylim=c(110,ymax) )\naxis( side=1 )\naxis( side=2 )\ntitle( xlab=\"Height (inches)\", ylab=\"Weight (lbs)\",\n       main=\"Relationship Between Height and Weight of MLB Players\" )\n\npoints( height.jitter, weight.jitter, \n        pch=19, cex=2, col=gray(0.5,0.02) )\n\n# highlight average-sized players\n\nbmi &lt;- (People$weight * 0.45359237) / (People$height / 39.370)^2 \n\nbmi.25th &lt;- quantile( bmi, 0.25, na.rm=T )\nbmi.75th &lt;- quantile( bmi, 0.75, na.rm=T )\n\n# group definition:  bmi &gt; bmi.25th & bmi &lt; bmi.75th\n\npoints( height.jitter[ bmi &gt; bmi.25th & bmi &lt; bmi.75th ], \n        weight.jitter[ bmi &gt; bmi.25th & bmi &lt; bmi.75th ], \n        pch=19, cex=0.1, col=\"firebrick3\" )",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Customizing Plots</span>"
    ]
  },
  {
    "objectID": "6_30-customized-graphics.html#highlighting-cases",
    "href": "6_30-customized-graphics.html#highlighting-cases",
    "title": "21  Customizing Plots",
    "section": "21.5 Highlighting Cases",
    "text": "21.5 Highlighting Cases\nLet’s look back at the original plot. Do you see anything strange?\n\n\n\n\n\n\n\n\n\nThe smallest data point in the dataset seems to represent a baseball player that is 40 inches tall and weighs 80 pounds. That must be some sort of data entry error, right?\n\ndata( People) \nPeople[ which.min(People$height) , c(14,15,17:20,2) ] %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nnameFirst\nnameLast\nweight\nheight\nbats\nthrows\nbirthYear\n\n\n\n\n6300\nEddie\nGaedel\n65\n43\nR\nL\n1925\n\n\n\n\n\nIt turns out that Eddie Gaedel, at 3 foot 7 inches, was the shortest man to ever play on a professional team. He was hired by the owner of the St. Louis Browns as a publicity stunt. He was so short that his strike zone, which goes form the knee to mid-chest, was too small for most pitchers. Crowds would cheer as he would quickly accumulate walks.\n\n\n\n\n\n\n\n\n\nPerhaps it would be useful to annotate some of the outliers on our graph. To do this, we can use the text() function.\n\ntext(\n  x=x, y=y,                    # draws a line by connecting points \n  labels=some.text,          # vector of labels to plot on the graph\n  pos=3,                           # position: 1=below, 2=left, 3=above, 4=right\n  cex=2,                           # aspect ratio of text size\n  col=\"red\"                    # color of text\n )\n\nLet’s start by circling some data points. We do this by plotting a slightly larger point around the originals for the tallest, shortest, and heaviest players.\n\nplot.new()\nplot.window( xlim=c(0,1), ylim=c(0,1) )\npoints( 0.5, 0.5, pch=19, col=\"gray\", cex=3 )\npoints( 0.5, 0.5, pch=1, cex=5, col=\"firebrick\" )\ntitle( main=\"Large Hollow Point Plotted \\nOver Solid Point\" )\n\n\n\n\n\n\n\n\n\nthis.shortest &lt;- which.min(People$height)\nthis.tallest  &lt;- which.max(People$height)\nthis.heaviest &lt;- which.max(People$weight)\n\nPeople[ c(this.shortest,this.tallest,this.heaviest), c(14,15,17:20,2) ] %&gt;% pander()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \nnameFirst\nnameLast\nweight\nheight\nbats\nthrows\nbirthYear\n\n\n\n\n6300\nEddie\nGaedel\n65\n43\nR\nL\n1925\n\n\n8399\nSean\nHjelle\n228\n83\nR\nR\n1997\n\n\n20568\nWalter\nYoung\n320\n77\nL\nR\n1980\n\n\n\n\n\nLet’s highlight the three outliers in our dataset.\n\nplot( People$height, People$weight, \n      pch=19, col=\"gray\",\n      frame.plot=FALSE,\n      xlab=\"Height (inches)\", \n      ylab=\"Weight (lbs)\", \n      main=\"Relationship Between Height and Weight of MLB Players\" )\n\npoints( People$height[ this.shortest ], \n        People$weight[ this.shortest ], \n        col=\"firebrick\", cex=2, lwd=2 )\n\npoints( People$height[ this.tallest ], \n        People$weight[ this.tallest ], \n        col=\"firebrick\", cex=2, lwd=2 )\n\npoints( People$height[ this.heaviest ], \n        People$weight[ this.heaviest], \n        col=\"firebrick\", cex=2, lwd=2 )\n\ntext( People$height[ this.shortest ], \n      People$weight[ this.shortest ], \n      \"Some Text Here\", col=\"firebrick\", pos=3 )\n\n\n\n\n\n\n\n\nWe add text to the graph through the x,y coordinate for the text, the text itself, and the pos= argument is used to specify where the text should go.\n\npos=1: below\npos=2: left\npos=3: top\npos=4: right\nno pos: on the point\n\nYou can see that we have a problem above. Our text above Eddie Gaedel is lopped off because it wanders outside of the plot window. We can add additional real estate by expanding plot window with the xlim= and ylim= arguments.\nYou can add a line break to a string by including the carriage return symbol ‘’ in the string.\n\ndata( People) \n\nplot( People$height, People$weight, \n      pch=19, col=\"gray\", cex=0.8,\n      xlim=c(30,100), ylim=c(50,450),\n      frame.plot=FALSE,\n      xlab=\"Height (inches)\", \n      ylab=\"Weight (lbs)\", \n      main=\"Relationship Between Height and Weight of MLB Players\" )\n\nd.small &lt;- People[ c(this.shortest,this.tallest,this.heaviest), c(14,15,17:20,2) ]\n\npoints( d.small$height, \n        d.small$weight, \n        col=\"firebrick\", cex=2, lwd=2 )\n\ntext( People$height[ this.shortest ], \n      People$weight[ this.shortest ], \n      \"Eddie Gaedel \\nHeight: 43 Inches \\nWeight: 65 Lbs\", \n      col=\"firebrick\", cex=0.8, pos=3, offset=1 )\n\ntext( People$height[ this.tallest ], \n      People$weight[ this.tallest ], \n      \"Jon Rauch \\nHeight: 83 Inches \\nWeight: 290 Lbs\", \n      col=\"firebrick\", cex=0.8, pos=4, offset=1 )\n\ntext( People$height[ this.heaviest ], \n      People$weight[ this.heaviest ], \n      \"Walter Young \\nHeight: 77 Inches \\nWeight: 320 Lbs\", \n      col=\"firebrick\", cex=0.8, pos=3, offset=1 )",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Customizing Plots</span>"
    ]
  },
  {
    "objectID": "6_30-customized-graphics.html#using-text-as-plot-points",
    "href": "6_30-customized-graphics.html#using-text-as-plot-points",
    "title": "21  Customizing Plots",
    "section": "21.6 Using Text as Plot Points",
    "text": "21.6 Using Text as Plot Points\nAs you can see, the text() function can be used to add narrative to a graphic. It can also be used in place of plotting points.\nLet’s think about testing the hypothesis that players have gotten larger over time. When we look at the basic relationship between height and weight we see the distribution of player sizes, but this plot has no information about the time-periods in which they played so we can’t tell if they are growing larger over time. What if we replace the plotting points with birth years?\n\nplot.new()\nplot.window( xlim=c(60,xmax), ylim=c(110,ymax) )\n\ntext( x=People$height, \n      y=People$weight, \n      labels=People$birthYear )\n\n\n\n\n\n\n\n\nThis is too dense to be meaningful. Maybe we can try to make the text smaller?\n\nplot.new()\nplot.window( xlim=c(60,xmax), ylim=c(110,ymax) )\ntext( x=People$height, \n      y=People$weight, \n      labels=People$birthYear, cex=0.5 )\n\n\n\n\n\n\n\n\nStill not very insightful. We have over 19,000 players in the database, which appears to be too many for this graphic. Let’s thin the data out by taking a random sample of the full dataset.\n\npar( mar=c(0,0,0,0) )\nm.sample &lt;- sample_n( People, 100 ) \nplot.new()\nplot.window( xlim=c(65,78), ylim=c(130,280) )\ntext( x=m.sample$height, \n      y=m.sample$weight, \n      labels=m.sample$birthYear, cex=0.8 )\naxis( side=1, line=-2 )\naxis( side=2, line=-2 )\n\n\n\n\n\n\n\n\nThis is a big improvement. We can eyeball the data and start to pull out some trends. The people born in the 1800’s tend to be near the bottom of each pile of years, for example.\nPerhaps we can improve our visual hypothesis testing if we add some color coding. Let’s identify all of the individuals born since 1980 and color their birth years red to highlight the youngest cohort in the data. If people have gotten larger over time, we would expect this group to cluster near the top of the distribution.\n\nred.gray &lt;- ifelse( m.sample$birthYear &gt;= 1980, \"firebrick\", \"gray\" )\n\ntext( x=m.sample$height, \n      y=m.sample$weight, \n      labels=m.sample$birthYear,\n      col=red.gray )\n\n\n\n\n\n\n\n\n\n\nAnd we in fact see the pattern emerge. The youngest cohort, highlighted in red, seems to cluster near the top right, which suggests they have grown both taller and heavier over time.\nThis hypothesis can be test more rigorously in other ways, but the demonstration at least gives an idea about how we might explore the data efficiently using flexible R graphing functions.",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Customizing Plots</span>"
    ]
  },
  {
    "objectID": "6_30-customized-graphics.html#margins",
    "href": "6_30-customized-graphics.html#margins",
    "title": "21  Customizing Plots",
    "section": "22.1 Margins",
    "text": "22.1 Margins\nIt is sometimes helpful to include narrative in the margins instead of on actual graph. In these cases you need to expand your margins and use the title(), axis() and mtext() functions. The margins themselves are controlled by the inner margin mar=c argument and outer margin oma=c arguments in the graphical parameters function par(), which is called prior to your plot function to change how a new plot will be laid out.\nSimilar to other parameters, the four argument values refer to the bottom, left, top, and right regions on the graph.\n\npar( mar=c(5,4,4,2), oma=c(3,3,3,3) )\nplot( ... )",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Customizing Plots</span>"
    ]
  },
  {
    "objectID": "6_31-xkcd-stickfigures.html",
    "href": "6_31-xkcd-stickfigures.html",
    "title": "22  Stylized Plot Example",
    "section": "",
    "text": "22.1 Custom Graphics Packages\ninstall.packages( \"xkcd\",dependencies = TRUE )\ninstall.packages( \"gridExtra\" )\ninstall.packages( \"cowplot\" )\nlibrary( ggplot2 )\nlibrary( gridExtra )\nlibrary( cowplot )\nlibrary( xkcd )",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Stylized Plot Example</span>"
    ]
  },
  {
    "objectID": "6_31-xkcd-stickfigures.html#xkcd-style",
    "href": "6_31-xkcd-stickfigures.html#xkcd-style",
    "title": "22  Stylized Plot Example",
    "section": "22.2 xkcd Style",
    "text": "22.2 xkcd Style\n\nxrange &lt;- range(mtcars$mpg)\nyrange &lt;- range(mtcars$wt)\nset.seed(123) # for reproducibility\np &lt;- ggplot() + geom_point(aes(mpg, wt), data=mtcars) +\n     xkcdaxis(xrange,yrange)\np",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Stylized Plot Example</span>"
    ]
  },
  {
    "objectID": "6_31-xkcd-stickfigures.html#add-stick-figure",
    "href": "6_31-xkcd-stickfigures.html#add-stick-figure",
    "title": "22  Stylized Plot Example",
    "section": "22.3 Add Stick Figure",
    "text": "22.3 Add Stick Figure\n\nratioxy &lt;- diff(xrange)/diff(yrange)\n\nmapping &lt;- aes( x, y, scale, ratioxy, \n                angleofspine, \n                anglerighthumerus, \n                anglelefthumerus,\n                anglerightradius, \n                angleleftradius,\n                anglerightleg, \n                angleleftleg, \n                angleofneck,\n                linetype=city )\n\ndataman &lt;- data.frame(x= c(15,30), y=c(3, 4), \n                      scale = c(0.3,0.51) ,\n                      ratioxy = ratioxy,\n                      angleofspine = -pi/2 ,\n                      anglerighthumerus = c(pi/4, -pi/6),\n                      anglelefthumerus = c(pi/2 + pi/4, pi +pi/6),\n                      anglerightradius = c(pi/3, -pi/3),\n                      angleleftradius = c(pi/3, -pi/3),\n                      anglerightleg = 3*pi/2 - pi / 12,\n                      angleleftleg = 3*pi/2 + pi / 12 ,\n                      angleofneck = runif(1, 3*pi/2-pi/10, 3*pi/2+pi/10),\n                      city=c(\"Liliput\",\"Brobdingnag\") )\n\np &lt;- ggplot() + geom_point(aes(mpg, wt, colour=as.character(vs)), data=mtcars) +\n       xkcdaxis(xrange,yrange) +\n       xkcdman(mapping, dataman) +\n       theme(legend.position = \"none\") \n\np",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Stylized Plot Example</span>"
    ]
  },
  {
    "objectID": "6_31-xkcd-stickfigures.html#grid-options",
    "href": "6_31-xkcd-stickfigures.html#grid-options",
    "title": "22  Stylized Plot Example",
    "section": "22.4 Grid Options",
    "text": "22.4 Grid Options\n\n# library( gridExtra )\ngrid.arrange( p, p, p, p, ncol=2, nrow=2 )\n\n\n\n\n\n\n\n# library( cowplot )\nplot_grid( p, p, p, p, \n           labels= c(\"A\",\"B\",\"C\",\"D\"), ncol=2, nrow=2 )\n\n\n\n\n\n\n\n\n\ndataman &lt;- data.frame(x= c(15,30), y=c(3, 4), \n                      scale = c(0.3,0.51) ,\n                      ratioxy = ratioxy,\n                      angleofspine = -pi,\n                      anglerighthumerus = c(pi/2, -pi/3),\n                      anglelefthumerus = c(pi/2 + pi/4, pi +pi/6),\n                      anglerightradius = c(pi/3, -pi/3),\n                      angleleftradius = c(pi/6, -pi/2),\n                      anglerightleg = 3*pi/2 - pi / 8,\n                      angleleftleg = 3*pi/2 + pi / 12 ,\n                      angleofneck = runif(1, 3*pi/2-pi/10, 3*pi/2+pi/10),\n                      city=c(\"Liliput\",\"Brobdingnag\") )\n\np &lt;- ggplot() + geom_point(aes(mpg, wt, colour=as.character(vs)), data=mtcars) +\n       xkcdaxis(xrange,yrange) +\n       xkcdman(mapping, dataman) +\n       theme(legend.position = \"none\") \n\np",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Stylized Plot Example</span>"
    ]
  },
  {
    "objectID": "6_40-ggplot2.html",
    "href": "6_40-ggplot2.html",
    "title": "23  The Grammar of Graphics",
    "section": "",
    "text": "23.4 Why Visualize Data?\nWe visualize data for two principal reasons. We want to:",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>The Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "6_40-ggplot2.html#key-concepts",
    "href": "6_40-ggplot2.html#key-concepts",
    "title": "23  The Grammar of Graphics",
    "section": "23.1 Key Concepts",
    "text": "23.1 Key Concepts\nIn this chapter, we’ll explore the following key concepts:\n\nExploratory Data Analysis (EDA)\nExploratory Data Visualization (EDV)\nExplanatory vs. Explanatory Plots\nThe Grammar of Graphics\nData & Non-Data Ink\nAesthetic Mappings\nAttributes\nOverplotting\nFaceting",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>The Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "6_40-ggplot2.html#new-packages",
    "href": "6_40-ggplot2.html#new-packages",
    "title": "23  The Grammar of Graphics",
    "section": "23.2 New Packages",
    "text": "23.2 New Packages\nThis chapter uses the following packages:\n\nggplot2\nGGally\nscales\nplotly\n\n\nlibrary( datasets )\nlibrary( ggplot2 )\nlibrary( GGally )\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary( scales )\nlibrary( plotly )\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary( tidyverse )\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks plotly::filter(), stats::filter()\n✖ dplyr::lag()        masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>The Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "6_40-ggplot2.html#key-takeaways",
    "href": "6_40-ggplot2.html#key-takeaways",
    "title": "23  The Grammar of Graphics",
    "section": "23.3 Key Takeaways",
    "text": "23.3 Key Takeaways\nToo long; didn’t read? Here’s what you need to know:\n\nWe visualize data to:\n\nSubmit, Publish, Teach (“Explanatory Viz”)\nExplore, Learn, Share (“Exploratory Viz”)\n\nExploratory viz is a key part of any data analysis\n“Grammar of Graphics” framework inspired “ggplot2”\n“Layers” are like visual parts of speech\n\nThere are 7 layers - 3 are essential\nEach layer has a family of functions\n\nEssential layers include:\n\nData: Intakes data frame/tibble\nAesthetic: Maps variables to axes, color, etc.\nGeometry: Specifies the shape of your data\n\nNonessential layers include:\n\nStatistics: Modeling and computation\nCoordinates: Zooming in and modifying scales\nFacets: Small multiples, a.k.a. trellis plots\nThemes: Overall style - grid lines, text, etc.\n\nVariety of ggplot2 extensions (see resources)\nUse package “plotly” to make ggplot2 interactive",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>The Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "6_40-ggplot2.html#why-visualize-data",
    "href": "6_40-ggplot2.html#why-visualize-data",
    "title": "23  The Grammar of Graphics",
    "section": "",
    "text": "Learn about our data, i.e. exploratory data visualization\nTell our data’s stories to others, i.e. explanatory data visualization\n\n\n\n23.4.1 Explanatory Visualization\nExplanatory visualization is polished, publication-quality, and interpretable:\n\nMeant to be consumed by broad, non-specialist audiences\nTakes significant time and iterations to perfect\nConveys one or two “big ideas”, each\n\n\nJust note the length of the code and its result. What idea does this plot convey?\n\nlibrary(ggplot2)\n\nggplot(read_csv(url), aes(x = val, y = myr)) +\n  geom_jitter(alpha = 0.3, color = \"tomato\") +\n  geom_smooth(method = \"lm\", se = FALSE, \n              color = \"grey50\", lwd = 1, alpha = 0.3) +\n  facet_wrap(~ elm) +\n  labs(title = \"Annual income vs. Holland personality scores\", \n       x = \"Personality Score\",\n       y = \"Income (K)\",\n       caption = \"Sources: US DOL O*NET & BLS\") +\n  scale_y_continuous(labels = c(\"0\", \"50\", \"100\", \"150\", \"$200\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nTo experiment with Holland codes and predicted income, check it out in Shiny.\n \n\n\n23.4.2 Exploratory Visualization\nExploratory visualization is “quick and dirty” and intends to discover.\n\nMeant to be consumed by yourself, colleagues, or other specialists\nCreated quickly, with no polish, refinement, or audience in mind\nCan convey many ideas, or none - that’s why you do it!\n\n\nThis code is manageable! Observe a simple call to ggpairs() and the iris dataset.\n\nlibrary( ggplot2 )\nlibrary( GGally )\n\nggpairs( data=iris, aes( color=Species ) )\n\n\n\n\n\n\n\n\n\nWith just a little code, pairs plots visualize every variable against the other.\n\n\nHopefully, you wouldn’t publish this. But we can quickly find patterns in a pairs plot:\n\nWe can see positive correlations between sepal and petals for two species\nWe observe that “Setosa” have thinner, longer sepals compared to others\nWe also observe that “Versicolor” has the least variation in size\n\nSuch visual exploration can help refine hypotheses before analysis begins!\n \n\n\nYOUR TURN: EXPLORATORY VIZ WITH PAIRS PLOTS\n\nLoad the necessary packages with library().\nThen, call ggpairs() on the economics dataset from ggplot2.\n\n\n\n\ndc_light_exercise_example-01\n\n \n\n\n23.4.3 Do I have to Visualize?\nExploratory viz is a key component in exploratory data analysis, or EDA.\nFailing to visually explore your data can get you in hot water. Let’s try it!\n\nObserve the following data frame containing four data sets.\nVariable x1 corresponds to y1, x2 to y2, and so forth:\n\nlibrary(datasets)\n\nanscombe\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n\n\nAt a glance, they look like the have some similarities!\n\nCheck It: Let’s perform a few statistical EDA functions on each subset, 1-4.\nWhat do you notice when we figure out and organize:\n\nAverage of all X and Y values with mean()\nVariance of all X and Y values with var()\nCorrelation between X and Y for all sets with cor()\nLinear regression coefficients between X and Y with lm()\n\n\n\n      x_mean   y_mean x_var    y_var    correl     coeff\nSet 1      9 7.500909    11 4.127269 0.8164205 0.5000909\nSet 2      9 7.500909    11 4.127629 0.8162365 0.5000000\nSet 3      9 7.500000    11 4.122620 0.8162867 0.4997273\nSet 4      9 7.500909    11 4.123249 0.8165214 0.4999091\n\n\n\nHeavens to Murgatroyd! These are practically the same sets!\n\nThe mean and variance of X is the exact same across sets\nThe mean and variance of Y is almost exactly the same across sets\nThe correlation between X & Y is extremely close across sets\nThe coefficient of determination is also extremely close\n\n\nOnce visualized, all four linear relationships appear to be exactly the same:\n\n\n\n\n\n\n\n\n\n\n\n\nWell, that settles that. Pack it up, folks - the data are the same.\n\nHold up.\n\nWait a minute.\n\nSomething ain’t right.\n\nLet’s try replotting the actual datasets and not just their linear models.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoom! Not the same datasets at all.\n\nDespite having the same mean, variance, correlations, and coefficients:\n\nDataset 1 is a normally distributed linear relationship\nDataset 2 is a parabolic curve\nDataset 3 is a perfect linear relationship with a high-leverage outlier\nDataset 4 shows absolutely no relationship but again has an outlier\n\n\nConclusion: Always conduct exploratory visualization as a staple of any analysis.\n \n\nFUN FACT:\nYou just got Anscombe’d. Francis John “Frank” Anscombe was an English statistician who helped pioneer the field in the twentieth century. He wrote:\n\n\n“…a computer should make both calculations and graphs…”\n\n\nTo demonstrate, Anscombe invented these datasets: Anscombe’s Quartet.",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>The Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "6_40-ggplot2.html#the-grammar-of-graphics",
    "href": "6_40-ggplot2.html#the-grammar-of-graphics",
    "title": "23  The Grammar of Graphics",
    "section": "23.5 The Grammar of Graphics",
    "text": "23.5 The Grammar of Graphics\nWe’ve compared human language and programming languages in the past.\nLet’s take the analogy further. Observe the following sentence:\n\n\n\n\n\nThe quick brown fox jumps over the lazy dog.\n\n\n\n\n\nSource: Chionetti, A. (2013)\n\n\nRecall that language consists of nouns, verbs, adjectives, articles, prepositions, etc.\nIf we change any part of speech, we change the meaning of our sentence. Observe:\n\n\nThe quick brown fox jumps off the lazy dog.\n\n\nNow the fox has escalated things, opting to use the poor dog as a springboard.\n\n\nThe quick brown fox runs over the lazy dog.\n\n\nWell this is just getting graphic.\n\n\nThe decrepit brown fox jumps over the lazy dog.\n\n\nHe’s a good dog.\n\nParts of Speech: Every part of speech (like adjectives, e.g. “quick”, “lazy”) has a function.\nNouns describe things, verbs describe actions, adjectives describe qualities, etc.\n\nParts of Viz: Like language, there are parts of visualization. Each part has a function.\n\nA chart’s data could be anything, like quarterly revenues or ELA scores\nA chart’s geometry could represent the data as bars, points, lines, or shapes\nA chart’s theme could use different fonts, gridlines, transparencies, etc.\n\n\nThese are just some parts of viz in a larger framework: The Grammar of Graphics.\n \n\n23.5.1 A Brief Overview\nIn 1999, statistician Leland Wilkinson published The Grammar of Graphics.\nThis framework allows us to dissect and alter plots in the same way we would a sentence.\nLet’s begin with parts of viz, or layers.\n \n\n\n23.5.2 Layers: Parts of Viz\nIn the grammar of graphics framework, each visualization is comprised of layers.\n\nEach layer performs a unique function in a visualization\nLike parts of speech, a layer can perform one function in infinite ways\n\nFor example, the data layer functions to input your data\nA noun can be “fox” or “dog; a dataset can be”DOL” or “TSA”\n\n\n \n\n\n23.5.3 Essential Layers: Data, Mappings, & Shapes\nLike human sentences, every complete visualization has 3 essential layers:\n  \n\n\n\n\n\nThe three essential layers for a complete visualization.\n\n\n\n\n  \n\n23.5.3.1 The Data Layer\nThe data layer conveys the dataset informing the visualization.\n\nThe data layer inputs a dataset, but doesn’t specify the variables you show\nVisualizations can have more than one data layer, e.g. overlay plots\n\n\nThe data layer in everyday conversation:\n\n“Are you pulling occupations from O*NET or BLS? We only need SOC-level.”\n\n\nIntepretation: Your data layer is comprised of your data source.\n \nObserve the same plot - the only difference is the data layer. What changes?\n \n\n\n\n\n\n\n\n\n\n \n\n\n23.5.3.2 The Aesthetics Layer\nThe aesthetics layer conveys which variables to visualize.\n\nAesthetics refers to the visual ways to represent variables\nAesthetics include size, shape, color, fill, line type, transparency, etc.\nThe most common aesthetic mapping is using x- and y-axes to show quantities\nAesthetics adjust dynamically with your data - they are not static\n\n\nThe aesthetics layer in everyday conversation:\n\n“Can you color-code the datapoints by gender?”\n\n\nIntepretation: Aesthetics use visual elements like color to convey more data.\n \nObserve the same plot - the only difference is the aesthetics layer. What changes?\n \n\n\n\n\n\n\n\n\n\n \n\n\n23.5.3.3 The Geometry Layer\nThe geometry layer conveys the shape your variables use to visualize data.\n\nGeometries include scatter plots, bar charts, line graphs, etc.\nSome geometries are only compatible with certain variables\nGeometries can also take attributes, like size, shape, color, etc.\nGeometry attributes do not adjust dynamically with data - they are static\n\n\nThe geometry layer in everyday conversation:\n\n“I’m trying to emphasize the increase in elevated blood lead levels over time.”\n\n\nIntepretation: It sounds like they’d want a line graph or bar chart to show change.\n \nObserve the same plot - the only difference is the geometry layer. What changes?\n \n\n\n\n\n\n\n\n\n\n \n\nPRO TIP\nIt seems like a small distinction, but the difference is critical:\n\nBoth “Attributes” and “Aesthetics” use color, size, line width, fill, etc.\n“Attributes” are static and unchanging\n“Aesthetics” are dynamic and change with your data\n\n\nAll “aesthetics” are specified in the “aesthetics” layer.\nAll “attributes” are specified in the “geometries” layer.\n\n \n\n\n\n23.5.4 Remaining Layers\nThe 4 remaining layers in the grammar of graphics include:\n\nThe coordinates layer modifies plot zoom, truncation, and labeling\nThe statistics layer performs statistical transformations like lines of best fit\nThe facets layer creates multiple comparison plots, a.k.a. small multiples\nThe themes layer modifies font, gridlines, and other “non-data ink” polish\n\n\nWe’ll explore some key functions from these layers in the following sections.",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>The Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "6_40-ggplot2.html#download-the-practice-data",
    "href": "6_40-ggplot2.html#download-the-practice-data",
    "title": "23  The Grammar of Graphics",
    "section": "23.6 Download the Practice Data",
    "text": "23.6 Download the Practice Data\nOur practice data are public consruction project worker records in Syracuse, NY.\nThey were collected for a racial equity impact statement on hiring disparities.\n\nDownload: You can download the practice data here.\n\nTo Follow Along: Run this code in your R console:\n\nlibrary(readr)\n\nurl &lt;- paste0(\"https://raw.githubusercontent.com/DS4PS/\",\n              \"dp4ss-textbook/master/tables/hancock.csv\")\n\nhancock &lt;- read_csv(url)\n\nrm(url)\n\n\n\n\n\n\n\nThe impact statement uses ggplot2 for the majority if its viz (p. 95).",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>The Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "6_40-ggplot2.html#package-ggplot2",
    "href": "6_40-ggplot2.html#package-ggplot2",
    "title": "23  The Grammar of Graphics",
    "section": "23.7 Package “ggplot2”",
    "text": "23.7 Package “ggplot2”\nPackage “ggplot2” is a popular, powerful package for data visualization in R.\n\nAuthored by Hadley Wickham; maintained by RStudio\nAn implementation of the “Grammar of Graphics” framework (hence “gg”)\nHas a series of function families, each corresponding to a different layer\n\nExpressions in “ggplot2” use a particular syntax. Note that + connects the layers.\n\nLet’s look at a “complete” graph:\n\nlibrary(ggplot2)\n\nggplot(data = hancock) +      # Data layer\n  aes(x = net) +              # Aesthetics layer\n  geom_histogram()            # Geometry layer\n\n\n\n\n\n\n\n\n\nNote that the preferred “ggplot2” format is as follows (both do the same thing):\n\nggplot(data = hancock,\n       aes(x = net)) +        # \"aes()\" is nested in \"ggplot()\"\n  geom_histogram()\n\n\nLet’s break it down. Here are the three layers of a “ggplot2” graphic and how they work:\n\n\n\n\n\n\nBreaking down a “complete” graphic.\n\n\n\n\n \n\n\nYOUR TURN: A BASIC GGPLOT\n\n\nLoad the necessary packages with library()\nCall ggplot() on the economics dataset from ggplot2\nIn aes(), map x = to date and y = to unemploy\nCall geom_line()\n\n\nA call to theme_classic() has been added for panache.\n\n\n\n\ndc_light_exercise_example-02\n\n \nAdding Aesthetics: Let’s plot the same data, except with:\n\naes() argument x = mapped to net\naes() argument y = mapped to gross\ngeom_histogram() changed to geom_point()\n\n\nggplot(data = hancock,\n       aes(x = net,\n           y = gross)) +    # Now mapping \"gross\" to y-axis\n  geom_point()              # Changed from geom_histogram()\n\n\n\n\n\n\n\n\n\nModifying Attributes: We can change attributes (non-data ink) in the geom_*() call.\n\nModify transparency with argument alpha =\nModify color and fill with color = and fill =, respectively\n\n\nggplot(data = hancock,\n       aes(x = net,\n           y = gross)) +\n  geom_point(alpha = 0.35,        # Modify transparency between 0 & 1\n             color = \"tomato\")    # Modify color; some colors are known\n\n\n\n\n\n\n\n\n\nModify Labels: We can add function labs() with + to modify labels and titles:\n\nggplot(data = hancock,\n       aes(x = net,\n           y = gross)) +\n  geom_point(alpha = 0.35,\n             color = \"tomato\") +\n  labs(title = \"Weekly gross v. net incomes in public construction\",\n       subtitle = \"Hancock Airport, 2018\",\n       x = \"Net Earnings\",\n       y = \"Gross Earnings\",\n       caption = \"Source: Syracuse Regional Airport Authority\")\n\n\n\n\n\n\n\n\n\nPremade Themes: Add premade themes with + and theme_ functions:\n\nggplot(data = hancock,\n       aes(x = net,\n           y = gross)) +\n  geom_point(alpha = 0.35,\n             color = \"tomato\") +\n  labs(title = \"Weekly gross v. net by public construction workers\",\n       subtitle = \"Hancock Airport, 2018\",\n       x = \"Net Earnings\",\n       y = \"Gross Earnings\",\n       caption = \"Source: Syracuse Regional Airport Authority\") +\n  theme_minimal()                                                   # New theme\n\n\n\n\n\n\n\n\n\nConclusions: In package “ggplot2”, we build on these three essential layers.\n\nThe data layer, the aesthetics layer, and the geometry layer\nCall data, aesthetics, and geometry in ggplot(), aes(), and geom*() functions\nOnce you’ve build an initial plot, you can add new layers to enhance it, like themes\nEach addition makes plots more decodable and better emphasizes big ideas\n\n \n\n\nYOUR TURN: AESTHETICS & ATTRIBUTES\n\nA basic ggplot has been prepared for you.\nLoad the necessary packages with library().\n\nModify geom_point() with the following:\n\nSet size to 2.75 (size =)\nSet transparency to 0.15 (alpha =)\nSet color to “darkslateblue” (color = )\n\n\nA theme_minimal() is provided for panache.\n\n\n\n\ndc_light_exercise_example-03",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>The Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "6_40-ggplot2.html#notable-layer-functions",
    "href": "6_40-ggplot2.html#notable-layer-functions",
    "title": "23  The Grammar of Graphics",
    "section": "23.8 Notable Layer Functions",
    "text": "23.8 Notable Layer Functions\nEach layer in “ggplot2” has an astounding number of functions. Let’s check some out.\n \n\n23.8.1 The Aesthetics Layer\nWe’ve seen the principal function of the aesthetics layer, aes().\nMost importantly, you should some arguments to which you can map variables:\n\nx = maps to the x-axis\ny = maps to the y-axis\nz = maps to the z-axis (3-dimensional, typically)\ncolor = maps to colors of points and lines\nfill = maps to colors of shapes, e.g. bars\nsize = maps to the size of points\nlabels = maps to the labels of points, lines, and shapes\nwidth = maps to the width of lines\ntype = maps to the line type\n\nRead more about aesthetics in the “ggplot2” vignette.\n\nTake the following scatter plot of U.S. unemployed against total population since 1970:\n\nlibrary(ggplot2)\n\nggplot(data = economics, \n       aes(x = unemploy,\n           y = pop)) +\n  geom_point() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe can convey additional information by mapping aesthetics to other variables.\nThe following maps point size to psavert, or the personal savings rate.\n\nlibrary(ggplot2)\n\nggplot(data = economics, \n       aes(x = unemploy,\n           y = pop,\n           size = psavert)) +     # Map personal savings rate to size\n  geom_point(alpha = 0.2) +       # Set transparency to 20%\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe can see that regardless of unemployment, savings dwindle as populations grow.\n(Population, by the way, is practically interchangeable with date).\n \n\nCAUTION: OVERPLOTTING & OVERMAPPING\n\nIt’s tempting to map many variables to different aesthetics, so be careful to not:\n\n“Overplot”, or obfuscate clarity with too many mappings\n“Overmapping”, or mapping a variable to two or more aesthetics\n\n\nFor example, don’t map “population”, e.g. to both color and the y-axis.\n\nAs datapoints move upward on the y-axis, the color changes\nIt may make your plot pretty, but it makes it harder to decode\nWhat’s more, color is redundant and pointless - axes are more decodable\nIn effect, this is known in data visualization as “belt-and-suspenders design”\n\n\nBelts or suspenders… you don’t need both!\n\n \n\n\n23.8.2 The Geometry Layer\nEach shape your data takes in the geometry layer has an associated function.\n\nView all geometries on the “ggplot2” reference page\nNotably, every geometry function begins with geom_\nSome statistical functions have equivalents that begin with stat_\nYou can combine more than one geom function in a singple visual\n\n\nRectangular Geoms: When using geoms with rectangular shapes, set stat = to “identity”.\n\noptions(scipen = 999)                       # Disable scientific notation\n\nggplot(data = hancock, \n       aes(x = reorder(ethnicity, -net),    # Reorder ethnicities by \"net\" \n           y = net)) +\n  geom_bar(stat = \"identity\") +             # Specify stat = \"identity\"\n  labs(x = NULL,\n       y = \"Weekly Net\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nUse Fill to Modify Geoms: Map aes() argument fill = to a categorical variable:\n\nggplot(data = hancock, \n       aes(x = reorder(ethnicity, -net), \n           y = net,\n           fill = reorder(class, -net))) +  # Add fill = and reorder() on \"net\"\n  geom_bar(stat = \"identity\") +\n  labs(x = NULL,\n       y = \"Weekly Net\",\n       fill = \"Class\") +                    # Use labs() and fill= to title legend\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nFill to 100%: With a fill = asthetic, set position = to “fill”:\n\nggplot(data = hancock, \n       aes(x = reorder(ethnicity, -net), \n           y = net,\n           fill = reorder(class, -net))) +\n  geom_bar(stat = \"identity\",\n           position = \"fill\") +             # Set position = to \"fill\"\n  labs(x = NULL,\n       y = \"Weekly Net\",\n       fill = \"Class\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nNew Geoms with New Positions: With a fill = asthetic, set position = to “dodge”:\n\nggplot(data = hancock, \n       aes(x = reorder(ethnicity, -net), \n           y = net,\n           fill = reorder(class, -net))) +\n  geom_bar(stat = \"identity\",\n           position = \"dodge\") +            # Set position = to \"dodge\"\n  labs(x = NULL,\n       y = \"Weekly Net\",\n       fill = \"Class\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nOverlapping Barplots: Set position = to position_dodge() and set width =:\n\nggplot(data = hancock, \n       aes(x = reorder(ethnicity, -net), \n           y = net,\n           fill = reorder(class, -net))) +\n  geom_bar(stat = \"identity\",\n           position = position_dodge(width = 0.5)) +            # Control the dodge!\n  labs(x = NULL,\n       y = \"Weekly Net\",\n       fill = \"Class\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n \n\nFUN FACT\nMany ggplot2 color functions contain brewer in the name. That’s because, by default, ggplot2 uses color palettes based on the research of Dr. Cynthia Brewer.\nDr. Brewer’s premade palettes, all recognized in ggplot2, are available in Color Brewer 2.\n\n \nOverlap & Transparency: When multiple points overlap, use transparency (alpha =).\n\nggplot(data = hancock, \n       aes(x = reorder(ethnicity, -net), \n           y = net)) +\n  geom_point(alpha = 0.10) +          # Set transparency to 10%\n  labs(x = NULL,\n       y = \"Weekly Net\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n \nOverlapping & Jittering: In combination with transparency, you can “jitter” your points.\n\n“Jittering” adds random horizontal and vertical noise to break up points\nIn function geom_point(), set position = to “jitter”\nArguments height = and width = controls jitter\nAlternatively, use function geom_jitter()\n\n\nggplot(data = hancock, \n       aes(x = reorder(ethnicity, -net), \n           y = net)) +\n  geom_jitter(alpha = 0.15,           # Change geom to geom_jitter()\n              width = 0.25) +         # Set \"jitter\" width to 0.25\n  labs(x = NULL,\n       y = \"Weekly Net\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n \n\n\nYOUR TURN: JITTER & TRANSPARENCY\n\nLoad the necessary packages with library().\nThen, call ggplot() on the midwest dataset from ggplot2.\n\n\nChange geom_point() to geom_jitter()\nIn geom_jitter(), set argument alpha = 0.25\nAlso in the geom, set argument width = 0.15\n\n\n\n\n\ndc_light_exercise_example-04\n\n \n\n\n23.8.3 The Statistics Layer\nThe statistics layer is where data undergo statistical transformations under the hood.\n\nAll stat layer functions begin with stat_\nMany geometries already use stats, like geom_smooth()\nArgument method = can use different models with “lm”, “glm”, “rlm”, etc.\nStat functions typically not called directly, but run within geom functions\n\n\nSmoothers: Function stat_smooth() adds a LOESS line and standard error area.\n\nggplot(hancock, \n       aes(x = net,\n           y = gross)) +\n  geom_point() +\n  stat_smooth() +                 # Function stat-smooth(), same as geom_smooth()\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nRemove Standard Error: Set argument se = to FALSE to disable standard error.\n\nggplot(hancock, \n       aes(x = net,\n           y = gross)) +\n  geom_point() +\n  stat_smooth(se = FALSE) +       # Argument se = controls standard error\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nChane Modeling Method: Set argument method = to “lm” for linear models, etc.\n\nggplot(hancock, \n       aes(x = net,\n           y = gross)) +\n  geom_point() +\n  stat_smooth(se = FALSE,\n              method = \"lm\") +    # Argument method = changes your model\n  theme_minimal()\n\n\n\n\n\n\n\n\n \n\n\n23.8.4 The Coordinates Layer: Planes\nThe coordinates layer ultimately controls the limits and labels of your axes.\n\nAll coordinates functions begin with coord_\nMost common is coord_cartesian(), which modifies a plot’s Cartesian plane\n\n\nFlipping Coordinates! No, we’re not cursing out coordinates.\nHowever, you can reverse the x- and y-axes by using function coord_flip():\n\n\n\n\n\n\n\n\n\n\noptions(scipen = 999)                 # Disable scientific notation\n\nggplot(data = hancock, \n       aes(x = reorder(ethnicity, net), \n           y = net)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Flipped!\",\n       x = NULL,\n       y = \"Weekly Net\") +\n  theme_minimal() +\n  coord_flip()                        # coord_flip() !\n\n\n\n\n\n\n\n\n \nZooming or Filtering: Modifying coordinates typically goes one of two ways:\n\nZoom in on some specified coordinates\nZoom in and filter out data that are no longer visible\n\nZoom with coord_cartesian().\nFilter with lims().\n\nThe Whole Plot (Before): Observe the entirety of the plot without zooming in:\n\nggplot(hancock,\n       aes(x = net,\n           y = gross)) +\n  geom_point() +\n  geom_smooth() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nZoomed In: We can zoom, without affecting our data, with coordcartesion().\nNote the direction and standard error of the LOESS line from geom_smooth().\n\nggplot(hancock,\n       aes(x = net,\n           y = gross)) +\n  geom_point() +\n  geom_smooth() +\n  theme_minimal() +\n  coord_cartesian(xlim = c(0, 1250), \n                  ylim = c(0, 2000))    # Zoom in and keep your data!\n\n\n\n\n\n\n\n\n\nFiltered: We can use lims() to zoom in and filter data no longer plotted.\nNow note how the LOESS line and standard error have changed per this “new” dataset.\n\nggplot(hancock,\n       aes(x = net,\n           y = gross)) +\n  geom_point() +\n  geom_smooth() +\n  theme_minimal() +\n  lims(x = c(0, 1250), \n       y = c(0, 2000))                  # Zoom in and filter your data!\n\n\n\n\n\n\n\n\n \n\nQUESTION\nWhy do the LOESS curves and standard errors change between plots?\n\n \n\n\n23.8.5 The Coordinates Layer: Scales\nScales in “ggplot2” affect your axis gridlines, ticks, breaks, and labels.\n\nScale functions begin with scale_\nSpecify which axis with scale_x_ or scale_y_\nAlso specify discrete, continuous, or another variable type\nExample: A discrete (categorical) variable on the y-axis is scale_y_discrete()\n\n\nAxis Titles: Change (or remove) axis titles with argument name =:\n\nggplot(hancock,\n       aes(x = net,\n           y = gross)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(color = \"grey80\", alpha = 0.1) +\n  theme_minimal() +\n  scale_x_continuous(name = \"Weekly Net\") +     # Axis? Continuous or discrete?\n  scale_y_continuous(name = \"Weekly Gross\")     # Argument name = for axis titles\n\n\n\n\n\n\n\n\n\nBreaks: Specify axes’ major label and tick marks with argument breaks = and c():\n\nggplot(hancock,\n       aes(x = net,\n           y = gross)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(color = \"grey80\", alpha = 0.1) +\n  theme_minimal() +\n  scale_x_continuous(name = \"Weekly Net\",\n                     breaks = c(0, 1250, 2500)) +\n  scale_y_continuous(name = \"Weekly Gross\",\n                     breaks = c(0, 1500, 3000))           # Even breaks\n\n\n\n\n\n\n\n\n\nLabels: Customize individual labels with argument labels = (ojo - can get lengthy):\n\nggplot(hancock,\n       aes(x = net,\n           y = gross)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(color = \"grey80\", alpha = 0.1) +\n  theme_minimal() +\n  scale_x_continuous(name = \"Net\",\n                     breaks = c(0, 1250, 2500),\n                     labels = c(\"$ 0\", \"1.25\", \"2.5 K\")) +\n  scale_y_continuous(name = \"Gross\",\n                     breaks = c(0, 1500, 3000),\n                     labels = c(\"$ 0\", \"1.5\", \"3 K\"))     # Labels in quotes\n\n\n\n\n\n\n\n\n \nPretty Breaks: Package “scales” allows us to more easily format scale_*() functions.\n\nFormat breaks automatically (pretty-like)\nLabel dollar amounts and percentages with ease\n\n\nPretty Names: With “scales”, argument name = accepts new standards:\n\nlibrary(scales)\n\nggplot(hancock,\n       aes(x = net,\n           y = gross)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(color = \"grey80\", alpha = 0.1) +\n  theme_minimal() +\n  scale_x_continuous(name = \"Net\",\n                     breaks = c(0, 1250, 2500),\n                     labels = dollar) +                   # Auto-labeling: Good\n  scale_y_continuous(name = \"Gross\",\n                     breaks = c(0, 1500, 3000),\n                     labels = dollar)                     # Less flexibility: Bad\n\n\n\n\n\n\n\n\n\nPretty Breaks: With “scales”, argument breaks = accepts pretty_breaks():\n\nlibrary(scales)\n\nggplot(hancock,\n       aes(x = net,\n           y = gross)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(color = \"grey80\", alpha = 0.1) +\n  theme_minimal() +\n  scale_x_continuous(name = \"Net\",\n                     breaks = pretty_breaks(5),\n                     labels = dollar) +                   # pretty_breaks() !\n  scale_y_continuous(name = \"Gross\",\n                     breaks = pretty_breaks(12),\n                     labels = dollar)                     # Pretty sick\n\n\n\n\n\n\n\n\n \n\nPRO TIP: PACKAGE “SCALES” APPLICATIONS\nThe “scales” package has a host of amazing formatting functions. Commonly:\n\ndollar() and dollar_format() for currency\npercent() and percent format() for percents\nnumber() and number_format() for numbers\n\nThese and other functions are incredibly useful, especially for cleaning.\n\n \n\n\nYOUR TURN: STATS, COORDINATES, AND SCALES\n\nLoad the necessary packages with library().\n\n\nIn geom_point(), set “alpha = 0.3”\nAdd geom_smooth() with +\nAdd lims() with:\n\nx = c(0, 10000)\ny = c(0, 50)\n\nIn scale_x_continuous(), add “label = name”\nAdd labs() with:\n\nx = “Population Density”\ny = “College Educated (%)”\n\n\n\n\n\n\ndc_light_exercise_example-05\n\n \n\n\n23.8.6 The Faceting Layer\nThe faceting layer allows us to create facets, a.k.a small multiples or trellis charts.\n\nAll faceting functions begin with facet_\nfacet_wrap() creates a grid; facet_grid() creates side-by-sides\nThere are other arguments, but the formula x ~ y (variables), is critical\n\n\nSingle-Variable Faceting: The formula x ~ y needs only one side.\nYou can replace the empty side with . or leave it blank.\n\nggplot(hancock,\n       aes(x = net,\n           y = gross)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(alpha = 0.5,\n              method = \"lm\") +\n  theme_minimal() +\n  scale_x_continuous(name = \"Net\",\n                     breaks = pretty_breaks(3),\n                     labels = dollar) +\n  scale_y_continuous(name = \"Gross\",\n                     breaks = pretty_breaks(3),\n                     labels = dollar) +\n  facet_wrap(facets = ethnicity ~ .)                     # Only one side needed\n\n\n\n\n\n\n\n\n\nFacet Grid: When you want side-by-side comparisons for all plots, use facet_grid():\n\nggplot(hancock,\n       aes(x = net,\n           y = gross)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(alpha = 0.5,\n              method = \"lm\") +\n  theme_minimal() +\n  scale_x_continuous(name = \"Net\",\n                     breaks = pretty_breaks(3),\n                     labels = dollar) +\n  scale_y_continuous(name = \"Gross\",\n                     breaks = pretty_breaks(3),\n                     labels = dollar) +\n  facet_grid(facets = . ~ ethnicity, )                     # Now with facet_grid() !\n\n\n\n\n\n\n\n\n\nModify facet_*() functions with arguments beyond the x ~ y formula.\n\nrows = accepts the number of rows in a facet grid\ncols = accepts the number of columns\nlabeller = accepts new strip names\n\n \n\n\nYOUR TURN: FACETING\n\nUnfortunately, the R widget cannot visualize faceting (yet).\nOpen a new RStudio script. Copy and paste the below code.\nUse a facet_*() function on variable cut.\n\n\n\n\ndc_light_exercise_example-06\n\n \n\n\n23.8.7 The Themes Layer\nThe themes layer allows you to customize ever pixel of non-data ink in your plot.\n\nFunction themes() is the workhorse function for this layer\nthemes() arguments are a large but consistent framework\nFunctions beginning with theme_ set premade themes\nThe themes layer adjusts every non-data pixel\n\n\nEverything we’ve done so far, plus changing the facet strip labels:\n\nggplot(hancock,\n       aes(x = net,\n           y = gross)) +\n  geom_point(alpha = 0.5,\n             color = \"grey30\") +\n  geom_smooth(alpha = 0.5,\n              method = \"lm\") +\n  theme_minimal() +\n  scale_x_continuous(name = \"Net\",\n                     breaks = pretty_breaks(3),\n                     labels = dollar) +\n  scale_y_continuous(name = \"Gross\",\n                     breaks = pretty_breaks(3),\n                     labels = dollar) +\n  facet_wrap(facets = . ~ ethnicity) +\n  theme(strip.text = element_text(face = \"bold\",\n                                  color = \"grey40\",\n                                  family = \"Verdana\"))   # The framework in action\n\n\n\n\n\n\n\n\n\nSo what the hell was that? There’s a whole consistent framework here!\nThere’s a few hierarchies of functions and arguments that we’ll look at below.\nWe’ve saved the above plot in object p, which we’ll use herein.\n\nFirst Entering theme(): You’ll get hit with four short arguments:\n\nline = modifies all lines in the plot\nrect = modifies all rectangles in the plot\ntext = modifies all text in the plot\ntitle = modifies just titles in the plot\n\n\np + theme(text = ...)\n\n\nLatter Arguments in theme(): More specific argument families for each element.\n\naxis arguments for axis lines, ticks, and text\nlegend arguments for boxes, keys, spacing, text, and titles\npanel arguments for overall plot spacing and grid lines\nplot arguments for tags, titles, and margins\nstrip arguments for facet strip text and backgrounds\n\n\np + theme(axis.title = ...)\n\n\nOnce Argument is Selected: Arguments typically accept theme element functions:\n\nAll element functions begin with element_\nelement_rect() applies to rectangles\nelement_line() applies to lines\nelement_text() applies to text\nelement_blank() removes it\n\n\np + theme(axis.title = element_text(color = \"gray50\",\n                                    face = \"bold\",\n                                    size = 10))\n\n\n\n\n\n\n\n\n \n\n\nYOUR TURN: CUSTOMIZING TITLE THEMES\n\nLoad package scales to help format our scales. In themes():\n\nChange argument ‘x =’ in labs() to “Year”\nFormat the plot’s axes title color to “grey20”\nMake the main title size 12 and “bold” face\nContinue experimenting with modifications\n\n\n\n\n\ndc_light_exercise_example-07",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>The Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "6_40-ggplot2.html#interactive-charts-with-plotly",
    "href": "6_40-ggplot2.html#interactive-charts-with-plotly",
    "title": "23  The Grammar of Graphics",
    "section": "23.9 Interactive Charts with “plotly”",
    "text": "23.9 Interactive Charts with “plotly”\nOn a brief side note, you can make your “ggplot2” plots interactive with “plotly”.\n\nSaving “ggplot” Objects: You can save plots as objects with assignment, or &lt;-:\n\nlibrary(scales)\nlibrary(ggplot2)\n\np &lt;- ggplot(hancock,              # Modified plot assigned to \"p\" with &lt;-\n       aes(x = net,\n           y = gross)) +\n  geom_point(alpha = 0.5,\n             color = \"grey30\") +\n  geom_smooth(alpha = 0.5,\n              method = \"lm\") +\n  theme_minimal() +\n  scale_x_continuous(name = \"Weekly Net\",\n                     labels = dollar,\n                     breaks = pretty_breaks(3)) +\n  scale_y_continuous(name = \"Weekly Gross\",\n                     labels = dollar,\n                     breaks = pretty_breaks(4)) +\n  facet_wrap(facets = . ~ ethnicity)\n\n\nPlotlify! Once loaded, use function ggplotly() on your ggplot oject:\n\nlibrary(plotly)   \n\nggplotly(p)       # Insert your ggplot object!\n\n\n\n\n\nSweet. Not perfect - but with a bit of TLC it’d be right as rain.",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>The Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "6_40-ggplot2.html#combining-plots",
    "href": "6_40-ggplot2.html#combining-plots",
    "title": "23  The Grammar of Graphics",
    "section": "23.10 Combining Plots",
    "text": "23.10 Combining Plots\nOverlaying one plot ontop of another is simply a matter of adding geoms.\nNeeded within those geoms may be new calls to aes().\n\nLet’s begin with a simple jitter plot of net distributed by ethnicity:\n\nggplot(hancock,\n       aes(x = net,\n           y = ethnicity)) +\n  geom_jitter(height = 0.3,\n              alpha = 0.3,\n              color = \"steelblue\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nOverlay plots by adding additional geom. Here, we add geom_boxplot():\n\nggplot(hancock,\n       aes(x = ethnicity,\n           y = net)) +\n  geom_jitter(height = 0.3,\n              alpha = 0.3,\n              color = \"steelblue\") +\n  coord_flip() +\n  theme_minimal() +\n  geom_boxplot(height = 0.3,              # Simply add the new geom\n               fill = \"transparent\",      # Modify attributes accordingly\n               color = \"grey50\")\n\n\n\n\n\n\n\n\n\nThe Final Product: A plot overlay with some finishing touches:\n\nlibrary(extrafont)\n\nggplot(hancock,\n       aes(x = reorder(ethnicity, net),\n           y = net)) +\n  geom_jitter(height = 0.3,\n              alpha = 0.4,\n              color = \"dodgerblue\") +\n  coord_flip() +\n  theme_minimal() +\n  geom_boxplot(height = 0.2,\n               fill = \"transparent\",\n               color = \"grey50\") +\n  labs(title = \"Distribution of weekly net by ethnicity\",\n       subtitle = \"Hancock Airport, 2018\",\n       x = NULL,\n       y = \"Weekly Net\",\n       caption = \"Source: Syracuse Regional Airport Authority\") +\n  scale_y_continuous(labels = dollar) +\n  theme(text = element_text(family = \"Calibri Light\"),\n        axis.title.x = element_text(vjust = -1,\n                                    color = \"grey20\"),\n        plot.caption = element_text(vjust = -1,\n                                    color = \"grey60\"),\n        plot.subtitle = element_text(vjust = 1,\n                                     color = \"grey30\"),\n        plot.title = element_text(vjust = 2,\n                                  color = \"grey10\"),\n        axis.text.x = element_text(colour = \"grey10\"),\n        axis.text.y = element_text(colour = \"grey10\"))",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>The Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "6_40-ggplot2.html#package-ggplot2-extensions",
    "href": "6_40-ggplot2.html#package-ggplot2-extensions",
    "title": "23  The Grammar of Graphics",
    "section": "23.11 Package “ggplot2” Extensions",
    "text": "23.11 Package “ggplot2” Extensions\nWe used a “ggplot2” extension in the introduction to this chapter: “GGally”.\nYou can find a list of every single extension at the ggplot2 extensions gallery.",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>The Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "6_40-ggplot2.html#further-resources",
    "href": "6_40-ggplot2.html#further-resources",
    "title": "23  The Grammar of Graphics",
    "section": "23.12 Further Resources",
    "text": "23.12 Further Resources\nTo review the practice data from Hancock Airport, get the raw data URL here.\nThere’s a lot to master here - but it’s worth it. These can help:\n\nggplot2 Tidyverse Page\nggplot2 Cheat Sheet (RStudio)\nggplot2 Extensions Gallery\nThe R Graph Gallery\nR Documentation: Scales\nR Graphing Library (Plotly)\n“Visualization with ggplot2: Part I”\n“Visualization with ggplot2: Part II”\n“Visualization with ggplot2: Part III”",
    "crumbs": [
      "Intro to Data Viz",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>The Grammar of Graphics</span>"
    ]
  },
  {
    "objectID": "1_10-getting-help.html#section",
    "href": "1_10-getting-help.html#section",
    "title": "5  Getting Help",
    "section": "5.2 ",
    "text": "5.2",
    "crumbs": [
      "Getting Started",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Getting Help</span>"
    ]
  },
  {
    "objectID": "index.html#data-programming-for-the-social-sciences-dp4ss",
    "href": "index.html#data-programming-for-the-social-sciences-dp4ss",
    "title": "DP4SS",
    "section": "Data Programming for the Social Sciences (DP4SS):",
    "text": "Data Programming for the Social Sciences (DP4SS):\nA gentle introduction to data programming in R for social science audiences.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#data-programming-for-the-social-sciences-dp4ss-a-gentle-introduction-to-data-programming-in-r-for-social-science-audiences",
    "href": "index.html#data-programming-for-the-social-sciences-dp4ss-a-gentle-introduction-to-data-programming-in-r-for-social-science-audiences",
    "title": "DP4SS",
    "section": "Data Programming for the Social Sciences (DP4SS): A gentle introduction to data programming in R for social science audiences",
    "text": "Data Programming for the Social Sciences (DP4SS): A gentle introduction to data programming in R for social science audiences",
    "crumbs": [
      "Introduction"
    ]
  }
]